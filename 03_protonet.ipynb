{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d19a75-2afa-4032-9602-28f5cae9f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp protonets\n",
    "#export\n",
    "\n",
    "from torchmeta.datasets.helpers import miniimagenet, omniglot\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "from torchmeta.utils.gradient_based import gradient_update_parameters\n",
    "from torchmeta.modules import MetaModule, MetaSequential, MetaConv2d, MetaBatchNorm2d, MetaLinear\n",
    "\n",
    "import kornia as K\n",
    "import higher\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchnet\n",
    "import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "from unsupervised_meta_learning.nn_utils import Flatten, conv3x3, get_proto_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e63c2-0db6-4168-865f-86641d781a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794058bf-0aa8-4b26-b3fe-29de9dadcb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284abea-4eb9-4bbc-a3e9-c4b40758187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class ProtoNet(nn.Module):\n",
    "    def __init__(self, encoder=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        if encoder is None:\n",
    "            x_dim = kwargs['x_dim']\n",
    "            hid_dim = kwargs['hid_dim']\n",
    "            self.encoder = nn.Sequential(\n",
    "                conv_block(x_dim, hid_dim),\n",
    "                conv_block(hid_dim, hid_dim),\n",
    "                conv_block(hid_dim, hid_dim),\n",
    "                conv_block(hid_dim, hid_dim),\n",
    "                Flatten()\n",
    "            )\n",
    "        \n",
    "    def loss(self, sample):\n",
    "        xs = sample['train']\n",
    "        xq = sample['test']\n",
    "\n",
    "        # might have to change this bit to catch the correct shapes and sizes\n",
    "        n_class = xs.shape[0]\n",
    "        n_support = xs.shape[1]\n",
    "        n_query = xq.shape[1]\n",
    "        \n",
    "        target_inds = torch.arange(0, n_class).view(n_class, 1, 1).expand(n_class, n_query, 1).long()\n",
    "        \n",
    "        if xq.is_cuda:\n",
    "            target_inds = target_inds.to('cuda')\n",
    "            \n",
    "        x = torch.cat([\n",
    "            xs.view(n_class * n_support, *xs.size()[2:] ),\n",
    "            xq.view(n_class * n_query, *xq.size()[2:])\n",
    "        ], 0)\n",
    "        \n",
    "        z = self.encoder(x)\n",
    "        z_dim = z.size(-1)\n",
    "        \n",
    "        z_proto = z[:n_class * n_support].view(n_class, n_support, z_dim).long()\n",
    "        zq = z[n_class * n_support:]\n",
    "        \n",
    "        dists = torch.cdist(zq, z_proto)\n",
    "        \n",
    "        log_p_y: torch.tensor = F.log_softmax(-dists, dim=1).view(n_class, n_query, -1)\n",
    "        \n",
    "        loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n",
    "        \n",
    "        _, y_hat = log_p_y.max(2)\n",
    "        \n",
    "        acc_val = torch.eq(y_hat, target_inds.squeeze()).float().mean()\n",
    "        \n",
    "        return loss_val, {\n",
    "            'loss': loss_val.item(),\n",
    "            'acc': acc_val.item()\n",
    "        }\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124fc817-5704-4066-b9c0-0205b9509962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_size=64):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv3x3(in_channels, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size),\n",
    "            conv3x3(hidden_size, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.encoder(inputs.view(-1, *inputs.shape[2:]))\n",
    "        return embeddings.view(*inputs.shape[:2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82021c18-c3d2-401f-be17-9cc185821768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_num_samples(targets, num_classes, dtype=None):\n",
    "    batch_size = targets.size(0)\n",
    "    with torch.no_grad():\n",
    "        ones = torch.ones_like(targets, dtype=dtype)\n",
    "        num_samples = ones.new_zeros((batch_size, num_classes))\n",
    "        num_samples.scatter_add_(1, targets, ones)\n",
    "    return num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aaea71-3db7-4f03-8c99-3cd4ef953c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_prototypes(emb, targets, num_classes):\n",
    "    \"\"\"Compute the prototypes (the mean vector of the embedded training/support \n",
    "    points belonging to its class) for each classes in the task.\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : `torch.FloatTensor` instance\n",
    "        A tensor containing the embeddings of the support points. This tensor \n",
    "        has shape `(batch_size, num_examples, embedding_size)`.\n",
    "    targets : `torch.LongTensor` instance\n",
    "        A tensor containing the targets of the support points. This tensor has \n",
    "        shape `(batch_size, num_examples)`.\n",
    "    num_classes : int\n",
    "        Number of classes in the task.\n",
    "    Returns\n",
    "    -------\n",
    "    prototypes : `torch.FloatTensor` instance\n",
    "        A tensor containing the prototypes for each class. This tensor has shape\n",
    "        `(batch_size, num_classes, embedding_size)`.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, emb_size = emb.size(0), emb.size(-1)\n",
    "    \n",
    "    num_samples = get_num_samples(targets, num_classes, dtype=emb.dtype)\n",
    "    num_samples.unsqueeze_(-1)\n",
    "    num_samples = torch.max(num_samples, torch.ones_like(num_samples))\n",
    "    \n",
    "    prototypes = emb.new_zeros((batch_size, num_classes, emb_size))\n",
    "    indices = targets.unsqueeze(-1).expand_as(emb)\n",
    "    \n",
    "    prototypes.scatter_add_(1, indices, emb).div_(num_samples)\n",
    "    \n",
    "    return prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb913d6c-b334-468c-90ce-55c48de23ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prototypical_loss(prototypes, emb, targets, **kwargs):\n",
    "    \"\"\"Compute the loss (i.e. negative log-likelihood) for the prototypical \n",
    "    network, on the test/query points.\n",
    "    Parameters\n",
    "    ----------\n",
    "    prototypes : `torch.FloatTensor` instance\n",
    "        A tensor containing the prototypes for each class. This tensor has shape \n",
    "        `(batch_size, num_classes, embedding_size)`.\n",
    "    embeddings : `torch.FloatTensor` instance\n",
    "        A tensor containing the embeddings of the query points. This tensor has \n",
    "        shape `(batch_size, num_examples, embedding_size)`.\n",
    "    targets : `torch.LongTensor` instance\n",
    "        A tensor containing the targets of the query points. This tensor has \n",
    "        shape `(batch_size, num_examples)`.\n",
    "    Returns\n",
    "    -------\n",
    "    loss : `torch.FloatTensor` instance\n",
    "        The negative log-likelihood on the query points.\n",
    "    \"\"\"\n",
    "    squared_distances = torch.sum(\n",
    "        (prototypes.unsqueeze(2) - emb.unsqueeze(1)) ** 2, dim = -1\n",
    "    )\n",
    "    return F.cross_entropy(-squared_distances, targets, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06519a4-5281-40c9-b102-ca710820d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def proto_train(model, folder=\"./data/\", num_shots=1, num_ways=5, batch_size=16, device='cuda', num_batches=100):\n",
    "    ds = omniglot(folder,\n",
    "                  shots=num_shots,\n",
    "                  ways=num_ways,\n",
    "                  shuffle=True,\n",
    "                  test_shots=15,\n",
    "                  meta_train=True,\n",
    "                  download=True\n",
    "                 )\n",
    "    dataloader = BatchMetaDataLoader(ds,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=4\n",
    "                                    )\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    with tqdm.notebook.tqdm(dataloader, total=num_batches) as pbar:\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            model.zero_grad()\n",
    "\n",
    "            train_inputs, train_targets = batch['train']\n",
    "            train_inputs, train_targets = train_inputs.to(device), train_targets.to(device)\n",
    "            \n",
    "            train_emb = model(train_inputs)\n",
    "            \n",
    "            test_inputs, test_targets = batch['test']\n",
    "            test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "            test_emb = model(test_inputs)\n",
    "            \n",
    "            prototypes = get_prototypes(train_emb, train_targets, ds.num_classes_per_task)\n",
    "            loss = prototypical_loss(prototypes, train_emb, train_targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                acc = get_proto_accuracy(prototypes, test_emb, test_targets)\n",
    "                pbar.set_postfix(accuracy='{0:.4f}'.format(acc.item()))\n",
    "                \n",
    "            if batch_idx >= num_batches:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806f9d1-7d39-4e28-b3ba-8f08031881b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_nn_utils.ipynb.\n",
      "Converted 02_maml.ipynb.\n",
      "Converted 03_protonet.ipynb.\n",
      "Converted 04_cactus.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7011a8-57f6-4958-8ccc-c5c935e497e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdf94a22f0d41c6a0834f3458a5aab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PrototypicalNetwork(1, 64, hidden_size=64)\n",
    "proto_train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e06ec7-e367-4b40-afbc-52d60d9759ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
