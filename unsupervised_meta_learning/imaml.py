# AUTOGENERATED! DO NOT EDIT! File to edit: 02b_iMAML.ipynb (unless otherwise specified).

__all__ = ['iMAML']

# Cell
#export
import warnings
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import higher
import wandb

import pytorch_lightning as pl
from itertools import repeat

from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger
from .pl_dataloaders import OmniglotDataModule
from .hessian_free import HessianFree
from .nn_utils import get_accuracy
from .maml import ConvolutionalNeuralNetwork

import unsupervised_meta_learning.hypergrad as hg

# Cell
class iMAML(pl.LightningModule):
    def __init__(self, model, meta_lr, inner_lr, inner_steps, cg_steps, reg_param, hg_mode='CG'):
        super().__init__()
        self.automatic_optimization = False
        self.model = model
        self.meta_lr = meta_lr
        self.inner_lr = inner_lr
        self.inner_steps = inner_steps
        self.cg_steps = cg_steps
        self.n_params = len(list(model.parameters()))
        self.reg_param = reg_param
        self.hg_mode = hg_mode
        self.T = 16
        self.K = 5

        self.fmodel = higher.monkeypatch(model, device=self.device, copy_initial_weights=True)
        self.inner_opt_cls = hg.GradientDescent

    def bias_reg_f(self, bias, params):
        return sum(
            [((b - p) ** 2).sum() for b, p in zip(bias, params)]
        )

    def train_loss_f(self, params, hparams):
        o = self.fmodel(self.tr_x, params=params)
        return F.cross_entropy(o, self.tr_y) + .5 + self.reg_param + self.bias_reg_f(hparams, params)

    def val_loss_f(self, params, hparams):
        o = self.fmodel(self.tst_x, params=params)
        val_loss = F.cross_entropy(o, self.tst_y) / self.batch_size
        self.val_loss = val_loss.item()
        pred = o.argmax(dim=1, keepdim=True)
        self.val_acc = pred.eq(self.tst_y.view_as(pred)).sum().item() / len(self.tst_y)
        return val_loss


    def get_inner_opt(self, train_loss, kwargs):
        return self.inner_opt_cls(train_loss, **kwargs)

    def inner_loop(self, hparams, params, optim, n_steps, log_interval, create_graph=False):
        params_history = [optim.get_opt_params(params)]

        for t in range(n_steps):
            params_history.append(optim(params_history[-1], hparams, create_graph=create_graph))
            self.log('loss', optim.curr_loss.item(), on_step=True, prog_bar=True, logger=True)

        return params_history

    def configure_optimizers(self):
        outer_opt = torch.optim.Adam(params=self.model.parameters(), lr=1e-3)
        return outer_opt

    @torch.enable_grad()
    def meta_learn(self, batch, batch_idx):
        meta_optimizer = self.optimizers()
        meta_optimizer = meta_optimizer.optimizer
        tr_xs, tr_ys = batch["train"][0].to(self.device), batch["train"][1].to(self.device)
        tst_xs, tst_ys = batch["test"][0].to(self.device), batch["test"][1].to(self.device)


        self.batch_size = tr_xs.shape[0]
        val_loss, val_acc = torch.tensor(0., device=self.device), torch.tensor(0., device=self.device)

        inner_opt_kwargs = {'step_size': self.inner_lr}


        meta_optimizer.zero_grad()
        for t_idx, (tr_x, tr_y, tst_x, tst_y) in enumerate(zip(tr_xs, tr_ys, tst_xs, tst_ys)):
            self.tr_x = tr_x
            self.tr_y = tr_y

            self.tst_x = tst_x
            self.tst_y = tst_y
            inner_opt = self.get_inner_opt(self.train_loss_f, inner_opt_kwargs)

            params = [p.detach().clone().requires_grad_(True) for p in self.model.parameters()]
            last_param = self.inner_loop(self.model.parameters(), params, inner_opt, self.T, log_interval=None)[-1]

            if self.hg_mode == 'CG':
                # This is the approximation used in the paper CG stands for conjugate gradient
                cg_fp_map = hg.GradientDescent(loss_f=self.train_loss_f, step_size=1.)
                hg.CG(last_param, list(self.model.parameters()), K=self.K, fp_map=cg_fp_map, outer_loss=self.val_loss_f)
            elif self.hg_mode == 'fixed_point':
                hg.fixed_point(last_param, list(seld.model.parameters()), K=self.K, fp_map=inner_opt,
                               outer_loss=self.val_loss_f)

            val_loss += self.val_loss
            val_acc += self.val_acc / self.batch_size

        meta_optimizer.step()
        return val_loss, val_acc

    def training_step(self, batch, batch_idx):
        train_loss, train_acc = self.meta_learn(batch, batch_idx)
        self.log_dict({
            'tr_accuracy': train_acc.item(),
            'tr_loss': train_loss.item()
        }, prog_bar=True, logger=True)
        return {'tr_loss': train_loss.item(), 'tr_acc': train_acc.item()}


    def validation_step(self, batch, batch_idx):
        val_loss, val_acc = self.meta_learn(batch, batch_idx)

        self.log_dict({
            'val_loss': val_loss.item(),
            'val_accuracy': val_acc.item()
        })
        return val_loss.item()

    def test_step(self, batch, batch_idx):
        test_loss, test_acc = self.meta_learn(batch, batch_idx)
        self.log_dict({
            'test_loss': test_loss.item(),
            'test_accuracy': test_acc.item()
        })
        return test_loss.item()