# AUTOGENERATED! DO NOT EDIT! File to edit: 04_cactus.ipynb (unless otherwise specified).

__all__ = ['Partition', 'CactusTaskLoader', 'get_partition_kmeans', 'DataOpt', 'LoaderOpt', 'load']

# Cell
#export
import os
from dataclasses import dataclass
import kornia as K

import numpy as np
import matplotlib.pyplot as plt
import torchvision
import torchvision.transforms as tfms
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchnet
import tqdm

from collections import OrderedDict
from functools import partial
from torchmeta.datasets.helpers import miniimagenet, omniglot
from torchmeta.utils.data import BatchMetaDataLoader
from torchmeta.utils.gradient_based import gradient_update_parameters
from torchmeta.modules import MetaModule, MetaSequential, MetaConv2d, MetaBatchNorm2d, MetaLinear
from torchnet.dataset import ListDataset, TransformDataset

from sklearn.cluster import KMeans

# Cell

class Partition():
    def __init__(self, labels, n_way, n_shot, n_query):
        partition = defaultdict(list)
        cleaned_partition = {}
        for ind, label in enumerate(labels):
            partition[label].append(ind)
        for label in list(partition.keys()):
            if len(partition[label]) >= n_shot + n_query:
                cleaned_partition[label] = np.array(partition[label], dtype=np.int)
        self.partition = cleaned_partition
        self.subset_ids = np.array(list(cleaned_partition.keys()))

    def __getitem__(self, key):
        return self.partition[key]

# Cell
class CactusTaskLoader():
    def __init__(self, data, partitions, n_way, n_shot, n_query, cuda, length):
        self.data = data
        self.partitions = partitions
        self.n_way = n_way
        self.n_shot = n_shot
        self.n_query = n_query
        self.cuda = cuda
        self.length = length
        self.iter = 0

    def reset(self):
        self.iter = 0

    def __iter__(self):
        return self

    def __len__(self):
        return self.length

    def __next__(self):
        if self.iter == self.length:
            raise StopIteration
        self.iter += 1

        i_partition = torch.randint(low=0, high=len(self.partitions), size=(1,), dtype=torch.int)
        partition = self.partitions[i_partition]
        sampled_subset_ids = np.random.choice(partition.subset_ids, size=self.n_way, replace=False)
        xs, xq = [], []
        for subset_id in sampled_subset_ids:
            indices = np.random.choice(partition[subset_id], self.n_shot + self.n_query, replace=False)
            x = self.data[indices]
            x = x.astype(np.float32) / 255.0
            if x.shape[1] != 1 and x.shape[1] != 3:
                x = np.transpose(x, [0, 3, 1, 2])
            x = torch.from_numpy(x)
            xs.append(x[:self.n_shot])
            xq.append(x[self.n_shot:])
        xs = torch.stack(xs, dim=0)
        xq = torch.stack(xq, dim=0)
        if self.cuda:
            xs = xs.cuda()
            xq = xq.cuda()
        return {'xs': xs, 'xq': xq}

# Cell
def get_partition_kmeans(encodings, n_way, n_shot, n_query, random_scaling=True, n_partitions=100, n_clusters=500):
    os.environ['JOBLIB_TEMP_FOLDER'] = '/tmp'  # default runs out of space for parallel processing

    encodings_list = [encodings]

    if random_scaling:
        n_clusters_list = [n_clusters]
        for i in range(n_partitions - 1):
            weight_vector = np.random.uniform(low=0., high=1., size=encodings.shape[1])
            encodings_list.append(np.multiply(encodings, weight_vector))

    else:
        n_clusters_list = [n_clusters] * n_partitions

    assert len(encodings_list) * len(n_clusters_list) == n_partitions

    if n_partitions != 1:
        n_init = 3
        init = 'k-means++'
    else:
        n_init = 10
        init = 'k-means++'

    print('Number of encodings: {}, number of n_clusters: {}, number of inits: '.format(len(encodings_list),
                                                                                        len(n_clusters_list)), n_init)
    kmeans_list = []

    for n_clusters in tqdm.tqdm(n_clusters_list, desc='get_partitions_kmeans_n_clusters'):
        for encodings in tqdm.tqdm(encodings_list, desc='get_partitions_kmeans_encodings'):
            while True:
                kmeans = KMeans(n_clusters=n_clusters,
                                init=init,
                                precompute_distances=True,
                                n_jobs=-1,
                                n_init=n_init,
                                max_iter=3000).fit(encodings)
                uniques, counts = np.unique(kmeans.labels_, return_counts=True)
                num_big_enough_clusters = np.sum(counts >= n_shot + n_query)

                if num_big_enough_clusters > .8 * n_clusters:
                    break
                else:
                    tqdm.write("Too few classes ({}) with greater than {} examples.".format(num_big_enough_clusters,
                                                                                            n_shot + n_query))
                    tqdm.write('Frequency: {}'.format(counts))
            kmeans_list.append(kmeans)
    partitions = []
    for kmeans in kmeans_list:
        partitions.append(Partition(labels=kmeans.labels_, n_way=n_way, n_shot=n_shot, n_query=n_query))
    return partitions

# Cell
@dataclass
class DataOpt:
    dataset: str
    test_way: int
    way: int
    test_shot: int
    shot: int
    test_query: int
    query: int
    test_episodes: int
    test_mode:int
    partitions:int
    clusters:int
    train_mode: bool
    train_episodes: int
    cuda: bool

@dataclass
class LoaderOpt:
    data: DataOpt

# Cell
def load(opt:LoaderOpt, splits, data_dir):
    encodings_dir = os.path.join(data_dir, '{}_encodings'.format(opt['data.encoder']))
    filenames = os.listdir(encodings_dir)

    ret = {}

    for split in splits:
        if split in ['val', 'test'] and opt.data.test_way != 0:
            n_way = opt.data.test_way
        else:
            n_way = opt.data.way

        if split in ['val', 'test'] and opt.data.test_shot != 0:
            n_support = opt.data.test_shot
        else:
            n_support = opt.data.shot

        if split in ['val', 'test'] and opt.data.test_query != 0:
            n_query = opt.data.test_query
        else:
            n_query = opt.data.query

        if split in ['val', 'test']:
            n_episodes = opt.data.test_episodes
            mode = opt.data.test_mode
        else:
            n_episodes = opt.data.train_episodes
            mode = opt.data.train_mode

        split_filename = [filename for filename in filenames if opt.data.dataset in filename and split in filename]
        split_filename = os.path.join(encodings_dir, split_filename[0])
        split_data = np.load(split_filename)
        images = split_data['X']    # (index, H, W, C)
        labels = split_data['Y']
        encodings = split_data['Z']

        if mode == 'ground_truth':
            if opt.data.dataset == 'celeba':
#                 TODO: need to change this part
                annotations_filename = os.path.join(DATA_DIR, 'celeba/cropped/Anno/list_attr_celeba.txt')
                partitions = celeba_partitions(labels=labels, split=split, annotations_filename=annotations_filename, n_way=n_way, n_shot=n_support, n_query=n_query)
            else:
                partitions = [Partition(labels=labels, n_way=n_way, n_shot=n_support, n_query=n_query)]

        elif mode == 'kmeans':
            partitions = get_partitions_kmeans(encodings=encodings, n_way=n_way, n_shot=n_support, n_query=n_query, n_partitions=opt.data.partitions, n_clusters=opt.data.clusters)

        elif mode == 'random':
            partitions = [Partition(labels=np.random.choice(opt.data.clusters, size=labels.shape, replace=True), n_way=n_way, n_shot=n_support, n_query=n_query) for i in range(opt.data.partitions)]
        else:
            raise ValueError
        ret[split] = TaskLoader(data=images, partitions=partitions, n_way=n_way, n_shot=n_support, n_query=n_query,
                                cuda=opt.data.cuda, length=n_episodes)

    return ret
