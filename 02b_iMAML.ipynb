{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f3e7e-4c0e-4f68-a282-018ae80ec094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp imaml\n",
    "#export\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import higher\n",
    "import wandb\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from itertools import repeat\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "from unsupervised_meta_learning.pl_dataloaders import OmniglotDataModule\n",
    "from unsupervised_meta_learning.hessian_free import HessianFree\n",
    "from unsupervised_meta_learning.nn_utils import get_accuracy\n",
    "from unsupervised_meta_learning.maml import ConvolutionalNeuralNetwork\n",
    "\n",
    "import unsupervised_meta_learning.hypergrad as hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc914cda-ab15-45f9-94a1-fa001d4f7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class iMAML(pl.LightningModule):\n",
    "    def __init__(self, model, meta_lr, inner_lr, inner_steps, cg_steps, reg_param, hg_mode='CG'):\n",
    "        super().__init__()\n",
    "        self.automatic_optimization = False\n",
    "        self.model = model\n",
    "        self.meta_lr = meta_lr\n",
    "        self.inner_lr = inner_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.cg_steps = cg_steps\n",
    "        self.n_params = len(list(model.parameters()))\n",
    "        self.reg_param = reg_param\n",
    "        self.hg_mode = hg_mode\n",
    "        self.T = 16\n",
    "        self.K = 5\n",
    "        \n",
    "        self.fmodel = higher.monkeypatch(model, device=self.device, copy_initial_weights=True)\n",
    "        self.inner_opt_cls = hg.GradientDescent\n",
    "    \n",
    "    def bias_reg_f(self, bias, params):\n",
    "        return sum(\n",
    "            [((b - p) ** 2).sum() for b, p in zip(bias, params)]\n",
    "        )\n",
    "    \n",
    "    def train_loss_f(self, params, hparams):\n",
    "        o = self.fmodel(self.tr_x, params=params)\n",
    "        return F.cross_entropy(o, self.tr_y) + .5 + self.reg_param + self.bias_reg_f(hparams, params)\n",
    "    \n",
    "    def val_loss_f(self, params, hparams):\n",
    "        o = self.fmodel(self.tst_x, params=params)\n",
    "        val_loss = F.cross_entropy(o, self.tst_y) / self.batch_size\n",
    "        self.val_loss = val_loss.item()\n",
    "        pred = o.argmax(dim=1, keepdim=True)\n",
    "        self.val_acc = pred.eq(self.tst_y.view_as(pred)).sum().item() / len(self.tst_y)\n",
    "        return val_loss\n",
    "    \n",
    "    \n",
    "    def get_inner_opt(self, train_loss, kwargs):\n",
    "        return self.inner_opt_cls(train_loss, **kwargs)\n",
    "    \n",
    "    def inner_loop(self, hparams, params, optim, n_steps, log_interval, create_graph=False):\n",
    "        params_history = [optim.get_opt_params(params)]\n",
    "\n",
    "        for t in range(n_steps):\n",
    "            params_history.append(optim(params_history[-1], hparams, create_graph=create_graph))\n",
    "            self.log('loss', optim.curr_loss.item(), on_step=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return params_history\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        outer_opt = torch.optim.Adam(params=self.model.parameters(), lr=1e-3)\n",
    "        return outer_opt\n",
    "    \n",
    "    @torch.enable_grad()\n",
    "    def meta_learn(self, batch, batch_idx):\n",
    "        meta_optimizer = self.optimizers()\n",
    "        meta_optimizer = meta_optimizer.optimizer        \n",
    "        tr_xs, tr_ys = batch[\"train\"][0].to(self.device), batch[\"train\"][1].to(self.device)\n",
    "        tst_xs, tst_ys = batch[\"test\"][0].to(self.device), batch[\"test\"][1].to(self.device)\n",
    "        \n",
    "                \n",
    "        self.batch_size = tr_xs.shape[0]\n",
    "        val_loss, val_acc = torch.tensor(0., device=self.device), torch.tensor(0., device=self.device)\n",
    "        \n",
    "        inner_opt_kwargs = {'step_size': self.inner_lr}\n",
    "        \n",
    "        \n",
    "        meta_optimizer.zero_grad()\n",
    "        for t_idx, (tr_x, tr_y, tst_x, tst_y) in enumerate(zip(tr_xs, tr_ys, tst_xs, tst_ys)):\n",
    "            self.tr_x = tr_x\n",
    "            self.tr_y = tr_y\n",
    "\n",
    "            self.tst_x = tst_x\n",
    "            self.tst_y = tst_y\n",
    "            inner_opt = self.get_inner_opt(self.train_loss_f, inner_opt_kwargs)\n",
    "            \n",
    "            params = [p.detach().clone().requires_grad_(True) for p in self.model.parameters()]\n",
    "            last_param = self.inner_loop(self.model.parameters(), params, inner_opt, self.T, log_interval=None)[-1]\n",
    "            \n",
    "            if self.hg_mode == 'CG':\n",
    "                # This is the approximation used in the paper CG stands for conjugate gradient\n",
    "                cg_fp_map = hg.GradientDescent(loss_f=self.train_loss_f, step_size=1.)\n",
    "                hg.CG(last_param, list(self.model.parameters()), K=self.K, fp_map=cg_fp_map, outer_loss=self.val_loss_f)\n",
    "            elif self.hg_mode == 'fixed_point':\n",
    "                hg.fixed_point(last_param, list(seld.model.parameters()), K=self.K, fp_map=inner_opt,\n",
    "                               outer_loss=self.val_loss_f)\n",
    "            \n",
    "            val_loss += self.val_loss\n",
    "            val_acc += self.val_acc / self.batch_size\n",
    "\n",
    "        meta_optimizer.step()\n",
    "        return val_loss, val_acc\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        train_loss, train_acc = self.meta_learn(batch, batch_idx)\n",
    "        self.log_dict({\n",
    "            'tr_accuracy': train_acc.item(),\n",
    "            'tr_loss': train_loss.item()\n",
    "        }, prog_bar=True, logger=True)\n",
    "        return {'tr_loss': train_loss.item(), 'tr_acc': train_acc.item()}TensorBoard\n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        val_loss, val_acc = self.meta_learn(batch, batch_idx)\n",
    "        \n",
    "        self.log_dict({\n",
    "            'val_loss': val_loss.item(),\n",
    "            'val_accuracy': val_acc.item()\n",
    "        })\n",
    "        return val_loss.item()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss, test_acc = self.meta_learn(batch, batch_idx)\n",
    "        self.log_dict({\n",
    "            'test_loss': test_loss.item(),\n",
    "            'test_accuracy': test_acc.item()\n",
    "        })\n",
    "        return test_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe0875-b062-416e-80ba-a2d8a9a964f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = OmniglotDataModule(\n",
    "        \"data\",\n",
    "        shots=1,\n",
    "        ways=5,\n",
    "        shuffle_ds=True,\n",
    "        test_shots=16,\n",
    "        meta_train=True,\n",
    "        download=True,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb9918-ec50-40eb-8cd1-d23e1027773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = iMAML(model=ConvolutionalNeuralNetwork(1, 5, hidden_size=64), meta_lr=1e-3, inner_lr=1e-2, reg_param=2, inner_steps=1, cg_steps=5, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f556c1-4515-4855-8229-933b9b87f929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "logger = WandbLogger(\n",
    "    project='iMAML',\n",
    "    config={\n",
    "        'batch_size': 16,\n",
    "        'steps': 100,\n",
    "        'dataset': \"omniglot\",\n",
    "        'T': 16,\n",
    "        'K': 5,\n",
    "        'val/test': 'enabled'\n",
    "    }\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "        profiler='simple',\n",
    "        max_steps=28,\n",
    "        val_check_interval=25,\n",
    "        limit_train_batches=26,\n",
    "        limit_val_batches=2,\n",
    "        limit_test_batches=2,\n",
    "        fast_dev_run=False,\n",
    "        gpus=1,\n",
    "        logger=logger,\n",
    "        log_every_n_steps=1,\n",
    "        flush_logs_every_n_steps=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab439e-8b49-4763-b740-4e823b101040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mp0int\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">fallen-snowflake-26</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/p0int/iMAML\" target=\"_blank\">https://wandb.ai/p0int/iMAML</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/p0int/iMAML/runs/xtkzj4yb\" target=\"_blank\">https://wandb.ai/p0int/iMAML/runs/xtkzj4yb</a><br/>\n",
       "                Run data is saved locally in <code>/home/ojass/Projects/meta-learning/wandb/run-20210607_214725-xtkzj4yb</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name   | Type                                 | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model  | ConvolutionalNeuralNetwork           | 112 K \n",
      "1 | fmodel | FunctionalConvolutionalNeuralNetwork | 112 K \n",
      "----------------------------------------------------------------\n",
      "224 K     Trainable params\n",
      "0         Non-trainable params\n",
      "224 K     Total params\n",
      "0.898     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceed943130b74d849211d8f3cca79f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  65.453         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  26.36          \t|2              \t|  52.719         \t|  80.545         \t|\n",
      "run_training_batch                 \t|  1.6216         \t|28             \t|  45.404         \t|  69.368         \t|\n",
      "model_forward                      \t|  1.6214         \t|28             \t|  45.399         \t|  69.362         \t|\n",
      "training_step                      \t|  1.6212         \t|28             \t|  45.395         \t|  69.355         \t|\n",
      "evaluation_step_and_end            \t|  1.9389         \t|4              \t|  7.7555         \t|  11.849         \t|\n",
      "validation_step                    \t|  1.9387         \t|4              \t|  7.7548         \t|  11.848         \t|\n",
      "get_train_batch                    \t|  0.076477       \t|28             \t|  2.1414         \t|  3.2716         \t|\n",
      "on_train_batch_end                 \t|  0.0014177      \t|28             \t|  0.039696       \t|  0.060648       \t|\n",
      "on_train_start                     \t|  0.018566       \t|1              \t|  0.018566       \t|  0.028365       \t|\n",
      "on_validation_start                \t|  0.0092088      \t|2              \t|  0.018418       \t|  0.028139       \t|\n",
      "on_validation_end                  \t|  0.005327       \t|2              \t|  0.010654       \t|  0.016277       \t|\n",
      "on_validation_batch_end            \t|  0.0016388      \t|4              \t|  0.0065553      \t|  0.010015       \t|\n",
      "on_train_epoch_start               \t|  0.0014712      \t|2              \t|  0.0029424      \t|  0.0044954      \t|\n",
      "cache_result                       \t|  1.6146e-05     \t|151            \t|  0.0024381      \t|  0.0037249      \t|\n",
      "on_train_end                       \t|  0.00086668     \t|1              \t|  0.00086668     \t|  0.0013241      \t|\n",
      "on_batch_start                     \t|  2.5675e-05     \t|28             \t|  0.00071891     \t|  0.0010984      \t|\n",
      "on_train_epoch_end                 \t|  0.00017159     \t|2              \t|  0.00034318     \t|  0.00052431     \t|\n",
      "on_train_batch_start               \t|  1.2216e-05     \t|28             \t|  0.00034204     \t|  0.00052257     \t|\n",
      "training_step_end                  \t|  1.048e-05      \t|28             \t|  0.00029345     \t|  0.00044834     \t|\n",
      "on_batch_end                       \t|  1.0125e-05     \t|28             \t|  0.00028351     \t|  0.00043315     \t|\n",
      "on_validation_batch_start          \t|  5.2635e-05     \t|4              \t|  0.00021054     \t|  0.00032167     \t|\n",
      "on_epoch_start                     \t|  1.4956e-05     \t|4              \t|  5.9825e-05     \t|  9.1402e-05     \t|\n",
      "validation_step_end                \t|  1.1657e-05     \t|4              \t|  4.663e-05      \t|  7.1242e-05     \t|\n",
      "on_epoch_end                       \t|  7.7597e-06     \t|4              \t|  3.1039e-05     \t|  4.7422e-05     \t|\n",
      "on_validation_epoch_end            \t|  1.3603e-05     \t|2              \t|  2.7206e-05     \t|  4.1566e-05     \t|\n",
      "on_before_accelerator_backend_setup\t|  2.5949e-05     \t|1              \t|  2.5949e-05     \t|  3.9645e-05     \t|\n",
      "on_fit_start                       \t|  2.1025e-05     \t|1              \t|  2.1025e-05     \t|  3.2122e-05     \t|\n",
      "on_validation_epoch_start          \t|  9.776e-06      \t|2              \t|  1.9552e-05     \t|  2.9872e-05     \t|\n",
      "on_val_dataloader                  \t|  9.013e-06      \t|1              \t|  9.013e-06      \t|  1.377e-05      \t|\n",
      "on_train_dataloader                \t|  7.089e-06      \t|1              \t|  7.089e-06      \t|  1.0831e-05     \t|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292be7e-85bf-4196-b476-1a791bb295bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Your test_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dce1f4aeac473a91ad6c3ace73bafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b850b2-b10f-41ec-b914-c80719ae1792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 107657<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/ojass/Projects/meta-learning/wandb/run-20210607_214725-xtkzj4yb/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/ojass/Projects/meta-learning/wandb/run-20210607_214725-xtkzj4yb/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>2.5236</td></tr><tr><td>tr_accuracy</td><td>0.84531</td></tr><tr><td>tr_loss</td><td>0.5584</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>trainer/global_step</td><td>27</td></tr><tr><td>_runtime</td><td>61</td></tr><tr><td>_timestamp</td><td>1623095306</td></tr><tr><td>_step</td><td>30</td></tr><tr><td>loss_step/epoch_0</td><td>2.53318</td></tr><tr><td>loss_epoch</td><td>2.52925</td></tr><tr><td>val_loss</td><td>0.67879</td></tr><tr><td>val_accuracy</td><td>0.81836</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>▇▄▅▆▇▆▆▇▆▅▄▅▄▄▄▅▅▃▅▃▄█▃▃▃▁▂▁</td></tr><tr><td>tr_accuracy</td><td>▂▂▁▃▄▅▆▅▃▄▅▇▄▇▆▆▆▅▆█▇██▇▆█▇▇</td></tr><tr><td>tr_loss</td><td>███▆▆▅▅▅▆▅▅▃▅▃▃▄▂▄▃▂▂▁▂▃▃▁▂▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▁▁▇▇██</td></tr><tr><td>_runtime</td><td>▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>loss_step/epoch_0</td><td>▁█</td></tr><tr><td>loss_epoch</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">fallen-snowflake-26</strong>: <a href=\"https://wandb.ai/p0int/iMAML/runs/xtkzj4yb\" target=\"_blank\">https://wandb.ai/p0int/iMAML/runs/xtkzj4yb</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b764773-52d9-496f-9987-c00fb220c8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
