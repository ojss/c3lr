{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473cb51-04f6-46ef-801c-0b0587ec6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp cactus\n",
    "#export\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import kornia as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as tfms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchnet\n",
    "import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from torchmeta.datasets.helpers import miniimagenet, omniglot\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "from torchmeta.utils.gradient_based import gradient_update_parameters\n",
    "from torchmeta.modules import MetaModule, MetaSequential, MetaConv2d, MetaBatchNorm2d, MetaLinear\n",
    "from torchnet.dataset import ListDataset, TransformDataset\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9cca5-edcf-4617-ba5a-0a060da0dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class Partition():\n",
    "    def __init__(self, labels, n_way, n_shot, n_query):\n",
    "        partition = defaultdict(list)\n",
    "        cleaned_partition = {}\n",
    "        for ind, label in enumerate(labels):\n",
    "            partition[label].append(ind)\n",
    "        for label in list(partition.keys()):\n",
    "            if len(partition[label]) >= n_shot + n_query:\n",
    "                cleaned_partition[label] = np.array(partition[label], dtype=np.int)\n",
    "        self.partition = cleaned_partition\n",
    "        self.subset_ids = np.array(list(cleaned_partition.keys()))\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.partition[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350b471-3cfe-4ec4-94a7-22ed8e67a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CactusTaskLoader():\n",
    "    def __init__(self, data, partitions, n_way, n_shot, n_query, cuda, length):\n",
    "        self.data = data\n",
    "        self.partitions = partitions\n",
    "        self.n_way = n_way\n",
    "        self.n_shot = n_shot\n",
    "        self.n_query = n_query\n",
    "        self.cuda = cuda\n",
    "        self.length = length\n",
    "        self.iter = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.iter = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iter == self.length:\n",
    "            raise StopIteration\n",
    "        self.iter += 1\n",
    "\n",
    "        i_partition = torch.randint(low=0, high=len(self.partitions), size=(1,), dtype=torch.int)\n",
    "        partition = self.partitions[i_partition]\n",
    "        sampled_subset_ids = np.random.choice(partition.subset_ids, size=self.n_way, replace=False)\n",
    "        xs, xq = [], []\n",
    "        for subset_id in sampled_subset_ids:\n",
    "            indices = np.random.choice(partition[subset_id], self.n_shot + self.n_query, replace=False)\n",
    "            x = self.data[indices]\n",
    "            x = x.astype(np.float32) / 255.0\n",
    "            if x.shape[1] != 1 and x.shape[1] != 3:\n",
    "                x = np.transpose(x, [0, 3, 1, 2])\n",
    "            x = torch.from_numpy(x)\n",
    "            xs.append(x[:self.n_shot])\n",
    "            xq.append(x[self.n_shot:])\n",
    "        xs = torch.stack(xs, dim=0)\n",
    "        xq = torch.stack(xq, dim=0)\n",
    "        if self.cuda:\n",
    "            xs = xs.cuda()\n",
    "            xq = xq.cuda()\n",
    "        return {'xs': xs, 'xq': xq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66ff54-4844-4a08-8ef2-c03c70f5a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_partition_kmeans(encodings, n_way, n_shot, n_query, random_scaling=True, n_partitions=100, n_clusters=500):\n",
    "    os.environ['JOBLIB_TEMP_FOLDER'] = '/tmp'  # default runs out of space for parallel processing\n",
    "    \n",
    "    encodings_list = [encodings]\n",
    "    \n",
    "    if random_scaling:\n",
    "        n_clusters_list = [n_clusters]\n",
    "        for i in range(n_partitions - 1):\n",
    "            weight_vector = np.random.uniform(low=0., high=1., size=encodings.shape[1])\n",
    "            encodings_list.append(np.multiply(encodings, weight_vector))\n",
    "            \n",
    "    else:\n",
    "        n_clusters_list = [n_clusters] * n_partitions\n",
    "    \n",
    "    assert len(encodings_list) * len(n_clusters_list) == n_partitions\n",
    "    \n",
    "    if n_partitions != 1:\n",
    "        n_init = 3\n",
    "        init = 'k-means++'\n",
    "    else:\n",
    "        n_init = 10\n",
    "        init = 'k-means++'\n",
    "    \n",
    "    print('Number of encodings: {}, number of n_clusters: {}, number of inits: '.format(len(encodings_list),\n",
    "                                                                                        len(n_clusters_list)), n_init)\n",
    "    kmeans_list = []\n",
    "    \n",
    "    for n_clusters in tqdm.tqdm(n_clusters_list, desc='get_partitions_kmeans_n_clusters'):\n",
    "        for encodings in tqdm.tqdm(encodings_list, desc='get_partitions_kmeans_encodings'):\n",
    "            while True:\n",
    "                kmeans = KMeans(n_clusters=n_clusters,\n",
    "                                init=init,\n",
    "                                precompute_distances=True,\n",
    "                                n_jobs=-1,\n",
    "                                n_init=n_init,\n",
    "                                max_iter=3000).fit(encodings)\n",
    "                uniques, counts = np.unique(kmeans.labels_, return_counts=True)\n",
    "                num_big_enough_clusters = np.sum(counts >= n_shot + n_query)\n",
    "                \n",
    "                if num_big_enough_clusters > .8 * n_clusters:\n",
    "                    break\n",
    "                else:\n",
    "                    tqdm.write(\"Too few classes ({}) with greater than {} examples.\".format(num_big_enough_clusters,\n",
    "                                                                                            n_shot + n_query))\n",
    "                    tqdm.write('Frequency: {}'.format(counts))\n",
    "            kmeans_list.append(kmeans)\n",
    "    partitions = []\n",
    "    for kmeans in kmeans_list:\n",
    "        partitions.append(Partition(labels=kmeans.labels_, n_way=n_way, n_shot=n_shot, n_query=n_query))\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29f85d-c36a-48dc-b9b2-d536411ffa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class DataOpt:\n",
    "    dataset: str\n",
    "    test_way: int\n",
    "    way: int\n",
    "    test_shot: int\n",
    "    shot: int\n",
    "    test_query: int\n",
    "    query: int\n",
    "    test_episodes: int\n",
    "    test_mode:int\n",
    "    partitions:int\n",
    "    clusters:int\n",
    "    train_mode: bool\n",
    "    train_episodes: int\n",
    "    cuda: bool\n",
    "\n",
    "@dataclass\n",
    "class LoaderOpt:\n",
    "    data: DataOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0994ad-5147-4439-b1f5-e0e393027111",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DataOpt(1,2,3,4,5,6,7,8,9,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238ec00-8e4b-4942-ba2b-260c082f897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = LoaderOpt(data=dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39978c-874f-4867-ad3d-1d4233e2e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load(opt:LoaderOpt, splits, data_dir):\n",
    "    encodings_dir = os.path.join(data_dir, '{}_encodings'.format(opt['data.encoder']))\n",
    "    filenames = os.listdir(encodings_dir)\n",
    "    \n",
    "    ret = {}\n",
    "    \n",
    "    for split in splits:\n",
    "        if split in ['val', 'test'] and opt.data.test_way != 0:\n",
    "            n_way = opt.data.test_way\n",
    "        else:\n",
    "            n_way = opt.data.way\n",
    "\n",
    "        if split in ['val', 'test'] and opt.data.test_shot != 0:\n",
    "            n_support = opt.data.test_shot\n",
    "        else:\n",
    "            n_support = opt.data.shot\n",
    "\n",
    "        if split in ['val', 'test'] and opt.data.test_query != 0:\n",
    "            n_query = opt.data.test_query\n",
    "        else:\n",
    "            n_query = opt.data.query\n",
    "\n",
    "        if split in ['val', 'test']:\n",
    "            n_episodes = opt.data.test_episodes\n",
    "            mode = opt.data.test_mode\n",
    "        else:\n",
    "            n_episodes = opt.data.train_episodes\n",
    "            mode = opt.data.train_mode\n",
    "        \n",
    "        split_filename = [filename for filename in filenames if opt.data.dataset in filename and split in filename]\n",
    "        split_filename = os.path.join(encodings_dir, split_filename[0])\n",
    "        split_data = np.load(split_filename)\n",
    "        images = split_data['X']    # (index, H, W, C)\n",
    "        labels = split_data['Y']\n",
    "        encodings = split_data['Z']\n",
    "        \n",
    "        if mode == 'ground_truth':\n",
    "            if opt.data.dataset == 'celeba':\n",
    "#                 TODO: need to change this part\n",
    "                annotations_filename = os.path.join(DATA_DIR, 'celeba/cropped/Anno/list_attr_celeba.txt')\n",
    "                partitions = celeba_partitions(labels=labels, split=split, annotations_filename=annotations_filename, n_way=n_way, n_shot=n_support, n_query=n_query)\n",
    "            else:\n",
    "                partitions = [Partition(labels=labels, n_way=n_way, n_shot=n_support, n_query=n_query)]\n",
    "\n",
    "        elif mode == 'kmeans':\n",
    "            partitions = get_partitions_kmeans(encodings=encodings, n_way=n_way, n_shot=n_support, n_query=n_query, n_partitions=opt.data.partitions, n_clusters=opt.data.clusters)\n",
    "\n",
    "        elif mode == 'random':\n",
    "            partitions = [Partition(labels=np.random.choice(opt.data.clusters, size=labels.shape, replace=True), n_way=n_way, n_shot=n_support, n_query=n_query) for i in range(opt.data.partitions)]\n",
    "        else:\n",
    "            raise ValueError\n",
    "        ret[split] = TaskLoader(data=images, partitions=partitions, n_way=n_way, n_shot=n_support, n_query=n_query,\n",
    "                                cuda=opt.data.cuda, length=n_episodes)\n",
    "\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a6a83-14a2-4a25-8ddb-a4679011bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_nn_utils.ipynb.\n",
      "Converted 02_maml.ipynb.\n",
      "Converted 03_protonet.ipynb.\n",
      "Converted 04_cactus.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b2454-c791-4953-b2cc-72b235f5f713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
