{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473cb51-04f6-46ef-801c-0b0587ec6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp cactus\n",
    "#export\n",
    "import os\n",
    "import tempfile\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import kornia as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as tfms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchnet\n",
    "import pytorch_lightning as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchnet.dataset import ListDataset, TransformDataset\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from unsupervised_meta_learning.nn_utils import c_imshow\n",
    "from unsupervised_meta_learning.protonets import CactusPrototypicalModel, ProtoModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2b314-e1cb-4f9b-a669-a37524d5222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9cca5-edcf-4617-ba5a-0a060da0dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Partition():\n",
    "    def __init__(self, labels, n_way, n_shot, n_query):\n",
    "        partition = defaultdict(list)\n",
    "        cleaned_partition = {}\n",
    "        for ind, label in enumerate(labels):\n",
    "            partition[label].append(ind)\n",
    "        for label in list(partition.keys()):\n",
    "            if len(partition[label]) >= n_shot + n_query:\n",
    "                cleaned_partition[label] = np.array(partition[label], dtype=np.int)\n",
    "        self.partition = cleaned_partition\n",
    "        self.subset_ids = np.array(list(cleaned_partition.keys()))\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.partition[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350b471-3cfe-4ec4-94a7-22ed8e67a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CactusTaskDataset(Dataset):\n",
    "    def __init__(self, data, partitions, n_way, n_shot, n_query, length):\n",
    "        self.data = data\n",
    "        self.partitions = partitions\n",
    "        self.n_way = n_way\n",
    "        self.n_shot = n_shot\n",
    "        self.n_query = n_query\n",
    "        self.length = length\n",
    "        self.iter = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.iter = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.__next__()\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iter == self.length:\n",
    "            raise StopIteration\n",
    "        self.iter += 1\n",
    "\n",
    "        i_partition = torch.randint(low=0, high=len(self.partitions), size=(1,), dtype=torch.int)\n",
    "        partition = self.partitions[i_partition]\n",
    "        sampled_subset_ids = np.random.choice(partition.subset_ids, size=self.n_way, replace=False)\n",
    "        xs, xq = [], []\n",
    "        for subset_id in sampled_subset_ids:\n",
    "            indices = np.random.choice(partition[subset_id], self.n_shot + self.n_query, replace=False)\n",
    "            x = self.data[indices]\n",
    "            x = x.astype(np.float32) / 255.0\n",
    "            if x.shape[1] != 1 and x.shape[1] != 3:\n",
    "                x = np.transpose(x, [0, 3, 1, 2])\n",
    "            x = torch.from_numpy(x)\n",
    "            xs.append(x[:self.n_shot])\n",
    "            xq.append(x[self.n_shot:])\n",
    "        xs = torch.stack(xs, dim=0)\n",
    "        xq = torch.stack(xq, dim=0)\n",
    "\n",
    "        return {'train': xs, 'test': xq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66ff54-4844-4a08-8ef2-c03c70f5a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_partitions_kmeans(encodings, n_way, n_shot, n_query, random_scaling=True, n_partitions=100, n_clusters=500):\n",
    "    tmp_dir = tempfile.TemporaryDirectory()\n",
    "    os.environ['JOBLIB_TEMP_FOLDER'] = tmp_dir.name  # default runs out of space for parallel processing\n",
    "    \n",
    "    encodings_list = [encodings]\n",
    "    \n",
    "    if random_scaling:\n",
    "        n_clusters_list = [n_clusters]\n",
    "        for i in range(n_partitions - 1):\n",
    "            weight_vector = np.random.uniform(low=0., high=1., size=encodings.shape[1])\n",
    "            encodings_list.append(np.multiply(encodings, weight_vector))\n",
    "            \n",
    "    else:\n",
    "        n_clusters_list = [n_clusters] * n_partitions\n",
    "    \n",
    "    assert len(encodings_list) * len(n_clusters_list) == n_partitions\n",
    "    \n",
    "    if n_partitions != 1:\n",
    "        n_init = 3\n",
    "        init = 'k-means++'\n",
    "    else:\n",
    "        n_init = 10\n",
    "        init = 'k-means++'\n",
    "    \n",
    "    print('Number of encodings: {}, number of n_clusters: {}, number of inits: '.format(len(encodings_list),\n",
    "                                                                                        len(n_clusters_list)), n_init)\n",
    "    kmeans_list = []\n",
    "    \n",
    "    for n_clusters in tqdm(n_clusters_list, desc='get_partitions_kmeans_n_clusters'):\n",
    "        for encodings in tqdm(encodings_list, desc='get_partitions_kmeans_encodings'):\n",
    "            while True:\n",
    "                kmeans = KMeans(n_clusters=n_clusters,\n",
    "                                init=init,\n",
    "                                n_init=n_init,\n",
    "                                max_iter=3000).fit(encodings)\n",
    "                uniques, counts = np.unique(kmeans.labels_, return_counts=True)\n",
    "                num_big_enough_clusters = np.sum(counts >= n_shot + n_query)\n",
    "                \n",
    "                if num_big_enough_clusters > .8 * n_clusters:\n",
    "                    break\n",
    "                else:\n",
    "                    tqdm.write(\"Too few classes ({}) with greater than {} examples.\".format(num_big_enough_clusters,\n",
    "                                                                                            n_shot + n_query))\n",
    "                    tqdm.write('Frequency: {}'.format(counts))\n",
    "            kmeans_list.append(kmeans)\n",
    "    partitions = []\n",
    "    for kmeans in kmeans_list:\n",
    "        partitions.append(Partition(labels=kmeans.labels_, n_way=n_way, n_shot=n_shot, n_query=n_query))\n",
    "    tmp_dir.cleanup()\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29f85d-c36a-48dc-b9b2-d536411ffa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class DataOpt:\n",
    "    dataset: str = 'omniglot'\n",
    "    encoder='acai'\n",
    "    test_way: int = None\n",
    "    way: int = 5\n",
    "    test_shot: int = None\n",
    "    shot: int = 1\n",
    "    test_query: int = None\n",
    "    query: int = 15\n",
    "    test_episodes: int = 100\n",
    "    test_mode:int = 'ground_truth'\n",
    "    partitions:int = 100\n",
    "    clusters:int = 500\n",
    "    train_mode: str = 'kmeans'\n",
    "    train_episodes: int = 100\n",
    "\n",
    "@dataclass\n",
    "class LoaderOpt:\n",
    "    data: DataOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0994ad-5147-4439-b1f5-e0e393027111",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DataOpt(\n",
    "    dataset='omniglot',\n",
    "    way=20,\n",
    "    shot=1,\n",
    "    test_shot=1,\n",
    "    test_way=5,\n",
    "    test_query=5,\n",
    "    query=15,\n",
    "    train_mode='kmeans',\n",
    "    train_episodes=100,\n",
    "    test_episodes=100,\n",
    "    test_mode='ground_truth',\n",
    "    partitions=1,\n",
    "    clusters=500\n",
    ")\n",
    "l = LoaderOpt(data=dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39978c-874f-4867-ad3d-1d4233e2e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load(opt:LoaderOpt, splits, data_dir):\n",
    "    encodings_dir = os.path.join(data_dir, '{}_encodings'.format(opt.data.encoder))\n",
    "    filenames = os.listdir(encodings_dir)\n",
    "    \n",
    "    ret = {}\n",
    "    \n",
    "    for split in splits:\n",
    "        if split in ['val', 'test'] and opt.data.test_way != 0:\n",
    "            n_way = opt.data.test_way\n",
    "        else:\n",
    "            n_way = opt.data.way\n",
    "\n",
    "        if split in ['val', 'test'] and opt.data.test_shot != 0:\n",
    "            n_support = opt.data.test_shot\n",
    "        else:\n",
    "            n_support = opt.data.shot\n",
    "\n",
    "        if split in ['val', 'test'] and opt.data.test_query != 0:\n",
    "            n_query = opt.data.test_query\n",
    "        else:\n",
    "            n_query = opt.data.query\n",
    "\n",
    "        if split in ['val', 'test']:\n",
    "            n_episodes = opt.data.test_episodes\n",
    "            mode = opt.data.test_mode\n",
    "        else:\n",
    "            n_episodes = opt.data.train_episodes\n",
    "            mode = opt.data.train_mode\n",
    "        \n",
    "        split_filename = [filename for filename in filenames if opt.data.dataset in filename and split in filename]\n",
    "        split_filename = os.path.join(encodings_dir, split_filename[0])\n",
    "        split_data = np.load(split_filename)\n",
    "        images = split_data['X']    # (index, H, W, C)\n",
    "        labels = split_data['Y']\n",
    "        encodings = split_data['Z']\n",
    "        \n",
    "        if mode == 'ground_truth':\n",
    "            if opt.data.dataset == 'celeba':\n",
    "#                 TODO: need to change this part\n",
    "                annotations_filename = os.path.join(DATA_DIR, 'celeba/cropped/Anno/list_attr_celeba.txt')\n",
    "                partitions = celeba_partitions(labels=labels, split=split, annotations_filename=annotations_filename, n_way=n_way, n_shot=n_support, n_query=n_query)\n",
    "            else:\n",
    "                partitions = [Partition(labels=labels, n_way=n_way, n_shot=n_support, n_query=n_query)]\n",
    "\n",
    "        elif mode == 'kmeans':\n",
    "            partitions = get_partitions_kmeans(encodings=encodings, n_way=n_way, n_shot=n_support, n_query=n_query, n_partitions=opt.data.partitions, n_clusters=opt.data.clusters)\n",
    "\n",
    "        elif mode == 'random':\n",
    "            partitions = [Partition(labels=np.random.choice(opt.data.clusters, size=labels.shape, replace=True), n_way=n_way, n_shot=n_support, n_query=n_query) for i in range(opt.data.partitions)]\n",
    "        else:\n",
    "            raise ValueError\n",
    "        ret[split] = CactusTaskDataset(data=images,\n",
    "                                partitions=partitions,\n",
    "                                n_way=n_way,\n",
    "                                n_shot=n_support,\n",
    "                                n_query=n_query,\n",
    "                                length=n_episodes)\n",
    "\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f6bb0-66d2-4643-a9e0-df7a14e0da07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of encodings: 1, number of n_clusters: 1, number of inits:  10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb05c28afe5419087c45ba924d38f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_partitions_kmeans_n_clusters:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a68c81da114d85bd58933a24d0c988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_partitions_kmeans_encodings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-57a1d7989bcd>:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  cleaned_partition[label] = np.array(partition[label], dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "r = load(l, ['train', 'val'], data_dir='data/cactus_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59736d-791d-42be-89cd-b3b5dc42b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only to write !! DONT RUN OTHERWISE\n",
    "with open('saved_op/partitions.pkl', 'wb') as of:\n",
    "    pickle.dump(r, of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f215e569-d1e5-4e14-befb-eba49bbe6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_op/partitions.pkl', 'rb') as f:\n",
    "    r = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760acc1-1d6f-4661-97cd-838f66df35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torchnet.dataset.TransformDataset(r['train'],\n",
    "                                 lambda x: {'train': x['train'],\n",
    "                                            'test': x['test']\n",
    "                                           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810df9b-4b37-4ed4-908b-9dd70f41d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(r['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a6707e-66b3-4205-b24f-6a7c69afd9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.dataset.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc9df0-5981-4634-9c76-f3a0dde02aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CactusDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 ways,\n",
    "                 shots,\n",
    "                 query,\n",
    "                 train_mode='kmeans',\n",
    "                 train_episodes=100,\n",
    "                 test_way=5,\n",
    "                 test_shot=1,\n",
    "                 test_query=5,\n",
    "                 test_mode='ground_truth',\n",
    "                 test_episodes=100,\n",
    "                 partitions=100,\n",
    "                 clusters=500,\n",
    "                 batch_size=1,\n",
    "                 use_precomputed_partitions=False,\n",
    "                 precomputed_partition_path='saved_op/partitions.pkl',\n",
    "                 dataset='omniglot',\n",
    "                 emb_data_dir='data/cactus_data/'):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_precomputed_partitions = use_precomputed_partitions\n",
    "        self.precomputed_partition_path = precomputed_partition_path\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.ways = ways\n",
    "        self.shots = shots\n",
    "        self.query_shots = query\n",
    "        self.train_mode = train_mode\n",
    "        self.train_episodes = train_episodes\n",
    "        \n",
    "        self.test_way = test_way\n",
    "        self.test_query = test_query\n",
    "        self.test_shot = test_shot\n",
    "        self.test_mode = test_mode\n",
    "        self.test_episodes = test_episodes\n",
    "        \n",
    "        self.partitions = partitions\n",
    "        self.clusters = clusters\n",
    "        \n",
    "        self.use_precomputed_partitions = use_precomputed_partitions\n",
    "        self.precomputed_partition_path= precomputed_partition_path\n",
    "        self.emb_data_dir = emb_data_dir\n",
    "        self.batch_size = batch_size\n",
    "            \n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        if not self.use_precomputed_partitions:\n",
    "            # normal running            \n",
    "            self.data_opt = DataOpt(\n",
    "                dataset=self.dataset,\n",
    "                way=self.ways,\n",
    "                shot=self.shots,\n",
    "                query=self.query_shots,\n",
    "                train_mode=self.train_mode,\n",
    "                train_episodes=self.train_episodes,\n",
    "                partitions=self.partitions,\n",
    "                clusters=self.clusters\n",
    "            )\n",
    "            \n",
    "            self.loader_opt = LoaderOpt(data=self.data_opt)\n",
    "            self.ds = load(self.loader_opt,\n",
    "                           ['train'],\n",
    "                           data_dir=self.emb_data_dir)\n",
    "            \n",
    "        else:\n",
    "            # when I don't have enough compute\n",
    "            with open(self.precomputed_partition_path, 'rb') as f:\n",
    "                self.ds = pickle.load(f)\n",
    "        \n",
    "        if 'val' in self.ds:\n",
    "            self.val_ds = self.ds['val']\n",
    "        # the batch dim is manually handled here\n",
    "        self.train_ds = self.ds['train']\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        # default batch_size = 1, batch dim is handled in setup of train_ds\n",
    "        return DataLoader(\n",
    "            self.train_ds\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_ds\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7690cc-5291-4bbe-ba23-ae6d1523988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = CactusDataModule(ways=20, shots=1, query=15, use_precomputed_partitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d8d71-217a-4706-b6f4-9d97389de3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProtoModule(encoder=CactusPrototypicalModel(in_channels=1, hidden_size=64), num_classes=20, cactus_flag=True, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae9ce0-1927-4698-b4bf-5f16c1f3a308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "        profiler='simple',\n",
    "#         max_steps=30_000,\n",
    "        max_epochs=300,\n",
    "        fast_dev_run=False,\n",
    "        gpus=1,\n",
    "        log_every_n_steps=1,\n",
    "        check_val_every_n_epoch=1,\n",
    "        flush_logs_every_n_steps=1,\n",
    "        num_sanity_val_steps=2,\n",
    "#         logger=logger\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467e42f-325e-4f63-9ef3-6dba40a00f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                    | Params\n",
      "--------------------------------------------------\n",
      "0 | model | CactusPrototypicalModel | 111 K \n",
      "--------------------------------------------------\n",
      "111 K     Trainable params\n",
      "0         Non-trainable params\n",
      "111 K     Total params\n",
      "0.448     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7353a9b07e1b42248b4af7c45d5b2ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a6a83-14a2-4a25-8ddb-a4679011bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_nn_utils.ipynb.\n",
      "Converted 01b_data_loaders_pl.ipynb.\n",
      "Converted 01c_grad_utils.ipynb.\n",
      "Converted 01d_hessian_free.ipynb.\n",
      "Converted 02_maml_pl.ipynb.\n",
      "Converted 02b_iMAML.ipynb.\n",
      "Converted 03_protonet_pl.ipynb.\n",
      "Converted 04_cactus.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b2454-c791-4953-b2cc-72b235f5f713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
