{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473cb51-04f6-46ef-801c-0b0587ec6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp cactus\n",
    "#export\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import kornia as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as tfms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchnet\n",
    "import pytorch_lightning as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchnet.dataset import ListDataset, TransformDataset\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9cca5-edcf-4617-ba5a-0a060da0dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Partition():\n",
    "    def __init__(self, labels, n_way, n_shot, n_query):\n",
    "        partition = defaultdict(list)\n",
    "        cleaned_partition = {}\n",
    "        for ind, label in enumerate(labels):\n",
    "            partition[label].append(ind)\n",
    "        for label in list(partition.keys()):\n",
    "            if len(partition[label]) >= n_shot + n_query:\n",
    "                cleaned_partition[label] = np.array(partition[label], dtype=np.int)\n",
    "        self.partition = cleaned_partition\n",
    "        self.subset_ids = np.array(list(cleaned_partition.keys()))\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.partition[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350b471-3cfe-4ec4-94a7-22ed8e67a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CactusTaskDataset(Dataset):\n",
    "    def __init__(self, data, partitions, n_way, n_shot, n_query, length):\n",
    "        self.data = data\n",
    "        self.partitions = partitions\n",
    "        self.n_way = n_way\n",
    "        self.n_shot = n_shot\n",
    "        self.n_query = n_query\n",
    "        self.length = length\n",
    "        self.iter = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.iter = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iter == self.length:\n",
    "            raise StopIteration\n",
    "        self.iter += 1\n",
    "\n",
    "        i_partition = torch.randint(low=0, high=len(self.partitions), size=(1,), dtype=torch.int)\n",
    "        partition = self.partitions[i_partition]\n",
    "        sampled_subset_ids = np.random.choice(partition.subset_ids, size=self.n_way, replace=False)\n",
    "        xs, xq = [], []\n",
    "        for subset_id in sampled_subset_ids:\n",
    "            indices = np.random.choice(partition[subset_id], self.n_shot + self.n_query, replace=False)\n",
    "            x = self.data[indices]\n",
    "            x = x.astype(np.float32) / 255.0\n",
    "            if x.shape[1] != 1 and x.shape[1] != 3:\n",
    "                x = np.transpose(x, [0, 3, 1, 2])\n",
    "            x = torch.from_numpy(x)\n",
    "            xs.append(x[:self.n_shot])\n",
    "            xq.append(x[self.n_shot:])\n",
    "        xs = torch.stack(xs, dim=0)\n",
    "        xq = torch.stack(xq, dim=0)\n",
    "\n",
    "        return {'train': xs, 'test': xq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66ff54-4844-4a08-8ef2-c03c70f5a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_partitions_kmeans(encodings, n_way, n_shot, n_query, random_scaling=True, n_partitions=100, n_clusters=500):\n",
    "    tmp_dir = tempfile.TemporaryDirectory()\n",
    "    os.environ['JOBLIB_TEMP_FOLDER'] = tmp_dir.name  # default runs out of space for parallel processing\n",
    "    \n",
    "    encodings_list = [encodings]\n",
    "    \n",
    "    if random_scaling:\n",
    "        n_clusters_list = [n_clusters]\n",
    "        for i in range(n_partitions - 1):\n",
    "            weight_vector = np.random.uniform(low=0., high=1., size=encodings.shape[1])\n",
    "            encodings_list.append(np.multiply(encodings, weight_vector))\n",
    "            \n",
    "    else:\n",
    "        n_clusters_list = [n_clusters] * n_partitions\n",
    "    \n",
    "    assert len(encodings_list) * len(n_clusters_list) == n_partitions\n",
    "    \n",
    "    if n_partitions != 1:\n",
    "        n_init = 3\n",
    "        init = 'k-means++'\n",
    "    else:\n",
    "        n_init = 10\n",
    "        init = 'k-means++'\n",
    "    \n",
    "    print('Number of encodings: {}, number of n_clusters: {}, number of inits: '.format(len(encodings_list),\n",
    "                                                                                        len(n_clusters_list)), n_init)\n",
    "    kmeans_list = []\n",
    "    \n",
    "    for n_clusters in tqdm(n_clusters_list, desc='get_partitions_kmeans_n_clusters'):\n",
    "        for encodings in tqdm(encodings_list, desc='get_partitions_kmeans_encodings'):\n",
    "            while True:\n",
    "                kmeans = KMeans(n_clusters=n_clusters,\n",
    "                                init=init,\n",
    "                                n_init=n_init,\n",
    "                                max_iter=3000).fit(encodings)\n",
    "                uniques, counts = np.unique(kmeans.labels_, return_counts=True)\n",
    "                num_big_enough_clusters = np.sum(counts >= n_shot + n_query)\n",
    "                \n",
    "                if num_big_enough_clusters > .8 * n_clusters:\n",
    "                    break\n",
    "                else:\n",
    "                    tqdm.write(\"Too few classes ({}) with greater than {} examples.\".format(num_big_enough_clusters,\n",
    "                                                                                            n_shot + n_query))\n",
    "                    tqdm.write('Frequency: {}'.format(counts))\n",
    "            kmeans_list.append(kmeans)\n",
    "    partitions = []\n",
    "    for kmeans in kmeans_list:\n",
    "        partitions.append(Partition(labels=kmeans.labels_, n_way=n_way, n_shot=n_shot, n_query=n_query))\n",
    "    tmp_dir.cleanup()\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29f85d-c36a-48dc-b9b2-d536411ffa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class DataOpt:\n",
    "    dataset: str = 'omniglot'\n",
    "    encoder='bigan'\n",
    "    test_way: int = None\n",
    "    way: int = 5\n",
    "    test_shot: int = None\n",
    "    shot: int = 1\n",
    "    test_query: int = None\n",
    "    query: int = 15\n",
    "    test_episodes: int = None\n",
    "    test_mode:int = None\n",
    "    partitions:int = 100\n",
    "    clusters:int = 500\n",
    "    train_mode: str = 'kmeans'\n",
    "    train_episodes: int = 100\n",
    "\n",
    "@dataclass\n",
    "class LoaderOpt:\n",
    "    data: DataOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0994ad-5147-4439-b1f5-e0e393027111",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DataOpt(\n",
    "    dataset='omniglot',\n",
    "    way=5,\n",
    "    shot=1,\n",
    "    query=15,\n",
    "    train_mode='kmeans',\n",
    "    train_episodes=100,\n",
    "    partitions=1,\n",
    "    clusters=500\n",
    ")\n",
    "l = LoaderOpt(data=dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39978c-874f-4867-ad3d-1d4233e2e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load(opt:LoaderOpt, splits, data_dir):\n",
    "    encodings_dir = os.path.join(data_dir, '{}_encodings'.format(opt.data.encoder))\n",
    "    filenames = os.listdir(encodings_dir)\n",
    "    \n",
    "    ret = {}\n",
    "    \n",
    "    for split in splits:\n",
    "        if split in ['val', 'test'] and opt.data.test_way != 0:\n",
    "            n_way = opt.data.test_way\n",
    "        else:\n",
    "            n_way = opt.data.way\n",
    "\n",
    "        if split in ['val', 'test'] and opt.data.test_shot != 0:\n",
    "            n_support = opt.data.test_shot\n",
    "        else:\n",
    "            n_support = opt.data.shot\n",
    "\n",
    "        if split in ['val', 'test'] and opt.data.test_query != 0:\n",
    "            n_query = opt.data.test_query\n",
    "        else:\n",
    "            n_query = opt.data.query\n",
    "\n",
    "        if split in ['val', 'test']:\n",
    "            n_episodes = opt.data.test_episodes\n",
    "            mode = opt.data.test_mode\n",
    "        else:\n",
    "            n_episodes = opt.data.train_episodes\n",
    "            mode = opt.data.train_mode\n",
    "        \n",
    "        split_filename = [filename for filename in filenames if opt.data.dataset in filename and split in filename]\n",
    "        split_filename = os.path.join(encodings_dir, split_filename[0])\n",
    "        split_data = np.load(split_filename)\n",
    "        images = split_data['X']    # (index, H, W, C)\n",
    "        labels = split_data['Y']\n",
    "        encodings = split_data['Z']\n",
    "        \n",
    "        if mode == 'ground_truth':\n",
    "            if opt.data.dataset == 'celeba':\n",
    "#                 TODO: need to change this part\n",
    "                annotations_filename = os.path.join(DATA_DIR, 'celeba/cropped/Anno/list_attr_celeba.txt')\n",
    "                partitions = celeba_partitions(labels=labels, split=split, annotations_filename=annotations_filename, n_way=n_way, n_shot=n_support, n_query=n_query)\n",
    "            else:\n",
    "                partitions = [Partition(labels=labels, n_way=n_way, n_shot=n_support, n_query=n_query)]\n",
    "\n",
    "        elif mode == 'kmeans':\n",
    "            partitions = get_partitions_kmeans(encodings=encodings, n_way=n_way, n_shot=n_support, n_query=n_query, n_partitions=opt.data.partitions, n_clusters=opt.data.clusters)\n",
    "\n",
    "        elif mode == 'random':\n",
    "            partitions = [Partition(labels=np.random.choice(opt.data.clusters, size=labels.shape, replace=True), n_way=n_way, n_shot=n_support, n_query=n_query) for i in range(opt.data.partitions)]\n",
    "        else:\n",
    "            raise ValueError\n",
    "        ret[split] = CactusTaskDataset(data=images,\n",
    "                                partitions=partitions,\n",
    "                                n_way=n_way,\n",
    "                                n_shot=n_support,\n",
    "                                n_query=n_query,\n",
    "                                length=n_episodes)\n",
    "\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f6bb0-66d2-4643-a9e0-df7a14e0da07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of encodings: 1, number of n_clusters: 1, number of inits:  10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6596ce1f0c454fcd91b91ab20d65fdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_partitions_kmeans_n_clusters:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafd373c291f4ac09de57be0bce8c2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_partitions_kmeans_encodings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-57a1d7989bcd>:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  cleaned_partition[label] = np.array(partition[label], dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "r = load(l, ['train'], data_dir='data/cactus_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc9df0-5981-4634-9c76-f3a0dde02aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CactusDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, use_precomputed=False, precomputed_partition_path=None, dataset='omniglot'):\n",
    "        self.use_precomputed = use_precomputed\n",
    "        self.precomputed_partition_path = precomputed_partition_path\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def setup(self):\n",
    "        pass\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        pass\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        pass\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a6a83-14a2-4a25-8ddb-a4679011bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_nn_utils.ipynb.\n",
      "Converted 01b_data_loaders_pl.ipynb.\n",
      "Converted 01c_grad_utils.ipynb.\n",
      "Converted 01d_hessian_free.ipynb.\n",
      "Converted 02_maml_pl.ipynb.\n",
      "Converted 02b_iMAML.ipynb.\n",
      "Converted 03_protonet_pl.ipynb.\n",
      "Converted 04_cactus.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b2454-c791-4953-b2cc-72b235f5f713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
