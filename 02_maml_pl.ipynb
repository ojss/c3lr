{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bacb13-3bbd-4e17-bbe3-ca0520aaad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp maml\n",
    "#export\n",
    "import logging\n",
    "import warnings \n",
    "\n",
    "import higher\n",
    "import kornia as K\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "from torchmeta.datasets.helpers import omniglot\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "from unsupervised_meta_learning.pl_dataloaders import OmniglotDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7bfc6-5fb5-4825-90fb-9429697c5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be03bb5-a108-4e93-a3be-c0001c8c6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b3d49-5aff-46c8-aed3-e37412b35106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, out_features, hidden_size=64):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            self.conv3x3(in_channels, hidden_size),\n",
    "            self.conv3x3(hidden_size, hidden_size),\n",
    "            self.conv3x3(hidden_size, hidden_size),\n",
    "            self.conv3x3(hidden_size, hidden_size),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, out_features)\n",
    "\n",
    "    def forward(self, inputs, params=None):\n",
    "        features = self.features(inputs)\n",
    "        features = features.view((features.size(0), -1))\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "    def conv3x3(self, in_channels, out_channels, **kwargs):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels, momentum=1.0, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c62b6-25f8-4f8d-bfa6-8b1049d3287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_accuracy(logits, targets):\n",
    "    \"\"\"Compute the accuracy (after adaptation) of MAML on the test/query points\n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : `torch.FloatTensor` instance\n",
    "        Outputs/logits of the model on the query points. This tensor has shape\n",
    "        `(num_examples, num_classes)`.\n",
    "    targets : `torch.LongTensor` instance\n",
    "        A tensor containing the targets of the query points. This tensor has \n",
    "        shape `(num_examples,)`.\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : `torch.FloatTensor` instance\n",
    "        Mean accuracy on the query points\n",
    "    \"\"\"\n",
    "    _, predictions = torch.max(logits, dim=-1)\n",
    "    return torch.mean(predictions.eq(targets).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf69734-98e0-4768-9f8f-8100bfd31382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MAML(pl.LightningModule):\n",
    "    def __init__(self, model, inner_steps=1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.accuracy = get_accuracy\n",
    "        self.automatic_optimization = False\n",
    "        self.inner_steps = inner_steps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def inner_loop(self, fmodel, diffopt, train_input, train_target):\n",
    "        train_logit = fmodel(train_input)\n",
    "        inner_loss = F.cross_entropy(train_logit, train_target)\n",
    "        diffopt.step(inner_loss)\n",
    "        \n",
    "        return inner_loss.item()\n",
    "    \n",
    "    @torch.enable_grad()\n",
    "    def meta_learn(self, batch, batch_idx, optimizer_idx=None):\n",
    "        meta_optimizer, inner_optimizer = self.optimizers()\n",
    "        meta_optimizer = meta_optimizer.optimizer\n",
    "        inner_optimizer = inner_optimizer.optimizer\n",
    "        \n",
    "        train_inputs, train_targets = batch['train']\n",
    "        test_inputs, test_targets = batch['test']\n",
    "        \n",
    "        batch_size = train_inputs.shape[0]\n",
    "        outer_loss = torch.tensor(0., device=self.device)\n",
    "        acc = torch.tensor(0., device=self.device)\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        for task_idx, (train_input, train_target, test_input, test_target) in enumerate(\n",
    "            zip(train_inputs, train_targets, test_inputs, test_targets)\n",
    "        ):\n",
    "#             inner_optimizer.zero_grad()\n",
    "            with higher.innerloop_ctx(self.model, inner_optimizer, copy_initial_weights=False) as (fmodel, diffopt):\n",
    "#                 train_logit = fmodel(train_input)\n",
    "#                 inner_loss = F.cross_entropy(train_logit, train_target)\n",
    "\n",
    "#                 diffopt.step(inner_loss)\n",
    "                for step in range(self.inner_steps):\n",
    "                    self.inner_loop(fmodel, diffopt, train_input, train_target)\n",
    "            \n",
    "                test_logit = fmodel(test_input)\n",
    "                outer_loss += F.cross_entropy(test_logit, test_target)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    preds = test_logit.softmax(dim=-1)\n",
    "                    acc += self.accuracy(test_logit, test_target)\n",
    "                \n",
    "\n",
    "#                     self.print(self.accuracy(test_logit, test_target))\n",
    "                \n",
    "        outer_loss.div_(batch_size)\n",
    "        acc.div_(batch_size)\n",
    "        self.log_dict({\n",
    "                    'outer_loss': outer_loss,\n",
    "                    'accuracy': acc\n",
    "                }, prog_bar=True)\n",
    "        \n",
    "        meta_optimizer.zero_grad()\n",
    "#         outer_loss.backward()\n",
    "        self.manual_backward(outer_loss, meta_optimizer)\n",
    "        meta_optimizer.step()\n",
    "        return outer_loss, acc\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        train_loss, acc = self.meta_learn(batch, batch_idx, optimizer_idx)\n",
    "        \n",
    "        self.log_dict({\n",
    "            'train_loss': train_loss.item(),\n",
    "            'train_accuracy': acc.item()\n",
    "        }, prog_bar=True)\n",
    "            \n",
    "        return train_loss.item()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        val_loss, val_acc = self.meta_learn(batch, batch_idx)\n",
    "        \n",
    "        self.log_dict({\n",
    "            'val_loss': val_loss.item(),\n",
    "            'val_accuracy': val_acc.item()\n",
    "        })\n",
    "        return val_loss.item()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss, test_acc = self.meta_learn(batch, batch_idx)\n",
    "        self.log_dict({\n",
    "            'test_loss': test_loss.item(),\n",
    "            'test_accuracy': test_acc.item()\n",
    "        })\n",
    "        return test_loss.item()\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        meta_optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        inner_optimizer = torch.optim.SGD(self.parameters(), lr=1e-1)\n",
    "        \n",
    "        return [meta_optimizer, inner_optimizer]\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7a973-d7cf-4a19-892a-e56d211c103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UMTRA(pl.LightningModule):\n",
    "    def __init__(self, model, augmentation, inner_steps):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.accuracy = get_accuracy\n",
    "        self.augmentation = augmentation\n",
    "        self.inner_steps = inner_steps\n",
    "        self.automatic_optimization = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def inner_loop(self, fmodel, diffopt, train_input, train_target):\n",
    "        train_logit = fmodel(train_input)\n",
    "        inner_loss = F.cross_entropy(train_logit, train_target)\n",
    "        diffopt.step(inner_loss)\n",
    "        \n",
    "        return inner_loss.item()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        meta_optimizer, inner_optimizer = self.optimizers(use_pl_optimizer=False)\n",
    "        train_inputs, train_targets = batch['train']\n",
    "        test_inputs, test_targets = batch['test']\n",
    "        \n",
    "        batch_size = train_inputs.shape[0]\n",
    "        outer_loss = torch.tensor(0., device=self.device)\n",
    "        acc = torch.tensor(0., device=self.device)\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        for task_idx, (train_input, train_target, test_input, test_target) in enumerate(\n",
    "            zip(train_inputs, train_targets, test_inputs, test_targets)\n",
    "        ):\n",
    "            val_input = self.augmentation(train_input).to(self.device)\n",
    "            val_target = deepcopy(train_target).to(self.device)\n",
    "            with higher.innerloop_ctx(self.model, inner_optimizer, copy_initial_weights=False) as (fmodel, diffopt):\n",
    "                for step in range(self.inner_steps):\n",
    "                    self.inner_loop(fmodel, diffopt, train_input, train_target)\n",
    "                \n",
    "                val_logits = fmodel(val_input)\n",
    "                outer_loss += F.cross_entropy(val_logits, val_target)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    test_logits = fmodel(test_input)\n",
    "                    acc += self.accuracy(test_logits, test_target)\n",
    "                \n",
    "        outer_loss.div_(batch_size)\n",
    "        acc.div_(batch_size)\n",
    "        self.log_dict({\n",
    "                    'outer_loss': outer_loss,\n",
    "                    'accuracy': acc\n",
    "                }, prog_bar=True)\n",
    "        \n",
    "        meta_optimizer.zero_grad()\n",
    "#         outer_loss.backward()\n",
    "\n",
    "        self.manual_backward(outer_loss, meta_optimizer)\n",
    "        meta_optimizer.step()\n",
    "        \n",
    "        return outer_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        meta_optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        inner_optimizer = torch.optim.SGD(self.parameters(), lr=1e-1)\n",
    "        \n",
    "        return [meta_optimizer, inner_optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7e713-faf8-4145-b8fb-e995a7cf487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = OmniglotDataModule(\n",
    "        \"data\",\n",
    "        shots=1,\n",
    "        ways=5,\n",
    "        shuffle_ds=True,\n",
    "        test_shots=15,\n",
    "        meta_train=True,\n",
    "        download=True,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067af73-1a84-4498-bb44-da7a88e93068",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAML(model=ConvolutionalNeuralNetwork(1, 5, hidden_size=64), inner_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919419b4-02a7-4119-a180-89161a124fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "logger = WandbLogger(\n",
    "    project='maml',\n",
    "    config={\n",
    "        'batch_size': 16,\n",
    "        'steps': 100,\n",
    "        'dataset': \"omniglot\",\n",
    "        'inner_steps': 1,\n",
    "        'val/test': 'enabled'\n",
    "    }\n",
    ")\n",
    "trainer = Trainer(\n",
    "        profiler='simple',\n",
    "        max_epochs=100,\n",
    "        max_steps=100,\n",
    "        limit_train_batches=100,\n",
    "        limit_val_batches=0.,\n",
    "        limit_test_batches=2,\n",
    "        fast_dev_run=False,\n",
    "        gpus=1,\n",
    "        log_every_n_steps=1,\n",
    "        flush_logs_every_n_steps=1,\n",
    "        num_sanity_val_steps=2,\n",
    "        logger=logger\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d445dfb-84ea-44c3-bd11-75516b58df96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">lyric-dust-16</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/p0int/maml\" target=\"_blank\">https://wandb.ai/p0int/maml</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/p0int/maml/runs/2eirmov8\" target=\"_blank\">https://wandb.ai/p0int/maml/runs/2eirmov8</a><br/>\n",
       "                Run data is saved locally in <code>/home/ojass/Projects/meta-learning/wandb/run-20210612_151517-2eirmov8</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | ConvolutionalNeuralNetwork | 112 K \n",
      "-----------------------------------------------------\n",
      "112 K     Trainable params\n",
      "0         Non-trainable params\n",
      "112 K     Total params\n",
      "0.449     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a2979101d745c5861d07678ac56e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  146.04         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  139.97         \t|1              \t|  139.97         \t|  95.844         \t|\n",
      "run_training_batch                 \t|  1.3831         \t|100            \t|  138.31         \t|  94.71          \t|\n",
      "model_forward                      \t|  1.3829         \t|100            \t|  138.29         \t|  94.697         \t|\n",
      "training_step                      \t|  1.3826         \t|100            \t|  138.26         \t|  94.676         \t|\n",
      "get_train_batch                    \t|  0.010267       \t|100            \t|  1.0267         \t|  0.70304        \t|\n",
      "on_train_batch_end                 \t|  0.0032118      \t|100            \t|  0.32118        \t|  0.21993        \t|\n",
      "on_train_start                     \t|  0.018699       \t|1              \t|  0.018699       \t|  0.012804       \t|\n",
      "cache_result                       \t|  3.5012e-05     \t|409            \t|  0.01432        \t|  0.0098056      \t|\n",
      "on_train_epoch_start               \t|  0.0024913      \t|1              \t|  0.0024913      \t|  0.0017059      \t|\n",
      "on_batch_start                     \t|  1.9688e-05     \t|100            \t|  0.0019688      \t|  0.0013482      \t|\n",
      "on_train_batch_start               \t|  1.1111e-05     \t|100            \t|  0.0011111      \t|  0.00076083     \t|\n",
      "training_step_end                  \t|  1.0715e-05     \t|100            \t|  0.0010715      \t|  0.00073372     \t|\n",
      "on_batch_end                       \t|  1.0178e-05     \t|100            \t|  0.0010178      \t|  0.00069694     \t|\n",
      "on_train_end                       \t|  0.00055686     \t|1              \t|  0.00055686     \t|  0.00038131     \t|\n",
      "on_train_epoch_end                 \t|  0.00012557     \t|1              \t|  0.00012557     \t|  8.5985e-05     \t|\n",
      "on_before_accelerator_backend_setup\t|  4.5516e-05     \t|1              \t|  4.5516e-05     \t|  3.1167e-05     \t|\n",
      "on_epoch_start                     \t|  3.5225e-05     \t|1              \t|  3.5225e-05     \t|  2.4121e-05     \t|\n",
      "on_fit_start                       \t|  3.0747e-05     \t|1              \t|  3.0747e-05     \t|  2.1054e-05     \t|\n",
      "on_train_dataloader                \t|  9.078e-06      \t|1              \t|  9.078e-06      \t|  6.2162e-06     \t|\n",
      "on_epoch_end                       \t|  7.773e-06      \t|1              \t|  7.773e-06      \t|  5.3226e-06     \t|\n",
      "on_val_dataloader                  \t|  5.653e-06      \t|1              \t|  5.653e-06      \t|  3.8709e-06     \t|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a42450-d257-4a0c-8052-e973f6ab9bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 65353<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/ojass/Projects/meta-learning/wandb/run-20210612_151517-2eirmov8/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/ojass/Projects/meta-learning/wandb/run-20210612_151517-2eirmov8/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>outer_loss</td><td>0.24109</td></tr><tr><td>accuracy</td><td>0.94083</td></tr><tr><td>train_loss</td><td>0.24109</td></tr><tr><td>train_accuracy</td><td>0.94083</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>trainer/global_step</td><td>99</td></tr><tr><td>_runtime</td><td>143</td></tr><tr><td>_timestamp</td><td>1623503860</td></tr><tr><td>_step</td><td>99</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>outer_loss</td><td>██▇▆▅▅▄▄▅▅▃▃▄▄▃▃▃▃▂▂▂▃▂▂▂▂▁▂▄▂▃▂▁▂▂▁▁▁▂▁</td></tr><tr><td>accuracy</td><td>▁▃▃▄▄▅▆▅▅▅▆▆▅▆▇▆▆▆█▇▇▆▆▇▇██▇▅█▇██▇▇███▇█</td></tr><tr><td>train_loss</td><td>██▇▆▅▅▄▄▅▅▃▃▄▄▃▃▃▃▂▂▂▃▂▂▂▂▁▂▄▂▃▂▁▂▂▁▁▁▂▁</td></tr><tr><td>train_accuracy</td><td>▁▃▃▄▄▅▆▅▅▅▆▆▅▆▇▆▆▆█▇▇▆▆▇▇██▇▅█▇██▇▇███▇█</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">lyric-dust-16</strong>: <a href=\"https://wandb.ai/p0int/maml/runs/2eirmov8\" target=\"_blank\">https://wandb.ai/p0int/maml/runs/2eirmov8</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61159219-fa62-4ef1-845a-1a22521394b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = nn.Sequential(\n",
    "    K.augmentation.RandomAffine(degrees=0, translate=(0.4, 0.4), padding_mode='border'),\n",
    "    K.augmentation.RandomGaussianNoise(mean=0., std=.1, p=.3)\n",
    ")\n",
    "model = UMTRA(model=ConvolutionalNeuralNetwork(1, 5, hidden_size=64), augmentation=aug, inner_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d75b5e-5fea-4699-88a8-ef6af51b8583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "logger = WandbLogger(\n",
    "    project='umtra',\n",
    "    config={\n",
    "        'batch_size': 16,\n",
    "        'steps': 100,\n",
    "        'dataset': \"omniglot\",\n",
    "        'inner_steps': 5\n",
    "    }\n",
    ")\n",
    "trainer = Trainer(\n",
    "        profiler='simple',\n",
    "        max_epochs=100,\n",
    "        max_steps=100,\n",
    "        limit_train_batches=50,\n",
    "        limit_val_batches=0.,\n",
    "        limit_test_batches=2,\n",
    "        fast_dev_run=False,\n",
    "        gpus=1,\n",
    "        log_every_n_steps=1,\n",
    "        flush_logs_every_n_steps=1,\n",
    "        num_sanity_val_steps=2,\n",
    "        logger=logger\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68b428-ce6c-4c94-90cf-8beeb08b88ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">celestial-gorge-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/p0int/umtra\" target=\"_blank\">https://wandb.ai/p0int/umtra</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/p0int/umtra/runs/29z11iiu\" target=\"_blank\">https://wandb.ai/p0int/umtra/runs/29z11iiu</a><br/>\n",
       "                Run data is saved locally in <code>/home/ojass/Projects/meta-learning/wandb/run-20210612_151113-29z11iiu</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                       | Params\n",
      "------------------------------------------------------------\n",
      "0 | model        | ConvolutionalNeuralNetwork | 112 K \n",
      "1 | augmentation | Sequential                 | 0     \n",
      "------------------------------------------------------------\n",
      "112 K     Trainable params\n",
      "0         Non-trainable params\n",
      "112 K     Total params\n",
      "0.449     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739df8828f464a158ba03d7bbdd53f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  89.389         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  41.453         \t|2              \t|  82.906         \t|  92.747         \t|\n",
      "run_training_batch                 \t|  0.80074        \t|100            \t|  80.074         \t|  89.579         \t|\n",
      "model_forward                      \t|  0.80059        \t|100            \t|  80.059         \t|  89.562         \t|\n",
      "training_step                      \t|  0.80026        \t|100            \t|  80.026         \t|  89.526         \t|\n",
      "get_train_batch                    \t|  0.019348       \t|100            \t|  1.9348         \t|  2.1645         \t|\n",
      "on_train_batch_end                 \t|  0.0023014      \t|100            \t|  0.23014        \t|  0.25746        \t|\n",
      "on_train_start                     \t|  0.020492       \t|1              \t|  0.020492       \t|  0.022925       \t|\n",
      "cache_result                       \t|  2.0442e-05     \t|412            \t|  0.008422       \t|  0.0094217      \t|\n",
      "on_train_epoch_start               \t|  0.0014264      \t|2              \t|  0.0028527      \t|  0.0031913      \t|\n",
      "on_batch_start                     \t|  2.2554e-05     \t|100            \t|  0.0022554      \t|  0.0025231      \t|\n",
      "on_train_batch_start               \t|  1.1124e-05     \t|100            \t|  0.0011124      \t|  0.0012444      \t|\n",
      "training_step_end                  \t|  1.0794e-05     \t|100            \t|  0.0010794      \t|  0.0012075      \t|\n",
      "on_batch_end                       \t|  1.064e-05      \t|100            \t|  0.001064       \t|  0.0011903      \t|\n",
      "on_train_end                       \t|  0.00055112     \t|1              \t|  0.00055112     \t|  0.00061654     \t|\n",
      "on_train_epoch_end                 \t|  0.00010181     \t|2              \t|  0.00020361     \t|  0.00022778     \t|\n",
      "on_fit_start                       \t|  6.4703e-05     \t|1              \t|  6.4703e-05     \t|  7.2383e-05     \t|\n",
      "on_epoch_start                     \t|  2.0287e-05     \t|2              \t|  4.0575e-05     \t|  4.5391e-05     \t|\n",
      "on_before_accelerator_backend_setup\t|  2.2075e-05     \t|1              \t|  2.2075e-05     \t|  2.4695e-05     \t|\n",
      "on_epoch_end                       \t|  8.879e-06      \t|2              \t|  1.7758e-05     \t|  1.9866e-05     \t|\n",
      "on_train_dataloader                \t|  1.1228e-05     \t|1              \t|  1.1228e-05     \t|  1.2561e-05     \t|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2192dceb-338b-4bfe-a05b-b37a6378d525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 64463<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/ojass/Projects/meta-learning/wandb/run-20210612_151113-29z11iiu/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/ojass/Projects/meta-learning/wandb/run-20210612_151113-29z11iiu/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>outer_loss</td><td>0.24974</td></tr><tr><td>accuracy</td><td>0.9125</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>trainer/global_step</td><td>99</td></tr><tr><td>_runtime</td><td>87</td></tr><tr><td>_timestamp</td><td>1623503560</td></tr><tr><td>_step</td><td>99</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>outer_loss</td><td>█▆▇▆▅▆▆▅▄▅▂▅▄▄▄▃▃▃▃▂▂▂▃▁▂▃▃▂▂▃▂▂▁▂▁▁▁▂▁▂</td></tr><tr><td>accuracy</td><td>▁▂▂▃▃▂▄▃▅▃▅▅▅▆▆▅▆▆▅▆█▅▇█▆█▇▇▆▇▅▇▅▆█▇▇▆▆█</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">celestial-gorge-4</strong>: <a href=\"https://wandb.ai/p0int/umtra/runs/29z11iiu\" target=\"_blank\">https://wandb.ai/p0int/umtra/runs/29z11iiu</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae0ad69-fbc6-46dd-b849-11158faa876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_nn_utils.ipynb.\n",
      "Converted 01b_data_loaders_pl.ipynb.\n",
      "Converted 02_maml.ipynb.\n",
      "Converted 02b_maml_pl.ipynb.\n",
      "Converted 03_protonet_pl.ipynb.\n",
      "Converted 04_cactus.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9caad-734d-41fb-8aa5-189f0a3f67bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
