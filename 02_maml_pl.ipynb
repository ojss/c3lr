{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bacb13-3bbd-4e17-bbe3-ca0520aaad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp maml\n",
    "#export\n",
    "import logging\n",
    "import warnings \n",
    "\n",
    "import higher\n",
    "import kornia as K\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "from torchmeta.datasets.helpers import omniglot\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "from unsupervised_meta_learning.pl_dataloaders import OmniglotDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7bfc6-5fb5-4825-90fb-9429697c5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be03bb5-a108-4e93-a3be-c0001c8c6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b3d49-5aff-46c8-aed3-e37412b35106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, out_features, hidden_size=64):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            self.conv3x3(in_channels, hidden_size),\n",
    "            self.conv3x3(hidden_size, hidden_size),\n",
    "            self.conv3x3(hidden_size, hidden_size),\n",
    "            self.conv3x3(hidden_size, hidden_size),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, out_features)\n",
    "\n",
    "    def forward(self, inputs, params=None):\n",
    "        features = self.features(inputs)\n",
    "        features = features.view((features.size(0), -1))\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "    def conv3x3(self, in_channels, out_channels, **kwargs):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels, momentum=1.0, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c62b6-25f8-4f8d-bfa6-8b1049d3287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_accuracy(logits, targets):\n",
    "    \"\"\"Compute the accuracy (after adaptation) of MAML on the test/query points\n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : `torch.FloatTensor` instance\n",
    "        Outputs/logits of the model on the query points. This tensor has shape\n",
    "        `(num_examples, num_classes)`.\n",
    "    targets : `torch.LongTensor` instance\n",
    "        A tensor containing the targets of the query points. This tensor has \n",
    "        shape `(num_examples,)`.\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : `torch.FloatTensor` instance\n",
    "        Mean accuracy on the query points\n",
    "    \"\"\"\n",
    "    _, predictions = torch.max(logits, dim=-1)\n",
    "    return torch.mean(predictions.eq(targets).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf69734-98e0-4768-9f8f-8100bfd31382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MAML(pl.LightningModule):\n",
    "    def __init__(self, model, inner_steps=1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.accuracy = get_accuracy\n",
    "        self.automatic_optimization = False\n",
    "        self.inner_steps = inner_steps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def inner_loop(self, fmodel, diffopt, train_input, train_target):\n",
    "        train_logit = fmodel(train_input)\n",
    "        inner_loss = F.cross_entropy(train_logit, train_target)\n",
    "        diffopt.step(inner_loss)\n",
    "        \n",
    "        return inner_loss.item()\n",
    "    \n",
    "    @torch.enable_grad()\n",
    "    def meta_learn(self, batch, batch_idx, optimizer_idx=None):\n",
    "        meta_optimizer, inner_optimizer = self.optimizers()\n",
    "        meta_optimizer = meta_optimizer.optimizer\n",
    "        inner_optimizer = inner_optimizer.optimizer\n",
    "        \n",
    "        train_inputs, train_targets = batch['train']\n",
    "        test_inputs, test_targets = batch['test']\n",
    "        \n",
    "        batch_size = train_inputs.shape[0]\n",
    "        outer_loss = torch.tensor(0., device=self.device)\n",
    "        acc = torch.tensor(0., device=self.device)\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        for task_idx, (train_input, train_target, test_input, test_target) in enumerate(\n",
    "            zip(train_inputs, train_targets, test_inputs, test_targets)\n",
    "        ):\n",
    "#             inner_optimizer.zero_grad()\n",
    "            with higher.innerloop_ctx(self.model, inner_optimizer, copy_initial_weights=False) as (fmodel, diffopt):\n",
    "#                 train_logit = fmodel(train_input)\n",
    "#                 inner_loss = F.cross_entropy(train_logit, train_target)\n",
    "\n",
    "#                 diffopt.step(inner_loss)\n",
    "                for step in range(self.inner_steps):\n",
    "                    self.inner_loop(fmodel, diffopt, train_input, train_target)\n",
    "            \n",
    "                test_logit = fmodel(test_input)\n",
    "                outer_loss += F.cross_entropy(test_logit, test_target)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    preds = test_logit.softmax(dim=-1)\n",
    "                    acc += self.accuracy(test_logit, test_target)\n",
    "                \n",
    "\n",
    "#                     self.print(self.accuracy(test_logit, test_target))\n",
    "                \n",
    "        outer_loss.div_(batch_size)\n",
    "        acc.div_(batch_size)\n",
    "        self.log_dict({\n",
    "                    'outer_loss': outer_loss,\n",
    "                    'accuracy': acc\n",
    "                }, prog_bar=True)\n",
    "        \n",
    "        meta_optimizer.zero_grad()\n",
    "#         outer_loss.backward()\n",
    "        self.manual_backward(outer_loss, meta_optimizer)\n",
    "        meta_optimizer.step()\n",
    "        return outer_loss, acc\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        train_loss, acc = self.meta_learn(batch, batch_idx, optimizer_idx)\n",
    "        \n",
    "        self.log_dict({\n",
    "            'train_loss': train_loss.item(),\n",
    "            'train_accuracy': acc.item()\n",
    "        }, prog_bar=True)\n",
    "            \n",
    "        return train_loss.item()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        val_loss, val_acc = self.meta_learn(batch, batch_idx)\n",
    "        \n",
    "        self.log_dict({\n",
    "            'val_loss': val_loss.item(),\n",
    "            'val_accuracy': val_acc.item()\n",
    "        })\n",
    "        return val_loss.item()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss, test_acc = self.meta_learn(batch, batch_idx)\n",
    "        self.log_dict({\n",
    "            'test_loss': test_loss.item(),\n",
    "            'test_accuracy': test_acc.item()\n",
    "        })\n",
    "        return test_loss.item()\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        meta_optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        inner_optimizer = torch.optim.SGD(self.parameters(), lr=1e-1)\n",
    "        \n",
    "        return [meta_optimizer, inner_optimizer]\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7a973-d7cf-4a19-892a-e56d211c103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UMTRA(pl.LightningModule):\n",
    "    def __init__(self, model, augmentation):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.accuracy = get_accuracy\n",
    "        self.augmentation = augmentation\n",
    "        self.automatic_optimization = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        meta_optimizer, inner_optimizer = self.optimizers()\n",
    "        meta_optimizer = meta_optimizer.optimizer\n",
    "        inner_optimizer = inner_optimizer.optimizer\n",
    "        \n",
    "        train_inputs, train_targets = batch['train']\n",
    "        test_inputs, test_targets = batch['test']\n",
    "        \n",
    "        batch_size = train_inputs.shape[0]\n",
    "        outer_loss = torch.tensor(0., device=self.device)\n",
    "        acc = torch.tensor(0., device=self.device)\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        for task_idx, (train_input, train_target, test_input, test_target) in enumerate(\n",
    "            zip(train_inputs, train_targets, test_inputs, test_targets)\n",
    "        ):\n",
    "#             inner_optimizer.zero_grad()\n",
    "            val_input = self.augmentation(train_input).to(self.device)\n",
    "            val_target = deepcopy(train_target).to(self.device)\n",
    "            with higher.innerloop_ctx(self.model, inner_optimizer, copy_initial_weights=False) as (fmodel, diffopt):\n",
    "                train_logit = fmodel(train_input)\n",
    "                inner_loss = F.cross_entropy(train_logit, train_target)\n",
    "                \n",
    "                diffopt.step(inner_loss)\n",
    "                \n",
    "                val_logits = fmodel(val_input)\n",
    "                outer_loss += F.cross_entropy(val_logits, val_target)\n",
    "#                 test_logit = fmodel(test_input)\n",
    "#                 outer_loss += F.cross_entropy(test_logit, test_target)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    test_logits = fmodel(test_input)\n",
    "                    acc += self.accuracy(test_logits, test_target)\n",
    "                \n",
    "\n",
    "#                     self.print(self.accuracy(test_logit, test_target))\n",
    "                \n",
    "        outer_loss.div_(batch_size)\n",
    "        acc.div_(batch_size)\n",
    "        self.log_dict({\n",
    "                    'outer_loss': outer_loss,\n",
    "                    'accuracy': acc\n",
    "                }, prog_bar=True)\n",
    "        \n",
    "        meta_optimizer.zero_grad()\n",
    "#         outer_loss.backward()\n",
    "\n",
    "        self.manual_backward(outer_loss, meta_optimizer)\n",
    "        meta_optimizer.step()\n",
    "        \n",
    "        return outer_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        meta_optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        inner_optimizer = torch.optim.SGD(self.parameters(), lr=1e-1)\n",
    "        \n",
    "        return [meta_optimizer, inner_optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7e713-faf8-4145-b8fb-e995a7cf487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = OmniglotDataModule(\n",
    "        \"data\",\n",
    "        shots=1,\n",
    "        ways=5,\n",
    "        shuffle_ds=True,\n",
    "        test_shots=15,\n",
    "        meta_train=True,\n",
    "        download=True,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067af73-1a84-4498-bb44-da7a88e93068",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAML(model=ConvolutionalNeuralNetwork(1, 5, hidden_size=64), inner_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919419b4-02a7-4119-a180-89161a124fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "logger = WandbLogger(\n",
    "    project='maml',\n",
    "    config={\n",
    "        'batch_size': 16,\n",
    "        'steps': 100,\n",
    "        'dataset': \"omniglot\",\n",
    "        'inner_steps': 1,\n",
    "        'val/test': 'enabled'\n",
    "    }\n",
    ")\n",
    "trainer = Trainer(\n",
    "        profiler='simple',\n",
    "        max_steps=100,\n",
    "        fast_dev_run=False,\n",
    "        val_check_interval=25,\n",
    "        limit_val_batches=1,\n",
    "        limit_test_batches=1,\n",
    "        num_sanity_val_steps=2, gpus=1, logger=logger\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d445dfb-84ea-44c3-bd11-75516b58df96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cool-water-15</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/p0int/maml\" target=\"_blank\">https://wandb.ai/p0int/maml</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/p0int/maml/runs/pao856jq\" target=\"_blank\">https://wandb.ai/p0int/maml/runs/pao856jq</a><br/>\n",
       "                Run data is saved locally in <code>/home/ojass/Projects/meta-learning/wandb/run-20210607_192514-pao856jq</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | ConvolutionalNeuralNetwork | 112 K \n",
      "-----------------------------------------------------\n",
      "112 K     Trainable params\n",
      "0         Non-trainable params\n",
      "112 K     Total params\n",
      "0.449     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1781fdc556cd4418b55917a0941ac8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  37.089         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  30.104         \t|1              \t|  30.104         \t|  81.167         \t|\n",
      "run_training_batch                 \t|  0.22786        \t|100            \t|  22.786         \t|  61.435         \t|\n",
      "model_forward                      \t|  0.2277         \t|100            \t|  22.77          \t|  61.393         \t|\n",
      "training_step                      \t|  0.22747        \t|100            \t|  22.747         \t|  61.33          \t|\n",
      "evaluation_step_and_end            \t|  0.35278        \t|5              \t|  1.7639         \t|  4.7558         \t|\n",
      "validation_step                    \t|  0.35246        \t|5              \t|  1.7623         \t|  4.7515         \t|\n",
      "get_train_batch                    \t|  0.010839       \t|100            \t|  1.0839         \t|  2.9225         \t|\n",
      "on_train_batch_end                 \t|  0.0014324      \t|100            \t|  0.14324        \t|  0.38619        \t|\n",
      "on_validation_start                \t|  0.014043       \t|5              \t|  0.070217       \t|  0.18932        \t|\n",
      "on_validation_end                  \t|  0.0053484      \t|5              \t|  0.026742       \t|  0.072101       \t|\n",
      "on_train_start                     \t|  0.023428       \t|1              \t|  0.023428       \t|  0.063167       \t|\n",
      "on_validation_batch_end            \t|  0.0036385      \t|5              \t|  0.018193       \t|  0.04905        \t|\n",
      "cache_result                       \t|  1.8187e-05     \t|459            \t|  0.008348       \t|  0.022508       \t|\n",
      "on_batch_start                     \t|  2.1055e-05     \t|100            \t|  0.0021055      \t|  0.0056768      \t|\n",
      "on_train_epoch_start               \t|  0.0014849      \t|1              \t|  0.0014849      \t|  0.0040037      \t|\n",
      "on_train_batch_start               \t|  1.185e-05      \t|100            \t|  0.001185       \t|  0.0031949      \t|\n",
      "training_step_end                  \t|  1.0322e-05     \t|100            \t|  0.0010322      \t|  0.002783       \t|\n",
      "on_batch_end                       \t|  1.0154e-05     \t|100            \t|  0.0010154      \t|  0.0027376      \t|\n",
      "on_train_end                       \t|  0.00083593     \t|1              \t|  0.00083593     \t|  0.0022538      \t|\n",
      "on_validation_batch_start          \t|  7.6704e-05     \t|5              \t|  0.00038352     \t|  0.001034       \t|\n",
      "on_train_epoch_end                 \t|  0.00021607     \t|1              \t|  0.00021607     \t|  0.00058258     \t|\n",
      "on_validation_epoch_end            \t|  2.5383e-05     \t|5              \t|  0.00012692     \t|  0.00034219     \t|\n",
      "validation_step_end                \t|  2.0376e-05     \t|5              \t|  0.00010188     \t|  0.00027468     \t|\n",
      "on_epoch_start                     \t|  1.6554e-05     \t|6              \t|  9.9323e-05     \t|  0.00026779     \t|\n",
      "on_epoch_end                       \t|  1.0333e-05     \t|6              \t|  6.1997e-05     \t|  0.00016716     \t|\n",
      "on_validation_epoch_start          \t|  8.217e-06      \t|5              \t|  4.1085e-05     \t|  0.00011077     \t|\n",
      "on_fit_start                       \t|  2.9514e-05     \t|1              \t|  2.9514e-05     \t|  7.9575e-05     \t|\n",
      "on_val_dataloader                  \t|  7.635e-06      \t|1              \t|  7.635e-06      \t|  2.0585e-05     \t|\n",
      "on_before_accelerator_backend_setup\t|  7.061e-06      \t|1              \t|  7.061e-06      \t|  1.9038e-05     \t|\n",
      "on_train_dataloader                \t|  6.308e-06      \t|1              \t|  6.308e-06      \t|  1.7008e-05     \t|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a42450-d257-4a0c-8052-e973f6ab9bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 72850<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/ojass/Projects/meta-learning/wandb/run-20210607_192514-pao856jq/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/ojass/Projects/meta-learning/wandb/run-20210607_192514-pao856jq/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>outer_loss</td><td>0.35524</td></tr><tr><td>accuracy</td><td>0.88667</td></tr><tr><td>val_loss</td><td>0.35524</td></tr><tr><td>val_accuracy</td><td>0.88667</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>trainer/global_step</td><td>99</td></tr><tr><td>_runtime</td><td>35</td></tr><tr><td>_timestamp</td><td>1623086749</td></tr><tr><td>_step</td><td>5</td></tr><tr><td>train_loss</td><td>0.2952</td></tr><tr><td>train_accuracy</td><td>0.91</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>outer_loss</td><td>█▄▄▁▁▂</td></tr><tr><td>accuracy</td><td>▁▅▄██▇</td></tr><tr><td>val_loss</td><td>█▄▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄█▇</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▃▃▆██</td></tr><tr><td>_runtime</td><td>▁▃▄▆██</td></tr><tr><td>_timestamp</td><td>▁▃▄▆██</td></tr><tr><td>_step</td><td>▁▂▄▅▇█</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>train_accuracy</td><td>▁█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">cool-water-15</strong>: <a href=\"https://wandb.ai/p0int/maml/runs/pao856jq\" target=\"_blank\">https://wandb.ai/p0int/maml/runs/pao856jq</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61159219-fa62-4ef1-845a-1a22521394b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = nn.Sequential(\n",
    "    K.augmentation.RandomAffine(degrees=0, translate=(0.4, 0.4), padding_mode='border'),\n",
    "    K.augmentation.RandomGaussianNoise(mean=0., std=.1, p=.3)\n",
    ")\n",
    "model = UMTRA(model=ConvolutionalNeuralNetwork(1, 5, hidden_size=64), augmentation=aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d75b5e-5fea-4699-88a8-ef6af51b8583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "logger = WandbLogger(\n",
    "    project='umtra',\n",
    "    config={\n",
    "        'batch_size': 16,\n",
    "        'steps': 100,\n",
    "        'dataset': \"omniglot\"\n",
    "    }\n",
    ")\n",
    "trainer = Trainer(\n",
    "        profiler='simple',\n",
    "        max_epochs=100,\n",
    "        max_steps=100,\n",
    "        fast_dev_run=False,\n",
    "        num_sanity_val_steps=2, gpus=1, logger=logger\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68b428-ce6c-4c94-90cf-8beeb08b88ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ethereal-water-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/p0int/umtra\" target=\"_blank\">https://wandb.ai/p0int/umtra</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/p0int/umtra/runs/3ab6fih1\" target=\"_blank\">https://wandb.ai/p0int/umtra/runs/3ab6fih1</a><br/>\n",
       "                Run data is saved locally in <code>/home/ojass/Projects/meta-learning/wandb/run-20210605_171038-3ab6fih1</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                       | Params\n",
      "------------------------------------------------------------\n",
      "0 | model        | ConvolutionalNeuralNetwork | 112 K \n",
      "1 | augmentation | Sequential                 | 0     \n",
      "------------------------------------------------------------\n",
      "112 K     Trainable params\n",
      "0         Non-trainable params\n",
      "112 K     Total params\n",
      "0.449     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c5d82221114478b5646330977ce5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  36.018         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  31.24          \t|1              \t|  31.24          \t|  86.733         \t|\n",
      "run_training_batch                 \t|  0.29617        \t|100            \t|  29.617         \t|  82.227         \t|\n",
      "model_forward                      \t|  0.29602        \t|100            \t|  29.602         \t|  82.185         \t|\n",
      "training_step                      \t|  0.29571        \t|100            \t|  29.571         \t|  82.099         \t|\n",
      "get_train_batch                    \t|  0.011208       \t|100            \t|  1.1208         \t|  3.1117         \t|\n",
      "on_train_batch_end                 \t|  0.0017598      \t|100            \t|  0.17598        \t|  0.48859        \t|\n",
      "on_train_start                     \t|  0.016859       \t|1              \t|  0.016859       \t|  0.046808       \t|\n",
      "cache_result                       \t|  2.0605e-05     \t|408            \t|  0.008407       \t|  0.023341       \t|\n",
      "on_batch_start                     \t|  2.0373e-05     \t|100            \t|  0.0020373      \t|  0.0056562      \t|\n",
      "on_train_epoch_start               \t|  0.0014782      \t|1              \t|  0.0014782      \t|  0.004104       \t|\n",
      "on_train_batch_start               \t|  1.1124e-05     \t|100            \t|  0.0011124      \t|  0.0030883      \t|\n",
      "on_batch_end                       \t|  1.0705e-05     \t|100            \t|  0.0010705      \t|  0.0029722      \t|\n",
      "training_step_end                  \t|  1.0611e-05     \t|100            \t|  0.0010611      \t|  0.002946       \t|\n",
      "on_train_end                       \t|  0.00081267     \t|1              \t|  0.00081267     \t|  0.0022563      \t|\n",
      "on_train_epoch_end                 \t|  0.00011435     \t|1              \t|  0.00011435     \t|  0.00031748     \t|\n",
      "on_fit_start                       \t|  2.9168e-05     \t|1              \t|  2.9168e-05     \t|  8.0981e-05     \t|\n",
      "on_before_accelerator_backend_setup\t|  2.7907e-05     \t|1              \t|  2.7907e-05     \t|  7.748e-05      \t|\n",
      "on_epoch_end                       \t|  1.2155e-05     \t|1              \t|  1.2155e-05     \t|  3.3747e-05     \t|\n",
      "on_train_dataloader                \t|  1.1593e-05     \t|1              \t|  1.1593e-05     \t|  3.2186e-05     \t|\n",
      "on_epoch_start                     \t|  1.0118e-05     \t|1              \t|  1.0118e-05     \t|  2.8091e-05     \t|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2192dceb-338b-4bfe-a05b-b37a6378d525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 76864<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/ojass/Projects/meta-learning/wandb/run-20210605_171038-3ab6fih1/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/ojass/Projects/meta-learning/wandb/run-20210605_171038-3ab6fih1/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>outer_loss</td><td>0.26876</td></tr><tr><td>accuracy</td><td>0.83417</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>trainer/global_step</td><td>99</td></tr><tr><td>_runtime</td><td>35</td></tr><tr><td>_timestamp</td><td>1622905873</td></tr><tr><td>_step</td><td>1</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>outer_loss</td><td>█▁</td></tr><tr><td>accuracy</td><td>▁█</td></tr><tr><td>epoch</td><td>▁▁</td></tr><tr><td>trainer/global_step</td><td>▁█</td></tr><tr><td>_runtime</td><td>▁█</td></tr><tr><td>_timestamp</td><td>▁█</td></tr><tr><td>_step</td><td>▁█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">ethereal-water-2</strong>: <a href=\"https://wandb.ai/p0int/umtra/runs/3ab6fih1\" target=\"_blank\">https://wandb.ai/p0int/umtra/runs/3ab6fih1</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae0ad69-fbc6-46dd-b849-11158faa876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_nn_utils.ipynb.\n",
      "Converted 01b_data_loaders_pl.ipynb.\n",
      "Converted 02_maml.ipynb.\n",
      "Converted 02b_maml_pl.ipynb.\n",
      "Converted 03_protonet_pl.ipynb.\n",
      "Converted 04_cactus.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9caad-734d-41fb-8aa5-189f0a3f67bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
