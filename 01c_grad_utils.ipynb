{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed3d24-256c-45e9-b57e-f440a4f38780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp hypergrad\n",
    "#export\n",
    "import torch\n",
    "from torch.autograd import grad as torch_grad\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98867f07-31a2-4add-8290-d02e66195500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\"\"\"from https://github.com/lrjconan/RBP/blob/9c6e68d1a7e61b1f4c06414fae04aeb43c8527cb/utils/model_helper.py\"\"\"\n",
    "\n",
    "def cg(Ax, b, max_iter=100, epsilon=1.0e-5):\n",
    "    \"\"\" Conjugate Gradient\n",
    "      Args:\n",
    "        Ax: function, takes list of tensors as input\n",
    "        b: list of tensors\n",
    "      Returns:\n",
    "        x_star: list of tensors\n",
    "    \"\"\"\n",
    "    x_last = [torch.zeros_like(bb) for bb in b]\n",
    "    r_last = [torch.zeros_like(bb).copy_(bb) for bb in b]\n",
    "    p_last = [torch.zeros_like(rr).copy_(rr) for rr in r_last]\n",
    "\n",
    "    for ii in range(max_iter):\n",
    "        Ap = Ax(p_last)\n",
    "        Ap_vec = cat_list_to_tensor(Ap)\n",
    "        p_last_vec = cat_list_to_tensor(p_last)\n",
    "        r_last_vec = cat_list_to_tensor(r_last)\n",
    "        rTr = torch.sum(r_last_vec * r_last_vec)\n",
    "        pAp = torch.sum(p_last_vec * Ap_vec)\n",
    "        alpha = rTr / pAp\n",
    "\n",
    "        x = [xx + alpha * pp for xx, pp in zip(x_last, p_last)]\n",
    "        r = [rr - alpha * pp for rr, pp in zip(r_last, Ap)]\n",
    "        r_vec = cat_list_to_tensor(r)\n",
    "\n",
    "        if float(torch.norm(r_vec)) < epsilon:\n",
    "            break\n",
    "\n",
    "        beta = torch.sum(r_vec * r_vec) / rTr\n",
    "        p = [rr + beta * pp for rr, pp in zip(r, p_last)]\n",
    "\n",
    "        x_last = x\n",
    "        p_last = p\n",
    "        r_last = r\n",
    "\n",
    "    return x_last\n",
    "\n",
    "\n",
    "def cat_list_to_tensor(list_tx):\n",
    "    return torch.cat([xx.view([-1]) for xx in list_tx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d953b3-618a-469d-bc45-36aaf223cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# noinspection PyUnusedLocal\n",
    "def reverse_unroll(params: List[Tensor],\n",
    "                   hparams: List[Tensor],\n",
    "                   outer_loss: Callable[[List[Tensor], List[Tensor]], Tensor],\n",
    "                   set_grad=True) -> List[Tensor]:\n",
    "    \"\"\"\n",
    "    Computes the hypergradient by backpropagating through a previously employed inner solver procedure.\n",
    "\n",
    "    Args:\n",
    "        params: the output of a torch differentiable inner solver (it must depend on hparams in the torch graph)\n",
    "        hparams: the outer variables (or hyperparameters), each element needs requires_grad=True\n",
    "        outer_loss: computes the outer objective taking parameters and hyperparameters as inputs\n",
    "        set_grad: if True set t.grad to the hypergradient for every t in hparams\n",
    "\n",
    "    Returns:\n",
    "        the list of hypergradients for each element in hparams\n",
    "    \"\"\"\n",
    "    o_loss = outer_loss(params, hparams)\n",
    "    grads = torch.autograd.grad(o_loss, hparams, retain_graph=True)\n",
    "    if set_grad:\n",
    "        update_tensor_grads(hparams, grads)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a23f1-7f89-40e3-9818-2f57f510c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# noinspection PyUnusedLocal\n",
    "def reverse(params_history: List[List[Tensor]],\n",
    "            hparams: List[Tensor],\n",
    "            update_map_history: List[Callable[[List[Tensor], List[Tensor]], List[Tensor]]],\n",
    "            outer_loss: Callable[[List[Tensor], List[Tensor]], Tensor],\n",
    "            set_grad=True) -> List[Tensor]:\n",
    "    \"\"\"\n",
    "    Computes the hypergradient by recomputing and backpropagating through each inner update\n",
    "    using the inner iterates and the update maps previously employed by the inner solver.\n",
    "    Similarly to checkpointing, this allows to save memory w.r.t. reverse_unroll by increasing computation time.\n",
    "    Truncated reverse can be performed by passing only part of the trajectory information, i.e. only the\n",
    "    last k inner iterates and updates.\n",
    "\n",
    "    Args:\n",
    "        params_history: the inner iterates (from first to last)\n",
    "        hparams: the outer variables (or hyperparameters), each element needs requires_grad=True\n",
    "        update_map_history: updates used to solve the inner problem (from first to last)\n",
    "        outer_loss: computes the outer objective taking parameters and hyperparameters as inputs\n",
    "        set_grad: if True set t.grad to the hypergradient for every t in hparams\n",
    "\n",
    "    Returns:\n",
    "         the list of hypergradients for each element in hparams\n",
    "\n",
    "    \"\"\"\n",
    "    params_history = [[w.detach().requires_grad_(True) for w in params] for params in params_history]\n",
    "    o_loss = outer_loss(params_history[-1], hparams)\n",
    "    grad_outer_w, grad_outer_hparams = get_outer_gradients(o_loss, params_history[-1], hparams)\n",
    "\n",
    "    alphas = grad_outer_w\n",
    "    grads = [torch.zeros_like(w) for w in hparams]\n",
    "    K = len(params_history) - 1\n",
    "    for k in range(-2, -(K + 2), -1):\n",
    "        w_mapped = update_map_history[k + 1](params_history[k], hparams)\n",
    "        bs = grad_unused_zero(w_mapped, hparams, grad_outputs=alphas, retain_graph=True)\n",
    "        grads = [g + b for g, b in zip(grads, bs)]\n",
    "        alphas = torch_grad(w_mapped, params_history[k], grad_outputs=alphas)\n",
    "\n",
    "    grads = [g + v for g, v in zip(grads, grad_outer_hparams)]\n",
    "    if set_grad:\n",
    "        update_tensor_grads(hparams, grads)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e857034-7c8c-4498-b0e9-04d64a174e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fixed_point(params: List[Tensor],\n",
    "                hparams: List[Tensor],\n",
    "                K: int ,\n",
    "                fp_map: Callable[[List[Tensor], List[Tensor]], List[Tensor]],\n",
    "                outer_loss: Callable[[List[Tensor], List[Tensor]], Tensor],\n",
    "                tol=1e-10,\n",
    "                set_grad=True,\n",
    "                stochastic=False) -> List[Tensor]:\n",
    "    \"\"\"\n",
    "    Computes the hypergradient by applying K steps of the fixed point method (it can end earlier when tol is reached).\n",
    "\n",
    "    Args:\n",
    "        params: the output of the inner solver procedure.\n",
    "        hparams: the outer variables (or hyperparameters), each element needs requires_grad=True\n",
    "        K: the maximum number of fixed point iterations\n",
    "        fp_map: the fixed point map which defines the inner problem\n",
    "        outer_loss: computes the outer objective taking parameters and hyperparameters as inputs\n",
    "        tol: end the method earlier when  the normed difference between two iterates is less than tol\n",
    "        set_grad: if True set t.grad to the hypergradient for every t in hparams\n",
    "        stochastic: set this to True when fp_map is not a deterministic function of its inputs\n",
    "\n",
    "    Returns:\n",
    "        the list of hypergradients for each element in hparams\n",
    "    \"\"\"\n",
    "\n",
    "    params = [w.detach().requires_grad_(True) for w in params]\n",
    "    o_loss = outer_loss(params, hparams)\n",
    "    grad_outer_w, grad_outer_hparams = get_outer_gradients(o_loss, params, hparams)\n",
    "\n",
    "    if not stochastic:\n",
    "        w_mapped = fp_map(params, hparams)\n",
    "\n",
    "    vs = [torch.zeros_like(w) for w in params]\n",
    "    vs_vec = cat_list_to_tensor(vs)\n",
    "    for k in range(K):\n",
    "        vs_prev_vec = vs_vec\n",
    "\n",
    "        if stochastic:\n",
    "            w_mapped = fp_map(params, hparams)\n",
    "            vs = torch_grad(w_mapped, params, grad_outputs=vs, retain_graph=False)\n",
    "        else:\n",
    "            vs = torch_grad(w_mapped, params, grad_outputs=vs, retain_graph=True)\n",
    "\n",
    "        vs = [v + gow for v, gow in zip(vs, grad_outer_w)]\n",
    "        vs_vec = cat_list_to_tensor(vs)\n",
    "        if float(torch.norm(vs_vec - vs_prev_vec)) < tol:\n",
    "            break\n",
    "\n",
    "    if stochastic:\n",
    "        w_mapped = fp_map(params, hparams)\n",
    "\n",
    "    grads = torch_grad(w_mapped, hparams, grad_outputs=vs, allow_unused=True)\n",
    "    grads = [g + v if g is not None else v for g, v in zip(grads, grad_outer_hparams)]\n",
    "\n",
    "    if set_grad:\n",
    "        update_tensor_grads(hparams, grads)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba05fe2-86a3-4bf8-95aa-a1e18103bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CG(params: List[Tensor],\n",
    "       hparams: List[Tensor],\n",
    "       K: int ,\n",
    "       fp_map: Callable[[List[Tensor], List[Tensor]], List[Tensor]],\n",
    "       outer_loss: Callable[[List[Tensor], List[Tensor]], Tensor],\n",
    "       tol=1e-10,\n",
    "       set_grad=True,\n",
    "       stochastic=False) -> List[Tensor]:\n",
    "    \"\"\"\n",
    "     Computes the hypergradient by applying K steps of the conjugate gradient method (CG).\n",
    "     It can end earlier when tol is reached.\n",
    "\n",
    "     Args:\n",
    "         params: the output of the inner solver procedure.\n",
    "         hparams: the outer variables (or hyperparameters), each element needs requires_grad=True\n",
    "         K: the maximum number of conjugate gradient iterations\n",
    "         fp_map: the fixed point map which defines the inner problem\n",
    "         outer_loss: computes the outer objective taking parameters and hyperparameters as inputs\n",
    "         tol: end the method earlier when the norm of the residual is less than tol\n",
    "         set_grad: if True set t.grad to the hypergradient for every t in hparams\n",
    "         stochastic: set this to True when fp_map is not a deterministic function of its inputs\n",
    "\n",
    "     Returns:\n",
    "         the list of hypergradients for each element in hparams\n",
    "     \"\"\"\n",
    "    params = [w.detach().requires_grad_(True) for w in params]\n",
    "    o_loss = outer_loss(params, hparams)\n",
    "    grad_outer_w, grad_outer_hparams = get_outer_gradients(o_loss, params, hparams)\n",
    "\n",
    "    if not stochastic:\n",
    "        w_mapped = fp_map(params, hparams)\n",
    "\n",
    "    def dfp_map_dw(xs):\n",
    "        if stochastic:\n",
    "            w_mapped_in = fp_map(params, hparams)\n",
    "            Jfp_mapTv = torch_grad(w_mapped_in, params, grad_outputs=xs, retain_graph=False)\n",
    "        else:\n",
    "            Jfp_mapTv = torch_grad(w_mapped, params, grad_outputs=xs, retain_graph=True)\n",
    "        return [v - j for v, j in zip(xs, Jfp_mapTv)]\n",
    "\n",
    "    vs = cg(dfp_map_dw, grad_outer_w, max_iter=K, epsilon=tol)  # K steps of conjugate gradient\n",
    "\n",
    "    if stochastic:\n",
    "        w_mapped = fp_map(params, hparams)\n",
    "\n",
    "    grads = torch_grad(w_mapped, hparams, grad_outputs=vs)\n",
    "    grads = [g + v for g, v in zip(grads, grad_outer_hparams)]\n",
    "\n",
    "    if set_grad:\n",
    "        update_tensor_grads(hparams, grads)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752bc4df-7096-439d-8db8-71f706811c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CG_normaleq(params: List[Tensor],\n",
    "                hparams: List[Tensor],\n",
    "                K: int ,\n",
    "                fp_map: Callable[[List[Tensor], List[Tensor]], List[Tensor]],\n",
    "                outer_loss: Callable[[List[Tensor], List[Tensor]], Tensor],\n",
    "                tol=1e-10,\n",
    "                set_grad=True) -> List[Tensor]:\n",
    "    \"\"\" Similar to CG but the conjugate gradient is applied on the normal equation (has a higher time complexity)\"\"\"\n",
    "    params = [w.detach().requires_grad_(True) for w in params]\n",
    "    o_loss = outer_loss(params, hparams)\n",
    "    grad_outer_w, grad_outer_hparams = get_outer_gradients(o_loss, params, hparams)\n",
    "\n",
    "    w_mapped = fp_map(params, hparams)\n",
    "\n",
    "    def dfp_map_dw(xs):\n",
    "        Jfp_mapTv = torch_grad(w_mapped, params, grad_outputs=xs, retain_graph=True)\n",
    "        v_minus_Jfp_mapTv = [v - j for v, j in zip(xs, Jfp_mapTv)]\n",
    "\n",
    "        # normal equation part\n",
    "        Jfp_mapv_minus_Jfp_mapJfp_mapTv = jvp(lambda _params: fp_map(_params, hparams), params, v_minus_Jfp_mapTv)\n",
    "        return [v - vv for v, vv in zip(v_minus_Jfp_mapTv, Jfp_mapv_minus_Jfp_mapJfp_mapTv)]\n",
    "\n",
    "    v_minus_Jfp_mapv = [g - jfp_mapv for g, jfp_mapv in zip(grad_outer_w, jvp(\n",
    "        lambda _params: fp_map(_params, hparams), params, grad_outer_w))]\n",
    "    vs = cg(dfp_map_dw, v_minus_Jfp_mapv, max_iter=K, epsilon=tol)  # K steps of conjugate gradient\n",
    "\n",
    "    grads = torch_grad(w_mapped, hparams, grad_outputs=vs, allow_unused=True)\n",
    "    grads = [g + v if g is not None else v for g, v in zip(grads, grad_outer_hparams)]\n",
    "\n",
    "    if set_grad:\n",
    "        update_tensor_grads(hparams, grads)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a600876-99c1-4d27-8975-678310961abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def neumann(params: List[Tensor],\n",
    "            hparams: List[Tensor],\n",
    "            K: int ,\n",
    "            fp_map: Callable[[List[Tensor], List[Tensor]], List[Tensor]],\n",
    "            outer_loss: Callable[[List[Tensor], List[Tensor]], Tensor],\n",
    "            tol=1e-10,\n",
    "            set_grad=True) -> List[Tensor]:\n",
    "    \"\"\" Saves one iteration from the fixed point method\"\"\"\n",
    "\n",
    "    # from https://arxiv.org/pdf/1803.06396.pdf,  should return the same gradient of fixed point K+1\n",
    "    params = [w.detach().requires_grad_(True) for w in params]\n",
    "    o_loss = outer_loss(params, hparams)\n",
    "    grad_outer_w, grad_outer_hparams = get_outer_gradients(o_loss, params, hparams)\n",
    "\n",
    "    w_mapped = fp_map(params, hparams)\n",
    "    vs, gs = grad_outer_w, grad_outer_w\n",
    "    gs_vec = cat_list_to_tensor(gs)\n",
    "    for k in range(K):\n",
    "        gs_prev_vec = gs_vec\n",
    "        vs = torch_grad(w_mapped, params, grad_outputs=vs, retain_graph=True)\n",
    "        gs = [g + v for g, v in zip(gs, vs)]\n",
    "        gs_vec = cat_list_to_tensor(gs)\n",
    "        if float(torch.norm(gs_vec - gs_prev_vec)) < tol:\n",
    "            break\n",
    "\n",
    "    grads = torch_grad(w_mapped, hparams, grad_outputs=gs)\n",
    "    grads = [g + v for g, v in zip(grads, grad_outer_hparams)]\n",
    "    if set_grad:\n",
    "        update_tensor_grads(hparams, grads)\n",
    "    return grads\n",
    "\n",
    "\n",
    "def exact(opt_params_f: Callable[[List[Tensor]], List[Tensor]],\n",
    "          hparams: List[Tensor],\n",
    "          outer_loss: Callable[[List[Tensor], List[Tensor]], Tensor],\n",
    "          set_grad=True) -> List[Tensor]:\n",
    "    \"\"\"\n",
    "    Computes the exact hypergradient using backpropagation and exploting the closed form torch differentiable function\n",
    "    that computes the optimal parameters given the hyperparameters (opt_params_f).\n",
    "    \"\"\"\n",
    "    grads = torch_grad(outer_loss(opt_params_f(hparams), hparams), hparams)\n",
    "    if set_grad:\n",
    "        update_tensor_grads(hparams, grads)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70d3c7-a534-4e48-b002-a4b71fbb2cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# UTILS\n",
    "\n",
    "def grd(a, b):\n",
    "    return torch.autograd.grad(a, b, create_graph=True, retain_graph=True)\n",
    "\n",
    "\n",
    "def list_dot(l1, l2):  # extended dot product for lists\n",
    "    return torch.stack([(a*b).sum() for a, b in zip(l1, l2)]).sum()\n",
    "\n",
    "\n",
    "def jvp(fp_map, params, vs):\n",
    "    dummy = [torch.ones_like(phw).requires_grad_(True) for phw in fp_map(params)]\n",
    "    g1 = grd(list_dot(fp_map(params), dummy), params)\n",
    "    return grd(list_dot(vs, g1), dummy)\n",
    "\n",
    "\n",
    "def get_outer_gradients(outer_loss, params, hparams, retain_graph=True):\n",
    "    grad_outer_w = grad_unused_zero(outer_loss, params, retain_graph=retain_graph)\n",
    "    grad_outer_hparams = grad_unused_zero(outer_loss, hparams, retain_graph=retain_graph)\n",
    "\n",
    "    return grad_outer_w, grad_outer_hparams\n",
    "\n",
    "\n",
    "def cat_list_to_tensor(list_tx):\n",
    "    return torch.cat([xx.view([-1]) for xx in list_tx])\n",
    "\n",
    "\n",
    "def update_tensor_grads(hparams, grads):\n",
    "    for l, g in zip(hparams, grads):\n",
    "        if l.grad is None:\n",
    "            l.grad = torch.zeros_like(l)\n",
    "        if g is not None:\n",
    "            l.grad += g\n",
    "\n",
    "\n",
    "def grad_unused_zero(output, inputs, grad_outputs=None, retain_graph=False, create_graph=False):\n",
    "    grads = torch.autograd.grad(output, inputs, grad_outputs=grad_outputs, allow_unused=True,\n",
    "                                retain_graph=retain_graph, create_graph=create_graph)\n",
    "\n",
    "    def grad_or_zeros(grad, var):\n",
    "        return torch.zeros_like(var) if grad is None else grad\n",
    "\n",
    "    return tuple(grad_or_zeros(g, v) for g, v in zip(grads, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93837c62-ff49-4386-9c91-e7b0d3db3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DifferentiableOptimizer:\n",
    "    def __init__(self, loss_f, dim_mult, data_or_iter=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loss_f: callable with signature (params, hparams, [data optional]) -> loss tensor\n",
    "            data_or_iter: (x, y) or iterator over the data needed for loss_f\n",
    "        \"\"\"\n",
    "        self.data_iterator = None\n",
    "        if data_or_iter:\n",
    "            self.data_iterator = data_or_iter if hasattr(data_or_iter, '__next__') else repeat(data_or_iter)\n",
    "\n",
    "        self.loss_f = loss_f\n",
    "        self.dim_mult = dim_mult\n",
    "        self.curr_loss = None\n",
    "\n",
    "    def get_opt_params(self, params):\n",
    "        opt_params = [p for p in params]\n",
    "        opt_params.extend([torch.zeros_like(p) for p in params for _ in range(self.dim_mult-1) ])\n",
    "        return opt_params\n",
    "\n",
    "    def step(self, params, hparams, create_graph):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, params, hparams, create_graph=True):\n",
    "        with torch.enable_grad():\n",
    "            return self.step(params, hparams, create_graph)\n",
    "\n",
    "    def get_loss(self, params, hparams):\n",
    "        if self.data_iterator:\n",
    "            data = next(self.data_iterator)\n",
    "            self.curr_loss = self.loss_f(params, hparams, data)\n",
    "        else:\n",
    "            self.curr_loss = self.loss_f(params, hparams)\n",
    "        return self.curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0fda0-f03e-4881-ac2b-151963aa8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HeavyBall(DifferentiableOptimizer):\n",
    "    def __init__(self, loss_f, step_size, momentum, data_or_iter=None):\n",
    "        super(HeavyBall, self).__init__(loss_f, dim_mult=2, data_or_iter=data_or_iter)\n",
    "        self.loss_f = loss_f\n",
    "        self.step_size_f = step_size if callable(step_size) else lambda x: step_size\n",
    "        self.momentum_f = momentum if callable(momentum) else lambda x: momentum\n",
    "\n",
    "    def step(self, params, hparams, create_graph):\n",
    "        n = len(params) // 2\n",
    "        p, p_aux = params[:n], params[n:]\n",
    "        loss = self.get_loss(p, hparams)\n",
    "        sz, mu = self.step_size_f(hparams), self.momentum_f(hparams)\n",
    "        p_new, p_new_aux = heavy_ball_step(p, p_aux, loss, sz,  mu, create_graph=create_graph)\n",
    "        return [*p_new, *p_new_aux]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1cfdfe-10fa-4513-b4c5-7ca892b69082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Momentum(DifferentiableOptimizer):\n",
    "    \"\"\"\n",
    "    GD with momentum step as implemented in torch.optim.SGD\n",
    "    .. math::\n",
    "              v_{t+1} = \\mu * v_{t} + g_{t+1} \\\\\n",
    "              p_{t+1} = p_{t} - lr * v_{t+1}\n",
    "    \"\"\"\n",
    "    def __init__(self, loss_f, step_size, momentum, data_or_iter=None):\n",
    "        super(Momentum, self).__init__(loss_f, dim_mult=2, data_or_iter=data_or_iter)\n",
    "        self.loss_f = loss_f\n",
    "        self.step_size_f = step_size if callable(step_size) else lambda x: step_size\n",
    "        self.momentum_f = momentum if callable(momentum) else lambda x: momentum\n",
    "\n",
    "    def step(self, params, hparams, create_graph):\n",
    "        n = len(params) // 2\n",
    "        p, p_aux = params[:n], params[n:]\n",
    "        loss = self.get_loss(p, hparams)\n",
    "        sz, mu = self.step_size_f(hparams), self.momentum_f(hparams)\n",
    "        p_new, p_new_aux = torch_momentum_step(p, p_aux, loss, sz,  mu, create_graph=create_graph)\n",
    "        return [*p_new, *p_new_aux]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fe62b-29cd-4d26-99f5-2fef0d662cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GradientDescent(DifferentiableOptimizer):\n",
    "    def __init__(self, loss_f, step_size, data_or_iter=None):\n",
    "        super(GradientDescent, self).__init__(loss_f, dim_mult=1, data_or_iter=data_or_iter)\n",
    "        self.step_size_f = step_size if callable(step_size) else lambda x: step_size\n",
    "\n",
    "    def step(self, params, hparams, create_graph):\n",
    "        loss = self.get_loss(params, hparams)\n",
    "        sz = self.step_size_f(hparams)\n",
    "        return gd_step(params, loss, sz, create_graph=create_graph)\n",
    "\n",
    "\n",
    "def gd_step(params, loss, step_size, create_graph=True):\n",
    "    grads = torch.autograd.grad(loss, params, create_graph=create_graph)\n",
    "    return [w - step_size * g for w, g in zip(params, grads)]\n",
    "\n",
    "\n",
    "def heavy_ball_step(params, aux_params, loss, step_size, momentum, create_graph=True):\n",
    "    grads = torch.autograd.grad(loss, params, create_graph=create_graph)\n",
    "    return [w - step_size * g + momentum * (w - v) for g, w, v in zip(grads, params, aux_params)], params\n",
    "\n",
    "\n",
    "def torch_momentum_step(params, aux_params, loss, step_size, momentum, create_graph=True):\n",
    "    \"\"\"\n",
    "    GD with momentum step as implemented in torch.optim.SGD\n",
    "    .. math::\n",
    "              v_{t+1} = \\mu * v_{t} + g_{t+1} \\\\\n",
    "              p_{t+1} = p_{t} - lr * v_{t+1}\n",
    "    \"\"\"\n",
    "    grads = torch.autograd.grad(loss, params, create_graph=create_graph)\n",
    "    new_aux_params = [momentum*v + g for v, g in zip(aux_params, grads)]\n",
    "    return [w - step_size * nv for w, nv in zip(params, new_aux_params)], new_aux_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce786ed1-82d6-4c82-ac0d-cd2657dc7296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_nn_utils.ipynb.\n",
      "Converted 01b_data_loaders_pl.ipynb.\n",
      "Converted 01c_grad_utils.ipynb.\n",
      "Converted 01d_hessian_free.ipynb.\n",
      "Converted 02_maml_pl.ipynb.\n",
      "Converted 02b_iMAML.ipynb.\n",
      "Converted 03_protonet_pl.ipynb.\n",
      "Converted 04_cactus.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373439c-5b36-41e7-89ab-54fdc701a044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
