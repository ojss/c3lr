{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b8422-d8fa-405d-855d-330f2e4b5cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp hessian_free\n",
    "#export\n",
    "import torch\n",
    "from torch.nn.utils.convert_parameters import vector_to_parameters, parameters_to_vector\n",
    "from functools import reduce\n",
    "\n",
    "# https://github.com/fmeirinhos/pytorch-hessianfree/blob/master/hessianfree.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8d324-cfd3-4a4c-b426-733fd8506484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HessianFree(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Implements the Hessian-free algorithm presented in `Training Deep and\n",
    "    Recurrent Networks with Hessian-Free Optimization`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1)\n",
    "        delta_decay (float, optional): Decay of the previous result of\n",
    "            computing delta with conjugate gradient method for the\n",
    "            initialization of the next conjugate gradient iteration\n",
    "        damping (float, optional): Initial value of the Tikhonov damping\n",
    "            coefficient. (default: 0.5)\n",
    "        max_iter (int, optional): Maximum number of Conjugate-Gradient\n",
    "            iterations (default: 50)\n",
    "        use_gnm (bool, optional): Use the generalized Gauss-Newton matrix:\n",
    "            probably solves the indefiniteness of the Hessian (Section 20.6)\n",
    "        verbose (bool, optional): Print statements (debugging)\n",
    "\n",
    "    .. _Training Deep and Recurrent Networks with Hessian-Free Optimization:\n",
    "        https://doi.org/10.1007/978-3-642-35289-8_27\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params,\n",
    "                 lr=1,\n",
    "                 damping=0.5,\n",
    "                 delta_decay=0.95,\n",
    "                 cg_max_iter=100,\n",
    "                 use_gnm=True,\n",
    "                 verbose=False):\n",
    "\n",
    "        if not (0.0 < lr <= 1):\n",
    "            raise ValueError(\"Invalid lr: {}\".format(lr))\n",
    "\n",
    "        if not (0.0 < damping <= 1):\n",
    "            raise ValueError(\"Invalid damping: {}\".format(damping))\n",
    "\n",
    "        if not cg_max_iter > 0:\n",
    "            raise ValueError(\"Invalid cg_max_iter: {}\".format(cg_max_iter))\n",
    "\n",
    "        defaults = dict(alpha=lr,\n",
    "                        damping=damping,\n",
    "                        delta_decay=delta_decay,\n",
    "                        cg_max_iter=cg_max_iter,\n",
    "                        use_gnm=use_gnm,\n",
    "                        verbose=verbose)\n",
    "        super(HessianFree, self).__init__(params, defaults)\n",
    "\n",
    "        if len(self.param_groups) != 1:\n",
    "            raise ValueError(\n",
    "                \"HessianFree doesn't support per-parameter options (parameter groups)\")\n",
    "\n",
    "        self._params = self.param_groups[0]['params']\n",
    "\n",
    "    def _gather_flat_grad(self):\n",
    "        views = list()\n",
    "        for p in self._params:\n",
    "            if p.grad is None:\n",
    "                view = p.data.new(p.data.numel()).zero_()\n",
    "            elif p.grad.data.is_sparse:\n",
    "                view = p.grad.data.to_dense().view(-1)\n",
    "            else:\n",
    "                view = p.grad.contiguous().view(-1)\n",
    "            views.append(view)\n",
    "        return torch.cat(views, 0)\n",
    "\n",
    "    def step(self, closure, b=None, M_inv=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable): A closure that re-evaluates the model\n",
    "                and returns a tuple of the loss and the output.\n",
    "            b (callable, optional): A closure that calculates the vector b in\n",
    "                the minimization problem x^T . A . x + x^T b.\n",
    "            M (callable, optional): The INVERSE preconditioner of A\n",
    "        \"\"\"\n",
    "        assert len(self.param_groups) == 1\n",
    "\n",
    "        group = self.param_groups[0]\n",
    "        alpha = group['alpha']\n",
    "        delta_decay = group['delta_decay']\n",
    "        cg_max_iter = group['cg_max_iter']\n",
    "        damping = group['damping']\n",
    "        use_gnm = group['use_gnm']\n",
    "        verbose = group['verbose']\n",
    "\n",
    "        state = self.state[self._params[0]]\n",
    "        state.setdefault('func_evals', 0)\n",
    "        state.setdefault('n_iter', 0)\n",
    "\n",
    "        loss_before, output = closure()\n",
    "        current_evals = 1\n",
    "        state['func_evals'] += 1\n",
    "\n",
    "        # Gather current parameters and respective gradients\n",
    "        flat_params = parameters_to_vector(self._params)\n",
    "        flat_grad = self._gather_flat_grad()\n",
    "\n",
    "        # Define linear operator\n",
    "        if use_gnm:\n",
    "            # Generalized Gauss-Newton vector product\n",
    "            def A(x):\n",
    "                return self._Gv(loss_before, output, x, damping)\n",
    "        else:\n",
    "            # Hessian-vector product\n",
    "            def A(x):\n",
    "                return self._Hv(flat_grad, x, damping)\n",
    "\n",
    "        if M_inv is not None:\n",
    "            m_inv = M_inv()\n",
    "\n",
    "            # Preconditioner recipe (Section 20.13)\n",
    "            if m_inv.dim() == 1:\n",
    "                m = (m_inv + damping) ** (-0.85)\n",
    "\n",
    "                def M(x):\n",
    "                    return m * x\n",
    "            else:\n",
    "                m = torch.inverse(m_inv + damping * torch.eye(*m_inv.shape))\n",
    "\n",
    "                def M(x):\n",
    "                    return m @ x\n",
    "        else:\n",
    "            M = None\n",
    "\n",
    "        b = flat_grad.detach() if b is None else b().detach().flatten()\n",
    "\n",
    "        # Initializing Conjugate-Gradient (Section 20.10)\n",
    "        if state.get('init_delta') is not None:\n",
    "            init_delta = delta_decay * state.get('init_delta')\n",
    "        else:\n",
    "            init_delta = torch.zeros_like(flat_params)\n",
    "\n",
    "        eps = torch.finfo(b.dtype).eps\n",
    "\n",
    "        # Conjugate-Gradient\n",
    "        deltas, Ms = self._CG(A=A, b=b.neg(), x0=init_delta,\n",
    "                              M=M, max_iter=cg_max_iter,\n",
    "                              tol=1e1 * eps, eps=eps, martens=True)\n",
    "\n",
    "        # Update parameters\n",
    "        delta = state['init_delta'] = deltas[-1]\n",
    "        M = Ms[-1]\n",
    "\n",
    "        vector_to_parameters(flat_params + delta, self._params)\n",
    "        loss_now = closure()[0]\n",
    "        current_evals += 1\n",
    "        state['func_evals'] += 1\n",
    "\n",
    "        # Conjugate-Gradient backtracking (Section 20.8.7)\n",
    "        if verbose:\n",
    "            print(\"Loss before CG: {}\".format(float(loss_before)))\n",
    "            print(\"Loss before BT: {}\".format(float(loss_now)))\n",
    "\n",
    "        for (d, m) in zip(reversed(deltas[:-1][::2]), reversed(Ms[:-1][::2])):\n",
    "            vector_to_parameters(flat_params + d, self._params)\n",
    "            loss_prev = closure()[0]\n",
    "            if float(loss_prev) > float(loss_now):\n",
    "                break\n",
    "            delta = d\n",
    "            M = m\n",
    "            loss_now = loss_prev\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Loss after BT:  {}\".format(float(loss_now)))\n",
    "\n",
    "        # The Levenberg-Marquardt Heuristic (Section 20.8.5)\n",
    "        reduction_ratio = (float(loss_now) -\n",
    "                           float(loss_before)) / M if M != 0 else 1\n",
    "\n",
    "        if reduction_ratio < 0.25:\n",
    "            group['damping'] *= 3 / 2\n",
    "        elif reduction_ratio > 0.75:\n",
    "            group['damping'] *= 2 / 3\n",
    "        if reduction_ratio < 0:\n",
    "            group['init_delta'] = 0\n",
    "\n",
    "        # Line Searching (Section 20.8.8)\n",
    "        beta = 0.8\n",
    "        c = 1e-2\n",
    "        min_improv = min(c * torch.dot(b, delta), 0)\n",
    "\n",
    "        for _ in range(60):\n",
    "            if float(loss_now) <= float(loss_before) + alpha * min_improv:\n",
    "                break\n",
    "\n",
    "            alpha *= beta\n",
    "            vector_to_parameters(flat_params + alpha * delta, self._params)\n",
    "            loss_now = closure()[0]\n",
    "        else:  # No good update found\n",
    "            alpha = 0.0\n",
    "            loss_now = loss_before\n",
    "\n",
    "        # Update the parameters (this time fo real)\n",
    "        vector_to_parameters(flat_params + alpha * delta, self._params)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Loss after LS:  {0} (lr: {1:.3f})\".format(\n",
    "                float(loss_now), alpha))\n",
    "            print(\"Tikhonov damping: {0:.3f} (reduction ratio: {1:.3f})\".format(\n",
    "                group['damping'], reduction_ratio), end='\\n\\n')\n",
    "\n",
    "        return loss_now\n",
    "\n",
    "    def _CG(self, A, b, x0, M=None, max_iter=50, tol=1.2e-6, eps=1.2e-7,\n",
    "            martens=False):\n",
    "        \"\"\"\n",
    "        Minimizes the linear system x^T.A.x - x^T b using the conjugate\n",
    "            gradient method\n",
    "\n",
    "        Arguments:\n",
    "            A (callable): An abstract linear operator implementing the\n",
    "                product A.x. A must represent a hermitian, positive definite\n",
    "                matrix.\n",
    "            b (torch.Tensor): The vector b.\n",
    "            x0 (torch.Tensor): An initial guess for x.\n",
    "            M (callable, optional): An abstract linear operator implementing\n",
    "            the product of the preconditioner (for A) matrix with a vector.\n",
    "            tol (float, optional): Tolerance for convergence.\n",
    "            martens (bool, optional): Flag for Martens' convergence criterion.\n",
    "        \"\"\"\n",
    "\n",
    "        x = [x0]\n",
    "        r = A(x[0]) - b\n",
    "\n",
    "        if M is not None:\n",
    "            y = M(r)\n",
    "            p = -y\n",
    "        else:\n",
    "            p = -r\n",
    "\n",
    "        res_i_norm = r @ r\n",
    "\n",
    "        if martens:\n",
    "            m = [0.5 * (r - b) @ x0]\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            Ap = A(p)\n",
    "\n",
    "            alpha = res_i_norm / ((p @ Ap) + eps)\n",
    "\n",
    "            x.append(x[i] + alpha * p)\n",
    "            r = r + alpha * Ap\n",
    "\n",
    "            if M is not None:\n",
    "                y = M(r)\n",
    "                res_ip1_norm = y @ r\n",
    "            else:\n",
    "                res_ip1_norm = r @ r\n",
    "\n",
    "            beta = res_ip1_norm / (res_i_norm + eps)\n",
    "            res_i_norm = res_ip1_norm\n",
    "\n",
    "            # Martens' Relative Progress stopping condition (Section 20.4)\n",
    "            if martens:\n",
    "                m.append(0.5 * A(x[i + 1]) @ x[i + 1] - b @ x[i + 1])\n",
    "\n",
    "                k = max(10, int(i / 10))\n",
    "                if i > k:\n",
    "                    stop = (m[i] - m[i - k]) / (m[i] + eps)\n",
    "                    if stop < 1e-4:\n",
    "                        break\n",
    "\n",
    "            if res_i_norm < tol or torch.isnan(res_i_norm):\n",
    "                break\n",
    "\n",
    "            if M is not None:\n",
    "                p = - y + beta * p\n",
    "            else:\n",
    "                p = - r + beta * p\n",
    "\n",
    "        return (x, m) if martens else (x, None)\n",
    "\n",
    "    def _Hv(self, gradient, vec, damping):\n",
    "        \"\"\"\n",
    "        Computes the Hessian vector product.\n",
    "        \"\"\"\n",
    "        Hv = self._Rop(gradient, self._params, vec)\n",
    "\n",
    "        # Tikhonov damping (Section 20.8.1)\n",
    "        return Hv.detach() + damping * vec\n",
    "\n",
    "    def _Gv(self, loss, output, vec, damping):\n",
    "        \"\"\"\n",
    "        Computes the generalized Gauss-Newton vector product.\n",
    "        \"\"\"\n",
    "        Jv = self._Rop(output, self._params, vec)\n",
    "\n",
    "        gradient = torch.autograd.grad(loss, output, create_graph=True)\n",
    "        HJv = self._Rop(gradient, output, Jv)\n",
    "\n",
    "        JHJv = torch.autograd.grad(\n",
    "            output, self._params, grad_outputs=HJv.reshape_as(output), retain_graph=True)\n",
    "\n",
    "        # Tikhonov damping (Section 20.8.1)\n",
    "        return parameters_to_vector(JHJv).detach() + damping * vec\n",
    "\n",
    "    @staticmethod\n",
    "    def _Rop(y, x, v, create_graph=False):\n",
    "        \"\"\"\n",
    "        Computes the product (dy_i/dx_j) v_j: R-operator\n",
    "        \"\"\"\n",
    "        if isinstance(y, tuple):\n",
    "            ws = [torch.zeros_like(y_i, requires_grad=True) for y_i in y]\n",
    "        else:\n",
    "            ws = torch.zeros_like(y, requires_grad=True)\n",
    "\n",
    "        jacobian = torch.autograd.grad(\n",
    "            y, x, grad_outputs=ws, create_graph=True)\n",
    "\n",
    "        Jv = torch.autograd.grad(parameters_to_vector(\n",
    "            jacobian), ws, grad_outputs=v, create_graph=create_graph)\n",
    "\n",
    "        return parameters_to_vector(Jv)\n",
    "\n",
    "\n",
    "# The empirical Fisher diagonal (Section 20.11.3)\n",
    "def empirical_fisher_diagonal(net, xs, ys, criterion):\n",
    "    grads = list()\n",
    "    for (x, y) in zip(xs, ys):\n",
    "        fi = criterion(net(x), y)\n",
    "        grads.append(torch.autograd.grad(fi, net.parameters(),\n",
    "                                         retain_graph=False))\n",
    "\n",
    "    vec = torch.cat([(torch.stack(p) ** 2).mean(0).detach().flatten()\n",
    "                     for p in zip(*grads)])\n",
    "    return vec\n",
    "\n",
    "\n",
    "# The empirical Fisher matrix (Section 20.11.3)\n",
    "def empirical_fisher_matrix(net, xs, ys, criterion):\n",
    "    grads = list()\n",
    "    for (x, y) in zip(xs, ys):\n",
    "        fi = criterion(net(x), y)\n",
    "        grad = torch.autograd.grad(fi, net.parameters(),\n",
    "                                   retain_graph=False)\n",
    "        grads.append(torch.cat([g.detach().flatten() for g in grad]))\n",
    "\n",
    "    grads = torch.stack(grads)\n",
    "    n_batch = grads.shape[0]\n",
    "    return torch.einsum('ij,ik->jk', grads, grads) / n_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ea81d-6ea2-4e38-b803-226d43a4386b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_nn_utils.ipynb.\n",
      "Converted 01b_data_loaders_pl.ipynb.\n",
      "Converted 01c_hessian_free.ipynb.\n",
      "Converted 02_maml_pl.ipynb.\n",
      "Converted 02b_iMAML.ipynb.\n",
      "Converted 03_protonet_pl.ipynb.\n",
      "Converted 04_cactus.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034adfec-e116-4e9a-b18b-d3a7042b05a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
