{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import reduce\n",
    "from operator import __add__\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import umap\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from unsupervised_meta_learning.pl_dataloaders import (UnlabelledDataModule, get_episode_loader,\n",
    "                                                       UnlabelledDataset)\n",
    "from unsupervised_meta_learning.proto_utils import euclidean_distance, cosine_similarity, nt_xent_loss\n",
    "from unsupervised_meta_learning.protoclr import ProtoCLR, get_train_images\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn import cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    profiler='simple',\n",
    "    max_epochs=2,\n",
    "    limit_train_batches=100,\n",
    "    fast_dev_run=False,\n",
    "    limit_val_batches=15,\n",
    "    limit_test_batches=600,\n",
    "    num_sanity_val_steps=2, gpus=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder = trainer.tuner.lr_find(model, train_dataloader=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_sizes = (3, 3)\n",
    "conv_padding = reduce(__add__, [(k // 2 + (k - 2 * (k // 2)) - 1, k // 2) for k in kernel_sizes[::-1]])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_size=64, out_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # nn.ZeroPad2d(conv_padding),\n",
    "            nn.Conv2d(in_channels, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14 x 14\n",
    "\n",
    "            # nn.ZeroPad2d(conv_padding),\n",
    "            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 7x7\n",
    "\n",
    "            # nn.ZeroPad2d(conv_padding),\n",
    "            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 3x3\n",
    "\n",
    "            # nn.ZeroPad2d(conv_padding),\n",
    "            nn.Conv2d(hidden_size, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 1x1\n",
    "            # nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.encoder(inputs)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_size=64, out_channels=64):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.UpsamplingNearest2d(size=(4, 4)),\n",
    "            nn.Conv2d(in_channels=out_channels,\n",
    "                      out_channels=hidden_size, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.UpsamplingNearest2d(size=(7, 7)),\n",
    "            nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size,\n",
    "                      kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.UpsamplingNearest2d(size=(14, 14)),\n",
    "            nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size,\n",
    "                      kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.UpsamplingNearest2d(size=(28, 28)),\n",
    "            nn.Conv2d(in_channels=hidden_size, out_channels=in_channels,\n",
    "                      kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.decoder(inputs)\n",
    "        \n",
    "class AE(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_size=64, out_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(in_channels=in_channels, hidden_size=hidden_size, out_channels=out_channels)\n",
    "        self.decoder = Decoder(in_channels=in_channels, hidden_size=hidden_size, out_channels=out_channels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        print(inputs.shape)\n",
    "        embeddings = self.encoder(inputs.view(-1, *inputs.shape[-3:]))\n",
    "        print(embeddings.shape)\n",
    "        recons = self.decoder(embeddings.unsqueeze(-1).unsqueeze(-1))\n",
    "        return embeddings.view(*inputs.shape[:-3], -1), recons.view(*inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProtoCLR(\n",
    "    n_support=1, n_query=3, batch_size=50, distance='cosine', Ï„=.5,\n",
    "    num_input_channels=1, decoder_class=Decoder, encoder_class=Encoder,\n",
    "    lr_decay_step=25000, lr_decay_rate=.5, ae=True, gamma=1., log_images=True)\n",
    "dataset_train = UnlabelledDataset(\n",
    "    dataset='omniglot',\n",
    "    datapath='./data/',\n",
    "    split='train',\n",
    "    n_support=1,\n",
    "    n_query=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(), # Image grid to single feature vector\n",
    "            nn.Linear(2*16*c_hid, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2*16*c_hid),\n",
    "            act_fn()\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 => 32x32\n",
    "            nn.Tanh() # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_channel_size: int,\n",
    "                 latent_dim: int,\n",
    "                 encoder_class : object = Encoder,\n",
    "                 decoder_class : object = Decoder,\n",
    "                 num_input_channels: int = 1,\n",
    "                 width: int = 28,\n",
    "                 height: int = 28):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        # Example input array needed for visualizing the graph of the network\n",
    "        self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward function takes in an image and returns the reconstructed image\n",
    "        \"\"\"\n",
    "        # print(x.shape)\n",
    "        z = self.encoder(x.view(-1, *x.shape[-3:]))\n",
    "        x_hat = self.decoder(z)\n",
    "        # z = self.encoder(x)\n",
    "        # x_hat = self.decoder(z)\n",
    "        return x_hat.view(*x.shape)\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch, ways, n_supp, n_query):\n",
    "        \"\"\"\n",
    "        Given a batch of images, this function returns the reconstruction loss (MSE in our case)\n",
    "        \"\"\"\n",
    "        x, _ = batch # We do not need the labels\n",
    "        x_hat = self.forward(x)\n",
    "\n",
    "        x_supp = x[:,:ways * n_supp]\n",
    "        r_supp = x_hat[:,:ways * n_supp]\n",
    "        r_query = x_hat[:,ways * n_supp:]\n",
    "        # print(\"####\", r_supp.shape, r_query.shape)\n",
    "\n",
    "        r_query = r_query.view(1, r_supp.shape[1], 3, 1, 28, 28)\n",
    "\n",
    "\n",
    "\n",
    "        # loss = F.mse_loss(x.squeeze(0), x_hat, reduction='none').sum(dim=[1, 2, 3,]).mean(dim=[0])\n",
    "        loss = F.mse_loss(\n",
    "                r_query,\n",
    "                torch.broadcast_to(x_supp.unsqueeze(2), r_query.shape),\n",
    "                reduction='none').sum(dim=[1, 2, 3, 4, 5]).mean(dim=[0])\n",
    "        # loss = F.mse_loss(\n",
    "        #         r_supp, \n",
    "        #         x_supp,\n",
    "        #         reduction='none').sum(dim=[1, 2, 3, 4,]).mean(dim=[0])\n",
    "\n",
    "        # loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        # loss = loss.sum(dim=[1,2,3]).mean(dim=[0])\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                         mode='min',\n",
    "                                                         factor=0.2,\n",
    "                                                         patience=20,\n",
    "                                                         min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"train_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data = batch['data'].to(self.device) # [batch_size x ways x shots x image_dim]\n",
    "        data = data.unsqueeze(0)\n",
    "        # e.g. 50 images, 2 support, 2 query, miniImageNet: torch.Size([1, 50, 4, 3, 84, 84])\n",
    "        batch_size = data.size(0)\n",
    "        ways = data.size(1)\n",
    "        x_support = data[:,:,:1]\n",
    "        x_support = x_support.reshape((batch_size, ways * 1, *x_support.shape[-3:])) # e.g. [1,50*n_support,*(3,84,84)]\n",
    "        x_query = data[:,:,1:]\n",
    "        x_query = x_query.reshape((batch_size, ways * 3, *x_query.shape[-3:])) # e.g. [1,50*n_query,*(3,84,84)]\n",
    "        # print(f'!!!! {x_support.shape}')\n",
    "        # Create dummy query labels\n",
    "        y_query = torch.arange(ways).unsqueeze(0).unsqueeze(2) # batch and shot dim\n",
    "        y_query = y_query.repeat(batch_size, 1, 1)\n",
    "        y_query = y_query.view(batch_size, -1).to('cuda')\n",
    "\n",
    "        y_support = torch.arange(ways).unsqueeze(0).unsqueeze(2) # batch and shot dim\n",
    "        y_support = y_support.repeat(batch_size, 1, 1)\n",
    "        y_support = y_support.view(batch_size, -1).to('cuda')\n",
    "        x = torch.cat([x_support, x_query], 1) # e.g. [1,50*(n_support+n_query),*(3,84,84)]\n",
    "\n",
    "        loss = self._get_reconstruction_loss((x, y_support), ways, 1, 3)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     loss = self._get_reconstruction_loss(batch)\n",
    "    #     self.log('val_loss', loss)\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     loss = self._get_reconstruction_loss(batch)\n",
    "    #     self.log('test_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_i = torch.tensor([[ 6.6270, 11.6813],\n",
    "        [ 6.4168,  7.7024],\n",
    "        [ 7.9163,  8.0348],\n",
    "        [ 4.6681,  7.2211],\n",
    "        [ 5.5762, 10.0012]])\n",
    "\n",
    "emb_j = torch.tensor([[ 6.3032, 10.0734],\n",
    "        [ 6.9568,  9.8194],\n",
    "        [ 8.8078,  8.1451],\n",
    "        [ 4.3386,  8.5691],\n",
    "        [ 7.0590,  6.7981],\n",
    "        [ 5.4431,  8.0148],\n",
    "        [ 5.0362,  6.1590],\n",
    "        [ 5.1802, 10.7874],\n",
    "        [ 7.7798,  7.3308],\n",
    "        [ 5.4246,  6.2691],\n",
    "        [ 5.8096, 10.1320],\n",
    "        [ 5.6292, 10.5785],\n",
    "        [ 4.9312,  5.8408],\n",
    "        [ 4.3520,  6.3029],\n",
    "        [ 5.4147,  9.8841],\n",
    "        [ 6.2309,  8.4320],\n",
    "        [ 6.0696,  8.4501],\n",
    "        [ 6.0402,  6.7177],\n",
    "        [ 4.5353,  8.5051],\n",
    "        [ 5.2821,  8.1228],\n",
    "        [ 4.8968,  7.7489],\n",
    "        [ 5.6221,  6.7439],\n",
    "        [ 7.4615,  7.0588],\n",
    "        [ 7.2516,  7.3432],\n",
    "        [ 6.0943,  9.1108],\n",
    "        [ 6.3836,  7.8931],\n",
    "        [ 4.9303,  6.2700],\n",
    "        [ 8.0341,  7.9363],\n",
    "        [ 5.0337, 10.5701],\n",
    "        [ 6.0564, 11.4888],\n",
    "        [ 6.2713,  8.1638],\n",
    "        [ 6.0115,  8.6252],\n",
    "        [ 8.0858,  8.3572],\n",
    "        [ 7.4930,  7.2508],\n",
    "        [ 5.3719,  6.0493],\n",
    "        [ 8.1590,  7.5640],\n",
    "        [ 6.8972,  7.4481],\n",
    "        [ 5.8029,  6.7042],\n",
    "        [ 6.5177,  8.4637],\n",
    "        [ 6.5807,  6.9593],\n",
    "        [ 7.0109,  9.6658],\n",
    "        [ 3.9612,  6.7887],\n",
    "        [ 7.0019, 12.0692],\n",
    "        [ 6.6533, 11.9660],\n",
    "        [ 6.8464, 12.0303],\n",
    "        [ 5.3894,  7.2251],\n",
    "        [ 6.4877, 11.8622],\n",
    "        [ 5.7942,  9.3818],\n",
    "        [ 8.7622,  8.5982],\n",
    "        [ 4.9367, 10.4112],\n",
    "        [ 8.2535,  7.8129],\n",
    "        [ 4.8408, 10.0947],\n",
    "        [ 5.4412, 10.4591],\n",
    "        [ 6.0605, 10.4132],\n",
    "        [ 6.6809,  8.1679],\n",
    "        [ 5.1636,  8.1599],\n",
    "        [ 6.0977,  8.0572],\n",
    "        [ 7.5062,  7.4989],\n",
    "        [ 6.5523,  7.0610],\n",
    "        [ 8.6864,  7.8542],\n",
    "        [ 7.8051,  9.0486],\n",
    "        [ 6.4663,  9.5589],\n",
    "        [ 8.1567,  7.3624],\n",
    "        [ 6.8541,  7.5712],\n",
    "        [ 4.4631,  7.2664],\n",
    "        [ 7.7564,  7.6146],\n",
    "        [ 4.2792,  8.4721],\n",
    "        [ 5.6382,  8.5117],\n",
    "        [ 4.7060,  7.7046],\n",
    "        [ 7.2034,  8.9113],\n",
    "        [ 8.4993,  7.6508],\n",
    "        [ 6.4934,  9.8171],\n",
    "        [ 7.7694,  8.6558],\n",
    "        [ 6.1207,  6.5721],\n",
    "        [ 5.0343,  7.8402],\n",
    "        [ 7.4249,  7.1609],\n",
    "        [ 4.5241,  6.9034],\n",
    "        [ 6.5902,  7.3029],\n",
    "        [ 6.9436, 12.0691],\n",
    "        [ 6.6224, 11.9718],\n",
    "        [ 6.5599, 12.0108],\n",
    "        [ 6.4960,  8.6471],\n",
    "        [ 4.2922,  7.0729],\n",
    "        [ 4.2204,  6.7476],\n",
    "        [ 7.2825,  8.0695],\n",
    "        [ 5.0677,  9.7671],\n",
    "        [ 4.0716,  7.9130],\n",
    "        [ 7.6866,  7.9514],\n",
    "        [ 6.5860, 10.6934],\n",
    "        [ 6.4237,  7.7053],\n",
    "        [ 4.9411,  7.0874],\n",
    "        [ 6.1804,  7.2141],\n",
    "        [ 5.3615,  6.0960],\n",
    "        [ 4.8339,  7.1372],\n",
    "        [ 6.9698,  8.4045],\n",
    "        [ 7.0948,  7.6626],\n",
    "        [ 5.8325,  7.4330],\n",
    "        [ 4.1195,  7.4736],\n",
    "        [ 4.3055,  7.2621],\n",
    "        [ 6.2035, 11.6645],\n",
    "        [ 6.8866,  6.4410],\n",
    "        [ 5.5439, 10.7403],\n",
    "        [ 6.3185,  7.0267],\n",
    "        [ 6.6733,  7.9577],\n",
    "        [ 5.0281,  6.9473],\n",
    "        [ 6.2958, 10.8109],\n",
    "        [ 5.4935, 10.6249],\n",
    "        [ 5.3473, 10.2931],\n",
    "        [ 8.0300,  7.0989],\n",
    "        [ 8.0298,  7.2092],\n",
    "        [ 8.5735,  8.2420],\n",
    "        [ 4.6567,  8.3166],\n",
    "        [ 4.4944,  8.3375],\n",
    "        [ 4.2823,  6.1382],\n",
    "        [ 6.2566, 10.9357],\n",
    "        [ 6.1064, 10.7109],\n",
    "        [ 6.1095, 10.9985],\n",
    "        [ 4.6992,  5.9350],\n",
    "        [ 4.6401,  8.5465],\n",
    "        [ 4.8380,  6.1790],\n",
    "        [ 4.3196,  7.8004],\n",
    "        [ 4.9383,  9.5967],\n",
    "        [ 7.2132,  8.6334],\n",
    "        [ 8.6895,  7.8514],\n",
    "        [ 7.7159,  7.6624],\n",
    "        [ 8.1368,  7.7046],\n",
    "        [ 6.6882,  8.9046],\n",
    "        [ 7.6113,  8.5579],\n",
    "        [ 4.9312,  8.7897],\n",
    "        [ 6.9077,  7.7897],\n",
    "        [ 8.3960,  8.0787],\n",
    "        [ 7.5751,  9.3934],\n",
    "        [ 5.2499,  9.0045],\n",
    "        [ 5.8971, 10.0067],\n",
    "        [ 4.7151, 10.0582],\n",
    "        [ 6.7354,  7.5874],\n",
    "        [ 6.4488,  8.1458],\n",
    "        [ 5.1967,  7.3349],\n",
    "        [ 4.6785,  6.5824],\n",
    "        [ 4.0479,  7.6312],\n",
    "        [ 6.2214,  6.7932],\n",
    "        [ 5.4997, 10.0227],\n",
    "        [ 5.1693,  6.3969],\n",
    "        [ 5.0809, 10.1422],\n",
    "        [ 4.8012,  9.8925],\n",
    "        [ 6.9810,  8.2349],\n",
    "        [ 8.0005,  8.0630],\n",
    "        [ 6.9695, 11.8812],\n",
    "        [ 5.6798, 10.3008],\n",
    "        [ 6.1110, 10.3863]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_i = torch.tensor([[0.4934, 0.8698],\n",
    "        [0.6401, 0.7683],\n",
    "        [0.7018, 0.7123],\n",
    "        [0.5429, 0.8398],\n",
    "        [0.4870, 0.8734]])\n",
    "\n",
    "z_j = torch.tensor([[0.5304, 0.8477],\n",
    "        [0.5781, 0.8160],\n",
    "        [0.7342, 0.6789],\n",
    "        [0.4517, 0.8922],\n",
    "        [0.7203, 0.6937],\n",
    "        [0.5618, 0.8273],\n",
    "        [0.6330, 0.7741],\n",
    "        [0.4329, 0.9014],\n",
    "        [0.7278, 0.6858],\n",
    "        [0.6543, 0.7562],\n",
    "        [0.4974, 0.8675],\n",
    "        [0.4698, 0.8828],\n",
    "        [0.6451, 0.7641],\n",
    "        [0.5682, 0.8229],\n",
    "        [0.4804, 0.8770],\n",
    "        [0.5943, 0.8042],\n",
    "        [0.5834, 0.8122],\n",
    "        [0.6686, 0.7436],\n",
    "        [0.4705, 0.8824],\n",
    "        [0.5452, 0.8383],\n",
    "        [0.5342, 0.8454],\n",
    "        [0.6403, 0.7681],\n",
    "        [0.7264, 0.6872],\n",
    "        [0.7027, 0.7115],\n",
    "        [0.5560, 0.8312],\n",
    "        [0.6288, 0.7775],\n",
    "        [0.6181, 0.7861],\n",
    "        [0.7114, 0.7028],\n",
    "        [0.4300, 0.9028],\n",
    "        [0.4663, 0.8846],\n",
    "        [0.6092, 0.7930],\n",
    "        [0.5718, 0.8204],\n",
    "        [0.6953, 0.7187],\n",
    "        [0.7186, 0.6954],\n",
    "        [0.6640, 0.7477],\n",
    "        [0.7333, 0.6799],\n",
    "        [0.6795, 0.7337],\n",
    "        [0.6544, 0.7561],\n",
    "        [0.6101, 0.7923],\n",
    "        [0.6871, 0.7266],\n",
    "        [0.5871, 0.8095],\n",
    "        [0.5040, 0.8637],\n",
    "        [0.5018, 0.8650],\n",
    "        [0.4859, 0.8740],\n",
    "        [0.4946, 0.8691],\n",
    "        [0.5979, 0.8016],\n",
    "        [0.4798, 0.8774],\n",
    "        [0.5255, 0.8508],\n",
    "        [0.7138, 0.7004],\n",
    "        [0.4284, 0.9036],\n",
    "        [0.7262, 0.6875],\n",
    "        [0.4324, 0.9017],\n",
    "        [0.4615, 0.8871],\n",
    "        [0.5030, 0.8643],\n",
    "        [0.6331, 0.7740],\n",
    "        [0.5347, 0.8450],\n",
    "        [0.6035, 0.7974],\n",
    "        [0.7075, 0.7068],\n",
    "        [0.6802, 0.7330],\n",
    "        [0.7417, 0.6707],\n",
    "        [0.6532, 0.7572],\n",
    "        [0.5603, 0.8283],\n",
    "        [0.7423, 0.6700],\n",
    "        [0.6711, 0.7413],\n",
    "        [0.5234, 0.8521],\n",
    "        [0.7136, 0.7006],\n",
    "        [0.4508, 0.8926],\n",
    "        [0.5522, 0.8337],\n",
    "        [0.5213, 0.8534],\n",
    "        [0.6286, 0.7777],\n",
    "        [0.7432, 0.6690],\n",
    "        [0.5517, 0.8341],\n",
    "        [0.6680, 0.7442],\n",
    "        [0.6815, 0.7318],\n",
    "        [0.5403, 0.8415],\n",
    "        [0.7198, 0.6942],\n",
    "        [0.5481, 0.8364],\n",
    "        [0.6700, 0.7424],\n",
    "        [0.4987, 0.8668],\n",
    "        [0.4840, 0.8750],\n",
    "        [0.4793, 0.8776],\n",
    "        [0.6006, 0.7995],\n",
    "        [0.5188, 0.8549],\n",
    "        [0.5303, 0.8478],\n",
    "        [0.6700, 0.7424],\n",
    "        [0.4606, 0.8876],\n",
    "        [0.4575, 0.8892],\n",
    "        [0.6950, 0.7190],\n",
    "        [0.5244, 0.8515],\n",
    "        [0.6403, 0.7681],\n",
    "        [0.5719, 0.8203],\n",
    "        [0.6506, 0.7594],\n",
    "        [0.6604, 0.7509],\n",
    "        [0.5608, 0.8280],\n",
    "        [0.6383, 0.7697],\n",
    "        [0.6794, 0.7338],\n",
    "        [0.6173, 0.7867],\n",
    "        [0.4827, 0.8758],\n",
    "        [0.5100, 0.8602],\n",
    "        [0.4695, 0.8829],\n",
    "        [0.7303, 0.6831],\n",
    "        [0.4587, 0.8886],\n",
    "        [0.6686, 0.7436],\n",
    "        [0.6426, 0.7662],\n",
    "        [0.5863, 0.8101],\n",
    "        [0.5032, 0.8641],\n",
    "        [0.4593, 0.8883],\n",
    "        [0.4610, 0.8874],\n",
    "        [0.7492, 0.6623],\n",
    "        [0.7441, 0.6681],\n",
    "        [0.7209, 0.6930],\n",
    "        [0.4886, 0.8725],\n",
    "        [0.4745, 0.8803],\n",
    "        [0.5722, 0.8201],\n",
    "        [0.4966, 0.8680],\n",
    "        [0.4953, 0.8687],\n",
    "        [0.4856, 0.8742],\n",
    "        [0.6208, 0.7840],\n",
    "        [0.4771, 0.8788],\n",
    "        [0.6165, 0.7874],\n",
    "        [0.4844, 0.8748],\n",
    "        [0.4576, 0.8892],\n",
    "        [0.6412, 0.7674],\n",
    "        [0.7420, 0.6704],\n",
    "        [0.7096, 0.7046],\n",
    "        [0.7261, 0.6876],\n",
    "        [0.6006, 0.7996],\n",
    "        [0.6646, 0.7472],\n",
    "        [0.4893, 0.8721],\n",
    "        [0.6635, 0.7482],\n",
    "        [0.7206, 0.6934],\n",
    "        [0.6277, 0.7784],\n",
    "        [0.5037, 0.8639],\n",
    "        [0.5077, 0.8615],\n",
    "        [0.4245, 0.9054],\n",
    "        [0.6639, 0.7478],\n",
    "        [0.6207, 0.7840],\n",
    "        [0.5781, 0.8160],\n",
    "        [0.5793, 0.8151],\n",
    "        [0.4686, 0.8834],\n",
    "        [0.6754, 0.7375],\n",
    "        [0.4811, 0.8767],\n",
    "        [0.6285, 0.7778],\n",
    "        [0.4479, 0.8941],\n",
    "        [0.4366, 0.8996],\n",
    "        [0.6466, 0.7628],\n",
    "        [0.7043, 0.7099],\n",
    "        [0.5060, 0.8625],\n",
    "        [0.4829, 0.8757],\n",
    "        [0.5071, 0.8619]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(z_i @ z_j.T).t().contiguous().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.cat([z_i, z_j], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = F.cosine_similarity(z_i.unsqueeze(1), z_j.unsqueeze(0), dim=2) / .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = sim.t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_i_j = torch.diag(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_i_j = sim[0, 5-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_i_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = torch.exp(sim_i_j)\n",
    "numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = torch.sum(\n",
    "    torch.ones((5, )).scatter_(0, torch.tensor([0]), 0.0).bool() * torch.exp(sim[0,:])\n",
    ")\n",
    "denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ij = -torch.log(numerator / denominator)\n",
    "loss_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### for ji?\n",
    "\n",
    "sim_i_j_2 = sim[0 + 5- 1, 0]\n",
    "sim_i_j_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator2 = torch.exp(sim_i_j_2)\n",
    "numerator2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator2 = torch.sum(\n",
    "    torch.ones((5, )).scatter_(0, torch.tensor([4]), 0.0).bool() * torch.exp(sim[4,:])\n",
    ")\n",
    "denominator2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ij2 = -torch.log(numerator2 / denominator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ij, loss_ij2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_ij(i, j):\n",
    "    # shape of sim initially is (n_clusters, n_aug_images)\n",
    "    # so we have (5, 150) - similarity between each cluster and augmented image\n",
    "    # transposed it becomes (150, 5) - similarity between each augmented image and cluster\n",
    "    sim = F.cosine_similarity(z_i.unsqueeze(1), z_j.unsqueeze(0), dim=2) / .5\n",
    "    sim = sim.t().contiguous()\n",
    "\n",
    "    # numerator math\n",
    "    sim_i_j = sim[i, j]\n",
    "    print(f\"sim({i}, {j})={sim_i_j}\")\n",
    "\n",
    "    numerator = torch.exp(sim_i_j)\n",
    "    print(\"Numerator\", numerator)\n",
    "    \n",
    "\n",
    "    # denominator math\n",
    "    # because there are 5 classes\n",
    "\n",
    "    mask = torch.ones((5, )).scatter(0, torch.tensor([i]), 0.0).bool()\n",
    "    print(f\"1{{k!={i}}}\", mask)\n",
    "    denominator = torch.sum(\n",
    "        mask * torch.exp(sim[i,:])\n",
    "    )\n",
    "    print(\"Denominator\", denominator)\n",
    "    loss_ij = -torch.log(numerator / denominator)\n",
    "    print(f\"loss({i},{j})={loss_ij}\\n\")\n",
    "    return loss_ij.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 # n_clusters\n",
    "loss = 0.\n",
    "for k in range(0, N):\n",
    "    loss += l_ij(k, k + N - 1) + l_ij(k + N - 1, k)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 5\n",
    "N = 2 * class_num\n",
    "mask = torch.ones((N, N))\n",
    "mask = mask.fill_diagonal_(0)\n",
    "for i in range(class_num):\n",
    "    mask[i, class_num + i] = 0\n",
    "    mask[class_num + i, i] = 0\n",
    "    mask = mask.bool()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i = z_i.sum(0).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_i.unsqueeze(1).shape, z_j.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dists = torch.sum((z_i.unsqueeze(1) - z_j.unsqueeze(0))** 2, dim=-1)\n",
    "dists = F.cosine_similarity(z_i.unsqueeze(1), z_j.unsqueeze(0), dim=2) / 0.5\n",
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_labels = torch.tensor([[1, 1, 4, 4, 4, 1, 0, 3, 4, 4, 2, 3, 4, 4, 1, 2, 1, 0, 4, 0, 0, 2,\n",
    "       1, 4, 3, 2, 1, 4, 3, 2, 0, 1, 4, 0, 0, 0, 4, 1, 0, 0, 0, 1, 0, 4,\n",
    "       2, 2, 4, 3, 0, 0, 1, 1, 1, 2, 1, 2, 0, 4, 0, 0, 2, 4, 2, 2, 4, 1,\n",
    "       3, 3, 4, 3, 0, 3, 3, 2, 0, 4, 4, 4, 4, 4, 3, 2, 0, 3, 1, 3, 4, 4,\n",
    "       4, 3, 2, 3, 1, 1, 2, 1, 3, 3, 3, 2, 1, 0, 4, 3, 4, 4, 4, 3, 4, 4,\n",
    "       0, 0, 0, 4, 4, 0, 2, 2, 2, 4, 4, 4, 1, 1, 2, 2, 2, 1, 0, 1, 4, 4,\n",
    "       3, 3, 3, 3, 2, 3, 3, 1, 2, 2, 1, 1, 1, 1, 4, 3, 0, 0, 0, 1, 1, 3,\n",
    "       0, 3, 4, 3, 4, 4, 2, 0, 2, 0, 3, 3, 0, 0, 2, 2, 4, 0, 0, 1, 1, 2,\n",
    "       2, 1, 1, 0, 0, 4, 3, 2, 2, 3, 0, 0, 1, 4, 2, 2, 2, 2, 3, 4, 1, 4,\n",
    "       0, 1]])\n",
    "query_labels = raw_labels[:, 50 * 1 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augtarget = torch.tensor([[ 0,  0,  0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  5,  5,\n",
    "          6,  6,  6,  7,  7,  7,  8,  8,  8,  9,  9,  9, 10, 10, 10, 11, 11, 11,\n",
    "         12, 12, 12, 13, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 16, 17, 17, 17,\n",
    "         18, 18, 18, 19, 19, 19, 20, 20, 20, 21, 21, 21, 22, 22, 22, 23, 23, 23,\n",
    "         24, 24, 24, 25, 25, 25, 26, 26, 26, 27, 27, 27, 28, 28, 28, 29, 29, 29,\n",
    "         30, 30, 30, 31, 31, 31, 32, 32, 32, 33, 33, 33, 34, 34, 34, 35, 35, 35,\n",
    "         36, 36, 36, 37, 37, 37, 38, 38, 38, 39, 39, 39, 40, 40, 40, 41, 41, 41,\n",
    "         42, 42, 42, 43, 43, 43, 44, 44, 44, 45, 45, 45, 46, 46, 46, 47, 47, 47,\n",
    "         48, 48, 48, 49, 49, 49]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(dists.unsqueeze(0), torch.Tensor([[0 for i in range(150)]]).long(), reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(dists.unsqueeze(0), query_labels, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_labels.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rz = torch.load('data/raw_embeddings.pt').cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rz.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = umap.UMAP(n_components=3, random_state=42).fit(rz.detach().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrz = mapper.transform(rz.detach().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrz_support = mrz[: 50 * 1]\n",
    "# e.g. [1,50*n_query,*(3,84,84)]\n",
    "mrz_query = mrz[50 * 1 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = cluster.KMeans(n_clusters=5)\n",
    "pred_labels = clf.fit_predict(mrz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = clf.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = torch.from_numpy(mapper.inverse_transform(centroids)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 64])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rz_query = rz[:, 50:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 64]), torch.Size([1, 150, 64]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.shape, rz_query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 150]), torch.Size([1, 150]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = cosine_similarity(centroids, rz_query)\n",
    "dists.shape, torch.from_numpy(pred_labels[50:]).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5604, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(dists, torch.from_numpy(pred_labels[50:]).unsqueeze(0).long(), reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_xent_loss(z_i, z_j, query_labels, temperature=.5, reduction='mean'):\n",
    "    N = z_j.shape[1]\n",
    "    # calculating distance from every centroid to every augmented image\n",
    "    dists = cosine_similarity(z_i, z_j) / temperature\n",
    "    labels = torch.zeros(N).to(dists.device).long()\n",
    "    print(dists.shape, query_labels.shape)\n",
    "    loss = F.cross_entropy(dists, labels.unsqueeze(0), reduction=reduction)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 150]) torch.Size([150])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.6147, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt_xent_loss(centroids, rz_query, torch.from_numpy(pred_labels[50:]).long(), temperature=1., reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e21c1a8776696032396b5307c1822f52f16abb97709b3cd46d099050af66c34"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
