{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5617368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmeta\n",
    "from PIL import Image\n",
    "from torchmeta.datasets.helpers import omniglot\n",
    "from functools import reduce\n",
    "from operator import __add__\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from unsupervised_meta_learning.pl_dataloaders import get_omniglot_transform, OracleOmniglot\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ccfb093",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = [28, 28]\n",
    "dataset = torchmeta.datasets.Omniglot('./data/untarred',\n",
    "                                      num_classes_per_task=5,\n",
    "                                      transform=Compose([Resize(28), ToTensor()]),\n",
    "                                      meta_train=True,\n",
    "                                      use_vinyals_split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ea5d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = OracleOmniglot(\"omniglot\", \"./data/untarred/\", split='train',\n",
    "                    train_oracle_mode=True, n_query=3, train_oracle_shots=5, train_oracle_ways=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6415e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2816b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchmeta.transforms.ClassSplitter(dataset,\n",
    "                                             shuffle=True,\n",
    "                                             num_train_per_class=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ad39a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torchmeta.utils.data.BatchMetaDataLoader(dataset, batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e51a902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9474438804480"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3915c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cdb8d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = omniglot(\"./data/untarred\", ways=10, shots=5, use_vinyals_split=True,meta_train=True, download=True)\n",
    "dataloader = torchmeta.utils.data.BatchMetaDataLoader(dataset, batch_size=1, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e876c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ojass/anaconda3/envs/ai/lib/python3.9/site-packages/torchvision/transforms/functional.py:991: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/ai/lib/python3.9/site-packages/torchvision/transforms/functional.py:991: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/ai/lib/python3.9/site-packages/torchvision/transforms/functional.py:991: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/ai/lib/python3.9/site-packages/torchvision/transforms/functional.py:991: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "xs = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4327ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 50, 1, 28, 28]),\n",
       " tensor([[6, 6, 6, 6, 6, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 8, 8, 8, 8,\n",
       "          8, 2, 2, 2, 2, 2, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 3, 3, 3,\n",
       "          3, 3]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs['train'][0].shape, xs['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32f67550",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xs['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "746700e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 28, 28, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.squeeze(0).permute(0, 2, 3, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cb8edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = list(map(lambda x: Image.fromarray(x), x.squeeze().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55adbb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8931fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 1, 28, 28])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99626618",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pic should be 2/3 dimensional. Got 5 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_60063/2378139827.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pic should be 2/3 dimensional. Got {} dimensions.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 5 dimensions."
     ]
    }
   ],
   "source": [
    "transforms.ToPILImage()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fa53f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_descriptor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f9892479",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "targets  = []\n",
    "cntr = 0\n",
    "for xs in dataloader:\n",
    "    if cntr > 5000:\n",
    "        break\n",
    "    else:\n",
    "        cntr += 1\n",
    "    d, t = xs['train']\n",
    "    data.append(d.squeeze(0))\n",
    "    targets.append(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7c0321d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25005, 1, 28, 28])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bfb0ea76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 1, 2, 0, 4],\n",
       "        [0, 3, 1, 4, 2],\n",
       "        [4, 0, 1, 3, 2],\n",
       "        ...,\n",
       "        [3, 2, 1, 4, 0],\n",
       "        [4, 3, 2, 1, 0],\n",
       "        [4, 3, 1, 0, 2]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a94a7ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 0,  ..., 2, 3, 1])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(targets).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05059bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = UnlabelledDataModule(\n",
    "    \"omniglot\",\n",
    "    \"./data/untarred\",\n",
    "    split=\"train\",\n",
    "    transform=None,\n",
    "    n_support=1,\n",
    "    n_query=3,\n",
    "    n_images=None,\n",
    "    n_classes=None,\n",
    "    batch_size=50,\n",
    "    seed=10,\n",
    "    mode=\"trainval\",\n",
    "    num_workers=0,\n",
    "    eval_ways=5,\n",
    "    eval_support_shots=1,\n",
    "    eval_query_shots=15,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5b9b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 100\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "dataset_train = UnlabelledDataset(\n",
    "            'omniglot',\n",
    "            'data/untarred/',\n",
    "            split=\"train\",\n",
    "            transform=None,\n",
    "            n_images=None,\n",
    "            n_classes=5,\n",
    "            n_support=1,\n",
    "            n_query=3,\n",
    "            no_aug_support=False,\n",
    "            no_aug_query=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb9e258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff3a955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d1d1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028 20560\n",
      "<class 'numpy.ndarray'>\n",
      "172 3440\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902bb84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    profiler='simple',\n",
    "    max_epochs=2,\n",
    "    limit_train_batches=100,\n",
    "    fast_dev_run=False,\n",
    "    limit_val_batches=15,\n",
    "    limit_test_batches=600,\n",
    "    num_sanity_val_steps=2, gpus=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_sizes = (3, 3)\n",
    "conv_padding = reduce(__add__, [(k // 2 + (k - 2 * (k // 2)) - 1, k // 2) for k in kernel_sizes[::-1]])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_size=64, out_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # nn.ZeroPad2d(conv_padding),\n",
    "            nn.Conv2d(in_channels, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14 x 14\n",
    "\n",
    "            # nn.ZeroPad2d(conv_padding),\n",
    "            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 7x7\n",
    "\n",
    "            # nn.ZeroPad2d(conv_padding),\n",
    "            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 3x3\n",
    "\n",
    "            # nn.ZeroPad2d(conv_padding),\n",
    "            nn.Conv2d(hidden_size, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 1x1\n",
    "            # nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.encoder(inputs)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_size=64, out_channels=64):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.UpsamplingNearest2d(size=(4, 4)),\n",
    "            nn.Conv2d(in_channels=out_channels,\n",
    "                      out_channels=hidden_size, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.UpsamplingNearest2d(size=(7, 7)),\n",
    "            nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size,\n",
    "                      kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.UpsamplingNearest2d(size=(14, 14)),\n",
    "            nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size,\n",
    "                      kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.UpsamplingNearest2d(size=(28, 28)),\n",
    "            nn.Conv2d(in_channels=hidden_size, out_channels=in_channels,\n",
    "                      kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.decoder(inputs)\n",
    "        \n",
    "class AE(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_size=64, out_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(in_channels=in_channels, hidden_size=hidden_size, out_channels=out_channels)\n",
    "        self.decoder = Decoder(in_channels=in_channels, hidden_size=hidden_size, out_channels=out_channels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        print(inputs.shape)\n",
    "        embeddings = self.encoder(inputs.view(-1, *inputs.shape[-3:]))\n",
    "        print(embeddings.shape)\n",
    "        recons = self.decoder(embeddings.unsqueeze(-1).unsqueeze(-1))\n",
    "        return embeddings.view(*inputs.shape[:-3], -1), recons.view(*inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2072c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProtoCLR(\n",
    "    n_support=1, n_query=3, batch_size=50, distance='cosine', τ=.5,\n",
    "    num_input_channels=1, decoder_class=Decoder, encoder_class=Encoder,\n",
    "    lr_decay_step=25000, lr_decay_rate=.5, ae=True, gamma=1., log_images=True)\n",
    "dataset_train = UnlabelledDataset(\n",
    "    dataset='omniglot',\n",
    "    datapath='./data/',\n",
    "    split='train',\n",
    "    n_support=1,\n",
    "    n_query=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caddb59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(), # Image grid to single feature vector\n",
    "            nn.Linear(2*16*c_hid, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b10fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2*16*c_hid),\n",
    "            act_fn()\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 => 32x32\n",
    "            nn.Tanh() # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_channel_size: int,\n",
    "                 latent_dim: int,\n",
    "                 encoder_class : object = Encoder,\n",
    "                 decoder_class : object = Decoder,\n",
    "                 num_input_channels: int = 1,\n",
    "                 width: int = 28,\n",
    "                 height: int = 28):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        # Example input array needed for visualizing the graph of the network\n",
    "        self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward function takes in an image and returns the reconstructed image\n",
    "        \"\"\"\n",
    "        # print(x.shape)\n",
    "        z = self.encoder(x.view(-1, *x.shape[-3:]))\n",
    "        x_hat = self.decoder(z)\n",
    "        # z = self.encoder(x)\n",
    "        # x_hat = self.decoder(z)\n",
    "        return x_hat.view(*x.shape)\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch, ways, n_supp, n_query):\n",
    "        \"\"\"\n",
    "        Given a batch of images, this function returns the reconstruction loss (MSE in our case)\n",
    "        \"\"\"\n",
    "        x, _ = batch # We do not need the labels\n",
    "        x_hat = self.forward(x)\n",
    "\n",
    "        x_supp = x[:,:ways * n_supp]\n",
    "        r_supp = x_hat[:,:ways * n_supp]\n",
    "        r_query = x_hat[:,ways * n_supp:]\n",
    "        # print(\"####\", r_supp.shape, r_query.shape)\n",
    "\n",
    "        r_query = r_query.view(1, r_supp.shape[1], 3, 1, 28, 28)\n",
    "\n",
    "\n",
    "\n",
    "        # loss = F.mse_loss(x.squeeze(0), x_hat, reduction='none').sum(dim=[1, 2, 3,]).mean(dim=[0])\n",
    "        loss = F.mse_loss(\n",
    "                r_query,\n",
    "                torch.broadcast_to(x_supp.unsqueeze(2), r_query.shape),\n",
    "                reduction='none').sum(dim=[1, 2, 3, 4, 5]).mean(dim=[0])\n",
    "        # loss = F.mse_loss(\n",
    "        #         r_supp, \n",
    "        #         x_supp,\n",
    "        #         reduction='none').sum(dim=[1, 2, 3, 4,]).mean(dim=[0])\n",
    "\n",
    "        # loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        # loss = loss.sum(dim=[1,2,3]).mean(dim=[0])\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                         mode='min',\n",
    "                                                         factor=0.2,\n",
    "                                                         patience=20,\n",
    "                                                         min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"train_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data = batch['data'].to(self.device) # [batch_size x ways x shots x image_dim]\n",
    "        data = data.unsqueeze(0)\n",
    "        # e.g. 50 images, 2 support, 2 query, miniImageNet: torch.Size([1, 50, 4, 3, 84, 84])\n",
    "        batch_size = data.size(0)\n",
    "        ways = data.size(1)\n",
    "        x_support = data[:,:,:1]\n",
    "        x_support = x_support.reshape((batch_size, ways * 1, *x_support.shape[-3:])) # e.g. [1,50*n_support,*(3,84,84)]\n",
    "        x_query = data[:,:,1:]\n",
    "        x_query = x_query.reshape((batch_size, ways * 3, *x_query.shape[-3:])) # e.g. [1,50*n_query,*(3,84,84)]\n",
    "        # print(f'!!!! {x_support.shape}')\n",
    "        # Create dummy query labels\n",
    "        y_query = torch.arange(ways).unsqueeze(0).unsqueeze(2) # batch and shot dim\n",
    "        y_query = y_query.repeat(batch_size, 1, 1)\n",
    "        y_query = y_query.view(batch_size, -1).to('cuda')\n",
    "\n",
    "        y_support = torch.arange(ways).unsqueeze(0).unsqueeze(2) # batch and shot dim\n",
    "        y_support = y_support.repeat(batch_size, 1, 1)\n",
    "        y_support = y_support.view(batch_size, -1).to('cuda')\n",
    "        x = torch.cat([x_support, x_query], 1) # e.g. [1,50*(n_support+n_query),*(3,84,84)]\n",
    "\n",
    "        loss = self._get_reconstruction_loss((x, y_support), ways, 1, 3)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     loss = self._get_reconstruction_loss(batch)\n",
    "    #     self.log('val_loss', loss)\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     loss = self._get_reconstruction_loss(batch)\n",
    "    #     self.log('test_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41ffb170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distance(z_j_1, z_j_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcb8dd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((z_j_1.unsqueeze(0) - z_j_1.unsqueeze(1))** 2, dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75ad593f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 1, 2]), torch.Size([1, 27, 2]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_j_1.unsqueeze(1).shape, z_j_1.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "846e04b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = F.cosine_similarity(z_j_1.unsqueeze(1), z_j_1.unsqueeze(0), dim=2) / .5\n",
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8564c2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2901)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(sim, torch.Tensor([0 for i in range(27)]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "922fd2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9916, 0.9948, 0.9888, 0.9997, 0.9840, 0.9487, 0.9652, 0.9932, 0.9709,\n",
       "        0.9991, 0.9631, 0.9999, 0.9989, 0.9460, 0.9816, 0.9933, 0.9951, 0.9948,\n",
       "        0.9868, 0.9812, 0.9757, 0.9996, 0.9969, 0.9909, 0.9986, 0.9937])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(sim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a890d77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.9763)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = 0.\n",
    "temperature = .5\n",
    "for label in uniq_labels:\n",
    "    z_j_t = z_j_labels[z_j_labels[:, 2] == label][:, :2]\n",
    "    sim = F.cosine_similarity(z_j_t.unsqueeze(1), z_j_t.unsqueeze(0), dim=2) / temperature\n",
    "    loss += F.cross_entropy(sim, torch.Tensor([1/temperature for i in range(sim.shape[0])]).long())\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec27cc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 150])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = F.cosine_similarity(z_i.unsqueeze(1), z_j.unsqueeze(0), dim=2).t().contiguous() / .5\n",
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2a4c0b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.2548)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "label_i = query_labels[i]\n",
    "numerator = torch.exp(sim[i, label_i])\n",
    "numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "72284146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1081.2456)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denominator = torch.exp(sim[1:, label_i]).sum()\n",
    "denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fa7a35d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0042)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(numerator/denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d2006e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2467, 0.4349],\n",
       "        [0.3201, 0.3841],\n",
       "        [0.3509, 0.3562],\n",
       "        [0.2715, 0.4199],\n",
       "        [0.2435, 0.4367]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_i / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1923c5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([1, 5, 2]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_i.shape, z_i.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d3648fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([150, 2]), torch.Size([150, 1, 2]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_j.shape, z_j.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cdaa657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 150])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((z_i.unsqueeze(1) - z_j.unsqueeze(0))**2, dim=-1).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(dists, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39782ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.cat([z_i, z_j], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = F.cosine_similarity(z_i.unsqueeze(1), z_j.unsqueeze(0), dim=2) / .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = sim.t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c503382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_i_j = torch.diag(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afdf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_i_j = sim[0, 5-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e012fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_i_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = torch.exp(sim_i_j)\n",
    "numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d68ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = torch.sum(\n",
    "    torch.ones((5, )).scatter_(0, torch.tensor([0]), 0.0).bool() * torch.exp(sim[0,:])\n",
    ")\n",
    "denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9bf74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ij = -torch.log(numerator / denominator)\n",
    "loss_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd200e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### for ji?\n",
    "\n",
    "sim_i_j_2 = sim[0 + 5- 1, 0]\n",
    "sim_i_j_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e414d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator2 = torch.exp(sim_i_j_2)\n",
    "numerator2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768347c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator2 = torch.sum(\n",
    "    torch.ones((5, )).scatter_(0, torch.tensor([4]), 0.0).bool() * torch.exp(sim[4,:])\n",
    ")\n",
    "denominator2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06756e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ij2 = -torch.log(numerator2 / denominator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cdd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ij, loss_ij2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddb1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_ij(i, j):\n",
    "    # shape of sim initially is (n_clusters, n_aug_images)\n",
    "    # so we have (5, 150) - similarity between each cluster and augmented image\n",
    "    # transposed it becomes (150, 5) - similarity between each augmented image and cluster\n",
    "    sim = F.cosine_similarity(z_i.unsqueeze(1), z_j.unsqueeze(0), dim=2) / .5\n",
    "    sim = sim.t().contiguous()\n",
    "\n",
    "    # numerator math\n",
    "    sim_i_j = sim[i, j]\n",
    "    print(f\"sim({i}, {j})={sim_i_j}\")\n",
    "\n",
    "    numerator = torch.exp(sim_i_j)\n",
    "    print(\"Numerator\", numerator)\n",
    "    \n",
    "\n",
    "    # denominator math\n",
    "    # because there are 5 classes\n",
    "\n",
    "    mask = torch.ones((5, )).scatter(0, torch.tensor([i]), 0.0).bool()\n",
    "    print(f\"1{{k!={i}}}\", mask)\n",
    "    denominator = torch.sum(\n",
    "        mask * torch.exp(sim[i,:])\n",
    "    )\n",
    "    print(\"Denominator\", denominator)\n",
    "    loss_ij = -torch.log(numerator / denominator)\n",
    "    print(f\"loss({i},{j})={loss_ij}\\n\")\n",
    "    return loss_ij.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74be81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 # n_clusters\n",
    "loss = 0.\n",
    "for k in range(0, N):\n",
    "    loss += l_ij(k, k + N - 1) + l_ij(k + N - 1, k)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f94c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 5\n",
    "N = 2 * class_num\n",
    "mask = torch.ones((N, N))\n",
    "mask = mask.fill_diagonal_(0)\n",
    "for i in range(class_num):\n",
    "    mask[i, class_num + i] = 0\n",
    "    mask[class_num + i, i] = 0\n",
    "    mask = mask.bool()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e690769",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i = z_i.sum(0).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14503027",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_i.unsqueeze(1).shape, z_j.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa00b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dists = torch.sum((z_i.unsqueeze(1) - z_j.unsqueeze(0))** 2, dim=-1)\n",
    "dists = F.cosine_similarity(z_i.unsqueeze(1), z_j.unsqueeze(0), dim=2) / 0.5\n",
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f319a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_labels = torch.tensor([[1, 1, 4, 4, 4, 1, 0, 3, 4, 4, 2, 3, 4, 4, 1, 2, 1, 0, 4, 0, 0, 2,\n",
    "       1, 4, 3, 2, 1, 4, 3, 2, 0, 1, 4, 0, 0, 0, 4, 1, 0, 0, 0, 1, 0, 4,\n",
    "       2, 2, 4, 3, 0, 0, 1, 1, 1, 2, 1, 2, 0, 4, 0, 0, 2, 4, 2, 2, 4, 1,\n",
    "       3, 3, 4, 3, 0, 3, 3, 2, 0, 4, 4, 4, 4, 4, 3, 2, 0, 3, 1, 3, 4, 4,\n",
    "       4, 3, 2, 3, 1, 1, 2, 1, 3, 3, 3, 2, 1, 0, 4, 3, 4, 4, 4, 3, 4, 4,\n",
    "       0, 0, 0, 4, 4, 0, 2, 2, 2, 4, 4, 4, 1, 1, 2, 2, 2, 1, 0, 1, 4, 4,\n",
    "       3, 3, 3, 3, 2, 3, 3, 1, 2, 2, 1, 1, 1, 1, 4, 3, 0, 0, 0, 1, 1, 3,\n",
    "       0, 3, 4, 3, 4, 4, 2, 0, 2, 0, 3, 3, 0, 0, 2, 2, 4, 0, 0, 1, 1, 2,\n",
    "       2, 1, 1, 0, 0, 4, 3, 2, 2, 3, 0, 0, 1, 4, 2, 2, 2, 2, 3, 4, 1, 4,\n",
    "       0, 1]])\n",
    "query_labels = raw_labels[:, 50 * 1 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05bd07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "augtarget = torch.tensor([[ 0,  0,  0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  5,  5,\n",
    "          6,  6,  6,  7,  7,  7,  8,  8,  8,  9,  9,  9, 10, 10, 10, 11, 11, 11,\n",
    "         12, 12, 12, 13, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 16, 17, 17, 17,\n",
    "         18, 18, 18, 19, 19, 19, 20, 20, 20, 21, 21, 21, 22, 22, 22, 23, 23, 23,\n",
    "         24, 24, 24, 25, 25, 25, 26, 26, 26, 27, 27, 27, 28, 28, 28, 29, 29, 29,\n",
    "         30, 30, 30, 31, 31, 31, 32, 32, 32, 33, 33, 33, 34, 34, 34, 35, 35, 35,\n",
    "         36, 36, 36, 37, 37, 37, 38, 38, 38, 39, 39, 39, 40, 40, 40, 41, 41, 41,\n",
    "         42, 42, 42, 43, 43, 43, 44, 44, 44, 45, 45, 45, 46, 46, 46, 47, 47, 47,\n",
    "         48, 48, 48, 49, 49, 49]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ee948",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d46cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a211e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67217462",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(dists.unsqueeze(0), torch.Tensor([[0 for i in range(150)]]).long(), reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6624bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(dists.unsqueeze(0), query_labels, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_labels.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf5bf7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rz = torch.load('data/raw_embeddings.pt').cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0197f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rz.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86eab886",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = umap.UMAP(n_components=3, random_state=42).fit(rz.detach().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84e131e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrz = mapper.transform(rz.detach().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1208993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrz_support = mrz[: 50 * 1]\n",
    "# e.g. [1,50*n_query,*(3,84,84)]\n",
    "mrz_query = mrz[50 * 1 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da07bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = cluster.KMeans(n_clusters=5)\n",
    "pred_labels = clf.fit_predict(mrz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11b3b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = clf.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "368ce1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = torch.from_numpy(mapper.inverse_transform(centroids)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8047bba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 64])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "32cf2ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rz_query = rz[:, 50:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "954d1b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 64]), torch.Size([1, 150, 64]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.shape, rz_query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3b4ec3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 150]), torch.Size([1, 150]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = cosine_similarity(centroids, rz_query)\n",
    "dists.shape, torch.from_numpy(pred_labels[50:]).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "456ed357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5604, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(dists, torch.from_numpy(pred_labels[50:]).unsqueeze(0).long(), reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c47c86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_xent_loss(z_i, z_j, query_labels, temperature=.5, reduction='mean'):\n",
    "    N = z_j.shape[1]\n",
    "    # calculating distance from every centroid to every augmented image\n",
    "    dists = cosine_similarity(z_i, z_j) / temperature\n",
    "    labels = torch.zeros(N).to(dists.device).long()\n",
    "    print(dists.shape, query_labels.shape)\n",
    "    loss = F.cross_entropy(dists, labels.unsqueeze(0), reduction=reduction)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a567a38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 150]) torch.Size([150])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.6147, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt_xent_loss(centroids, rz_query, torch.from_numpy(pred_labels[50:]).long(), temperature=1., reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e3157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c07cba5fc4ead51138727dff7e3432f9ddfc5280df3e75c751c30010a4f78a5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
