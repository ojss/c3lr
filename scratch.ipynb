{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import torchmeta\n",
    "from torchmeta.datasets.helpers import omniglot\n",
    "from functools import reduce\n",
    "from operator import __add__\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "import torch\n",
    "import umap\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from unsupervised_meta_learning.pl_dataloaders import (UnlabelledDataModule, get_episode_loader,\n",
    "                                                       UnlabelledDataset)\n",
    "from unsupervised_meta_learning.proto_utils import euclidean_distance, cosine_similarity, nt_xent_loss, cluster_diff_loss\n",
    "from unsupervised_meta_learning.protoclr import ProtoCLR, get_train_images\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn import cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchmeta.datasets.Omniglot('./data/untarred', num_classes_per_task=5, transform=Compose([Resize(28), ToTensor()]), meta_train=True, use_vinyals_split=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchmeta.transforms.ClassSplitter(dataset, shuffle=True, num_train_per_class=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torchmeta.utils.data.BatchMetaDataLoader(dataset, batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = omniglot(\"./data/untarred\", ways=5, shots=1, use_vinyals_split=True,meta_train=True, download=True)\n",
    "dataloader = torchmeta.utils.data.BatchMetaDataLoader(dataset, batch_size=1, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_descriptor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "targets  = []\n",
    "cntr = 0\n",
    "for xs in dataloader:\n",
    "    if cntr > 5000:\n",
    "        break\n",
    "    else:\n",
    "        cntr += 1\n",
    "    d, t = xs['train']\n",
    "    data.append(d.squeeze(0))\n",
    "    targets.append(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25005, 1, 28, 28])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 1, 2, 0, 4],\n",
       "        [0, 3, 1, 4, 2],\n",
       "        [4, 0, 1, 3, 2],\n",
       "        ...,\n",
       "        [3, 2, 1, 4, 0],\n",
       "        [4, 3, 2, 1, 0],\n",
       "        [4, 3, 1, 0, 2]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 0,  ..., 2, 3, 1])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(targets).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = UnlabelledDataModule(\n",
    "    \"omniglot\",\n",
    "    \"./data/untarred\",\n",
    "    split=\"train\",\n",
    "    transform=None,\n",
    "    n_support=1,\n",
    "    n_query=3,\n",
    "    n_images=None,\n",
    "    n_classes=None,\n",
    "    batch_size=50,\n",
    "    seed=10,\n",
    "    mode=\"trainval\",\n",
    "    num_workers=0,\n",
    "    eval_ways=5,\n",
    "    eval_support_shots=1,\n",
    "    eval_query_shots=15,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 100\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "dataset_train = UnlabelledDataset(\n",
    "            'omniglot',\n",
    "            'data/untarred/',\n",
    "            split=\"train\",\n",
    "            transform=None,\n",
    "            n_images=None,\n",
    "            n_classes=5,\n",
    "            n_support=1,\n",
    "            n_query=3,\n",
    "            no_aug_support=False,\n",
    "            no_aug_query=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028 20560\n",
      "<class 'numpy.ndarray'>\n",
      "172 3440\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    profiler='simple',\n",
    "    max_epochs=2,\n",
    "    limit_train_batches=100,\n",
    "    fast_dev_run=False,\n",
    "    limit_val_batches=15,\n",
    "    limit_test_batches=600,\n",
    "    num_sanity_val_steps=2, gpus=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_sizes = (3, 3)\n",
    "conv_padding = reduce(__add__, [(k // 2 + (k - 2 * (k // 2)) - 1, k // 2) for k in kernel_sizes[::-1]])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_size=64, out_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # nn.ZeroPad2d(conv_padding),\n",
    "            nn.Conv2d(in_channels, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14 x 14\n",
    "\n",
    "            # nn.ZeroPad2d(conv_padding),\n",
    "            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 7x7\n",
    "\n",
    "            # nn.ZeroPad2d(conv_padding),\n",
    "            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 3x3\n",
    "\n",
    "            # nn.ZeroPad2d(conv_padding),\n",
    "            nn.Conv2d(hidden_size, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 1x1\n",
    "            # nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.encoder(inputs)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_size=64, out_channels=64):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.UpsamplingNearest2d(size=(4, 4)),\n",
    "            nn.Conv2d(in_channels=out_channels,\n",
    "                      out_channels=hidden_size, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.UpsamplingNearest2d(size=(7, 7)),\n",
    "            nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size,\n",
    "                      kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.UpsamplingNearest2d(size=(14, 14)),\n",
    "            nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size,\n",
    "                      kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.UpsamplingNearest2d(size=(28, 28)),\n",
    "            nn.Conv2d(in_channels=hidden_size, out_channels=in_channels,\n",
    "                      kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.decoder(inputs)\n",
    "        \n",
    "class AE(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_size=64, out_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(in_channels=in_channels, hidden_size=hidden_size, out_channels=out_channels)\n",
    "        self.decoder = Decoder(in_channels=in_channels, hidden_size=hidden_size, out_channels=out_channels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        print(inputs.shape)\n",
    "        embeddings = self.encoder(inputs.view(-1, *inputs.shape[-3:]))\n",
    "        print(embeddings.shape)\n",
    "        recons = self.decoder(embeddings.unsqueeze(-1).unsqueeze(-1))\n",
    "        return embeddings.view(*inputs.shape[:-3], -1), recons.view(*inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProtoCLR(\n",
    "    n_support=1, n_query=3, batch_size=50, distance='cosine', τ=.5,\n",
    "    num_input_channels=1, decoder_class=Decoder, encoder_class=Encoder,\n",
    "    lr_decay_step=25000, lr_decay_rate=.5, ae=True, gamma=1., log_images=True)\n",
    "dataset_train = UnlabelledDataset(\n",
    "    dataset='omniglot',\n",
    "    datapath='./data/',\n",
    "    split='train',\n",
    "    n_support=1,\n",
    "    n_query=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(), # Image grid to single feature vector\n",
    "            nn.Linear(2*16*c_hid, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2*16*c_hid),\n",
    "            act_fn()\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 => 32x32\n",
    "            nn.Tanh() # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_channel_size: int,\n",
    "                 latent_dim: int,\n",
    "                 encoder_class : object = Encoder,\n",
    "                 decoder_class : object = Decoder,\n",
    "                 num_input_channels: int = 1,\n",
    "                 width: int = 28,\n",
    "                 height: int = 28):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        # Example input array needed for visualizing the graph of the network\n",
    "        self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward function takes in an image and returns the reconstructed image\n",
    "        \"\"\"\n",
    "        # print(x.shape)\n",
    "        z = self.encoder(x.view(-1, *x.shape[-3:]))\n",
    "        x_hat = self.decoder(z)\n",
    "        # z = self.encoder(x)\n",
    "        # x_hat = self.decoder(z)\n",
    "        return x_hat.view(*x.shape)\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch, ways, n_supp, n_query):\n",
    "        \"\"\"\n",
    "        Given a batch of images, this function returns the reconstruction loss (MSE in our case)\n",
    "        \"\"\"\n",
    "        x, _ = batch # We do not need the labels\n",
    "        x_hat = self.forward(x)\n",
    "\n",
    "        x_supp = x[:,:ways * n_supp]\n",
    "        r_supp = x_hat[:,:ways * n_supp]\n",
    "        r_query = x_hat[:,ways * n_supp:]\n",
    "        # print(\"####\", r_supp.shape, r_query.shape)\n",
    "\n",
    "        r_query = r_query.view(1, r_supp.shape[1], 3, 1, 28, 28)\n",
    "\n",
    "\n",
    "\n",
    "        # loss = F.mse_loss(x.squeeze(0), x_hat, reduction='none').sum(dim=[1, 2, 3,]).mean(dim=[0])\n",
    "        loss = F.mse_loss(\n",
    "                r_query,\n",
    "                torch.broadcast_to(x_supp.unsqueeze(2), r_query.shape),\n",
    "                reduction='none').sum(dim=[1, 2, 3, 4, 5]).mean(dim=[0])\n",
    "        # loss = F.mse_loss(\n",
    "        #         r_supp, \n",
    "        #         x_supp,\n",
    "        #         reduction='none').sum(dim=[1, 2, 3, 4,]).mean(dim=[0])\n",
    "\n",
    "        # loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        # loss = loss.sum(dim=[1,2,3]).mean(dim=[0])\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                         mode='min',\n",
    "                                                         factor=0.2,\n",
    "                                                         patience=20,\n",
    "                                                         min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"train_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data = batch['data'].to(self.device) # [batch_size x ways x shots x image_dim]\n",
    "        data = data.unsqueeze(0)\n",
    "        # e.g. 50 images, 2 support, 2 query, miniImageNet: torch.Size([1, 50, 4, 3, 84, 84])\n",
    "        batch_size = data.size(0)\n",
    "        ways = data.size(1)\n",
    "        x_support = data[:,:,:1]\n",
    "        x_support = x_support.reshape((batch_size, ways * 1, *x_support.shape[-3:])) # e.g. [1,50*n_support,*(3,84,84)]\n",
    "        x_query = data[:,:,1:]\n",
    "        x_query = x_query.reshape((batch_size, ways * 3, *x_query.shape[-3:])) # e.g. [1,50*n_query,*(3,84,84)]\n",
    "        # print(f'!!!! {x_support.shape}')\n",
    "        # Create dummy query labels\n",
    "        y_query = torch.arange(ways).unsqueeze(0).unsqueeze(2) # batch and shot dim\n",
    "        y_query = y_query.repeat(batch_size, 1, 1)\n",
    "        y_query = y_query.view(batch_size, -1).to('cuda')\n",
    "\n",
    "        y_support = torch.arange(ways).unsqueeze(0).unsqueeze(2) # batch and shot dim\n",
    "        y_support = y_support.repeat(batch_size, 1, 1)\n",
    "        y_support = y_support.view(batch_size, -1).to('cuda')\n",
    "        x = torch.cat([x_support, x_query], 1) # e.g. [1,50*(n_support+n_query),*(3,84,84)]\n",
    "\n",
    "        loss = self._get_reconstruction_loss((x, y_support), ways, 1, 3)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     loss = self._get_reconstruction_loss(batch)\n",
    "    #     self.log('val_loss', loss)\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     loss = self._get_reconstruction_loss(batch)\n",
    "    #     self.log('test_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distance(z_j_1, z_j_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((z_j_1.unsqueeze(0) - z_j_1.unsqueeze(1))** 2, dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 1, 2]), torch.Size([1, 27, 2]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_j_1.unsqueeze(1).shape, z_j_1.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = F.cosine_similarity(z_j_1.unsqueeze(1), z_j_1.unsqueeze(0), dim=2) / .5\n",
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2901)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(sim, torch.Tensor([0 for i in range(27)]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9916, 0.9948, 0.9888, 0.9997, 0.9840, 0.9487, 0.9652, 0.9932, 0.9709,\n",
       "        0.9991, 0.9631, 0.9999, 0.9989, 0.9460, 0.9816, 0.9933, 0.9951, 0.9948,\n",
       "        0.9868, 0.9812, 0.9757, 0.9996, 0.9969, 0.9909, 0.9986, 0.9937])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(sim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.9763)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = 0.\n",
    "temperature = .5\n",
    "for label in uniq_labels:\n",
    "    z_j_t = z_j_labels[z_j_labels[:, 2] == label][:, :2]\n",
    "    sim = F.cosine_similarity(z_j_t.unsqueeze(1), z_j_t.unsqueeze(0), dim=2) / temperature\n",
    "    loss += F.cross_entropy(sim, torch.Tensor([1/temperature for i in range(sim.shape[0])]).long())\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 150])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = F.cosine_similarity(z_i.unsqueeze(1), z_j.unsqueeze(0), dim=2).t().contiguous() / .5\n",
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.2548)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "label_i = query_labels[i]\n",
    "numerator = torch.exp(sim[i, label_i])\n",
    "numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1081.2456)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denominator = torch.exp(sim[1:, label_i]).sum()\n",
    "denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0042)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(numerator/denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2467, 0.4349],\n",
       "        [0.3201, 0.3841],\n",
       "        [0.3509, 0.3562],\n",
       "        [0.2715, 0.4199],\n",
       "        [0.2435, 0.4367]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_i / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([1, 5, 2]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_i.shape, z_i.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([150, 2]), torch.Size([150, 1, 2]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_j.shape, z_j.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 150])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((z_i.unsqueeze(1) - z_j.unsqueeze(0))**2, dim=-1).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(dists, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.cat([z_i, z_j], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = F.cosine_similarity(z_i.unsqueeze(1), z_j.unsqueeze(0), dim=2) / .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = sim.t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_i_j = torch.diag(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_i_j = sim[0, 5-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_i_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = torch.exp(sim_i_j)\n",
    "numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = torch.sum(\n",
    "    torch.ones((5, )).scatter_(0, torch.tensor([0]), 0.0).bool() * torch.exp(sim[0,:])\n",
    ")\n",
    "denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ij = -torch.log(numerator / denominator)\n",
    "loss_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### for ji?\n",
    "\n",
    "sim_i_j_2 = sim[0 + 5- 1, 0]\n",
    "sim_i_j_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator2 = torch.exp(sim_i_j_2)\n",
    "numerator2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator2 = torch.sum(\n",
    "    torch.ones((5, )).scatter_(0, torch.tensor([4]), 0.0).bool() * torch.exp(sim[4,:])\n",
    ")\n",
    "denominator2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ij2 = -torch.log(numerator2 / denominator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ij, loss_ij2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_ij(i, j):\n",
    "    # shape of sim initially is (n_clusters, n_aug_images)\n",
    "    # so we have (5, 150) - similarity between each cluster and augmented image\n",
    "    # transposed it becomes (150, 5) - similarity between each augmented image and cluster\n",
    "    sim = F.cosine_similarity(z_i.unsqueeze(1), z_j.unsqueeze(0), dim=2) / .5\n",
    "    sim = sim.t().contiguous()\n",
    "\n",
    "    # numerator math\n",
    "    sim_i_j = sim[i, j]\n",
    "    print(f\"sim({i}, {j})={sim_i_j}\")\n",
    "\n",
    "    numerator = torch.exp(sim_i_j)\n",
    "    print(\"Numerator\", numerator)\n",
    "    \n",
    "\n",
    "    # denominator math\n",
    "    # because there are 5 classes\n",
    "\n",
    "    mask = torch.ones((5, )).scatter(0, torch.tensor([i]), 0.0).bool()\n",
    "    print(f\"1{{k!={i}}}\", mask)\n",
    "    denominator = torch.sum(\n",
    "        mask * torch.exp(sim[i,:])\n",
    "    )\n",
    "    print(\"Denominator\", denominator)\n",
    "    loss_ij = -torch.log(numerator / denominator)\n",
    "    print(f\"loss({i},{j})={loss_ij}\\n\")\n",
    "    return loss_ij.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 # n_clusters\n",
    "loss = 0.\n",
    "for k in range(0, N):\n",
    "    loss += l_ij(k, k + N - 1) + l_ij(k + N - 1, k)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 5\n",
    "N = 2 * class_num\n",
    "mask = torch.ones((N, N))\n",
    "mask = mask.fill_diagonal_(0)\n",
    "for i in range(class_num):\n",
    "    mask[i, class_num + i] = 0\n",
    "    mask[class_num + i, i] = 0\n",
    "    mask = mask.bool()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i = z_i.sum(0).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_i.unsqueeze(1).shape, z_j.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dists = torch.sum((z_i.unsqueeze(1) - z_j.unsqueeze(0))** 2, dim=-1)\n",
    "dists = F.cosine_similarity(z_i.unsqueeze(1), z_j.unsqueeze(0), dim=2) / 0.5\n",
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_labels = torch.tensor([[1, 1, 4, 4, 4, 1, 0, 3, 4, 4, 2, 3, 4, 4, 1, 2, 1, 0, 4, 0, 0, 2,\n",
    "       1, 4, 3, 2, 1, 4, 3, 2, 0, 1, 4, 0, 0, 0, 4, 1, 0, 0, 0, 1, 0, 4,\n",
    "       2, 2, 4, 3, 0, 0, 1, 1, 1, 2, 1, 2, 0, 4, 0, 0, 2, 4, 2, 2, 4, 1,\n",
    "       3, 3, 4, 3, 0, 3, 3, 2, 0, 4, 4, 4, 4, 4, 3, 2, 0, 3, 1, 3, 4, 4,\n",
    "       4, 3, 2, 3, 1, 1, 2, 1, 3, 3, 3, 2, 1, 0, 4, 3, 4, 4, 4, 3, 4, 4,\n",
    "       0, 0, 0, 4, 4, 0, 2, 2, 2, 4, 4, 4, 1, 1, 2, 2, 2, 1, 0, 1, 4, 4,\n",
    "       3, 3, 3, 3, 2, 3, 3, 1, 2, 2, 1, 1, 1, 1, 4, 3, 0, 0, 0, 1, 1, 3,\n",
    "       0, 3, 4, 3, 4, 4, 2, 0, 2, 0, 3, 3, 0, 0, 2, 2, 4, 0, 0, 1, 1, 2,\n",
    "       2, 1, 1, 0, 0, 4, 3, 2, 2, 3, 0, 0, 1, 4, 2, 2, 2, 2, 3, 4, 1, 4,\n",
    "       0, 1]])\n",
    "query_labels = raw_labels[:, 50 * 1 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augtarget = torch.tensor([[ 0,  0,  0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  5,  5,\n",
    "          6,  6,  6,  7,  7,  7,  8,  8,  8,  9,  9,  9, 10, 10, 10, 11, 11, 11,\n",
    "         12, 12, 12, 13, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 16, 17, 17, 17,\n",
    "         18, 18, 18, 19, 19, 19, 20, 20, 20, 21, 21, 21, 22, 22, 22, 23, 23, 23,\n",
    "         24, 24, 24, 25, 25, 25, 26, 26, 26, 27, 27, 27, 28, 28, 28, 29, 29, 29,\n",
    "         30, 30, 30, 31, 31, 31, 32, 32, 32, 33, 33, 33, 34, 34, 34, 35, 35, 35,\n",
    "         36, 36, 36, 37, 37, 37, 38, 38, 38, 39, 39, 39, 40, 40, 40, 41, 41, 41,\n",
    "         42, 42, 42, 43, 43, 43, 44, 44, 44, 45, 45, 45, 46, 46, 46, 47, 47, 47,\n",
    "         48, 48, 48, 49, 49, 49]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(dists.unsqueeze(0), torch.Tensor([[0 for i in range(150)]]).long(), reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(dists.unsqueeze(0), query_labels, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_labels.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rz = torch.load('data/raw_embeddings.pt').cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rz.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = umap.UMAP(n_components=3, random_state=42).fit(rz.detach().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrz = mapper.transform(rz.detach().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrz_support = mrz[: 50 * 1]\n",
    "# e.g. [1,50*n_query,*(3,84,84)]\n",
    "mrz_query = mrz[50 * 1 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = cluster.KMeans(n_clusters=5)\n",
    "pred_labels = clf.fit_predict(mrz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = clf.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = torch.from_numpy(mapper.inverse_transform(centroids)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 64])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rz_query = rz[:, 50:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 64]), torch.Size([1, 150, 64]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.shape, rz_query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 150]), torch.Size([1, 150]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = cosine_similarity(centroids, rz_query)\n",
    "dists.shape, torch.from_numpy(pred_labels[50:]).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5604, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(dists, torch.from_numpy(pred_labels[50:]).unsqueeze(0).long(), reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_xent_loss(z_i, z_j, query_labels, temperature=.5, reduction='mean'):\n",
    "    N = z_j.shape[1]\n",
    "    # calculating distance from every centroid to every augmented image\n",
    "    dists = cosine_similarity(z_i, z_j) / temperature\n",
    "    labels = torch.zeros(N).to(dists.device).long()\n",
    "    print(dists.shape, query_labels.shape)\n",
    "    loss = F.cross_entropy(dists, labels.unsqueeze(0), reduction=reduction)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 150]) torch.Size([150])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.6147, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt_xent_loss(centroids, rz_query, torch.from_numpy(pred_labels[50:]).long(), temperature=1., reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c07cba5fc4ead51138727dff7e3432f9ddfc5280df3e75c751c30010a4f78a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
