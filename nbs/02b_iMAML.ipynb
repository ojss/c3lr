{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f3e7e-4c0e-4f68-a282-018ae80ec094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp imaml\n",
    "#export\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import higher\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from itertools import repeat\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "from unsupervised_meta_learning.pl_dataloaders import OmniglotDataModule\n",
    "from unsupervised_meta_learning.nn_utils import get_accuracy\n",
    "from unsupervised_meta_learning.maml import ConvolutionalNeuralNetwork\n",
    "\n",
    "import unsupervised_meta_learning.hypergrad as hg\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4216b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cg_solve(f_Ax, b, cg_iters=10, callback=None, verbose=False, residual_tol=1e-10, x_init=None):\n",
    "    \"\"\"\n",
    "    Goal: Solve Ax=b equivalent to minimizing f(x) = 1/2 x^T A x - x^T b\n",
    "    Assumption: A is PSD, no damping term is used here (must be damped externally in f_Ax)\n",
    "    Algorithm template from wikipedia\n",
    "    Verbose mode works only with numpy\n",
    "    \"\"\"\n",
    "       \n",
    "    if type(b) == torch.Tensor:\n",
    "        x = torch.zeros(b.shape[0]) if x_init is None else x_init\n",
    "        x = x.to(b.device)\n",
    "        if b.dtype == torch.float16:\n",
    "            x = x.half()\n",
    "        r = b - f_Ax(x)\n",
    "        p = r.clone()\n",
    "    elif type(b) == np.ndarray:\n",
    "        x = np.zeros_like(b) if x_init is None else x_init\n",
    "        r = b - f_Ax(x)\n",
    "        p = r.copy()\n",
    "    else:\n",
    "        print(\"Type error in cg\")\n",
    "\n",
    "    fmtstr = \"%10i %10.3g %10.3g %10.3g\"\n",
    "    titlestr = \"%10s %10s %10s %10s\"\n",
    "    if verbose: print(titlestr % (\"iter\", \"residual norm\", \"soln norm\", \"obj fn\"))\n",
    "\n",
    "    for i in range(cg_iters):\n",
    "        if callback is not None:\n",
    "            callback(x)\n",
    "        if verbose:\n",
    "            obj_fn = 0.5*x.dot(f_Ax(x)) - 0.5*b.dot(x)\n",
    "            norm_x = torch.norm(x) if type(x) == torch.Tensor else np.linalg.norm(x)\n",
    "            print(fmtstr % (i, r.dot(r), norm_x, obj_fn))\n",
    "\n",
    "        rdotr = r.dot(r)\n",
    "        Ap = f_Ax(p)\n",
    "        alpha = rdotr/(p.dot(Ap))\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        newrdotr = r.dot(r)\n",
    "        beta = newrdotr/rdotr\n",
    "        p = r + beta * p\n",
    "\n",
    "        if newrdotr < residual_tol:\n",
    "            # print(\"Early CG termination because the residual was small\")\n",
    "            break\n",
    "\n",
    "    if callback is not None:\n",
    "        callback(x)\n",
    "    if verbose: \n",
    "        obj_fn = 0.5*x.dot(f_Ax(x)) - 0.5*b.dot(x)\n",
    "        norm_x = torch.norm(x) if type(x) == torch.Tensor else np.linalg.norm(x)\n",
    "        print(fmtstr % (i, r.dot(r), norm_x, obj_fn))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc914cda-ab15-45f9-94a1-fa001d4f7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class iMAML(pl.LightningModule):\n",
    "    def __init__(self, model, loss_function, inner_lr, outer_lr, lam_lr, inner_steps, cg_steps, cg_damping, lam=0., lam_min=0.):\n",
    "        super().__init__()\n",
    "        self.automatic_optimization = False\n",
    "        self.accuracy = get_accuracy\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.meta_lr = outer_lr\n",
    "        self.inner_lr = inner_lr\n",
    "        self.outer_lr = outer_lr\n",
    "        self.lam_lr = lam_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.cg_steps = cg_steps\n",
    "        self.n_params = len(list(model.parameters()))\n",
    "        self.lam = lam\n",
    "        self.lam_min = lam_min\n",
    "        self.cg_damping = cg_damping\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        outer_opt = torch.optim.Adam(params=self.model.parameters(), lr=self.outer_lr)\n",
    "        inner_opt = torch.optim.SGD(params=self.model.parameters(), lr=self.inner_lr)\n",
    "        return outer_opt, inner_opt\n",
    "\n",
    "    def regularization_loss(self, w_0, lam=0.):\n",
    "        \"\"\"\n",
    "        Add a regularization loss onto the weights\n",
    "        The proximal term regularizes around the point w_0\n",
    "        Strength of regularization is lambda\n",
    "        lambda can either be scalar (type float) or ndarray (numpy.ndarray)\n",
    "        \"\"\"\n",
    "        regu_loss = 0.\n",
    "        offset = 0\n",
    "\n",
    "        regu_lam = lam if type(lam) == float or np.float64 else utils.to_tensor(lam)\n",
    "        if w_0.dtype == torch.float16:\n",
    "            try:\n",
    "                regu_lam.half()\n",
    "            except:\n",
    "                regu_lam = np.float16(regu_lam)\n",
    "        for param in self.model.parameters():\n",
    "            delta = param.view(-1) - w_0[offset:offset + param.nelement()].view(-1)\n",
    "            if type(regu_lam) == float or np.float64:\n",
    "                regu_loss += 0.5 * regu_lam * torch.sum(delta ** 2)\n",
    "            else:\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "                param_lam = regu_lam[offset:offset + param.nelement()].view(-1)\n",
    "                param_delta = delta * param_lam\n",
    "                regu_loss += 0.5 * torch.sum(param_delta ** 2)\n",
    "            offset += param.nelement()\n",
    "        return regu_loss\n",
    "\n",
    "    def get_loss(self, fmodel, x, y, return_np=False):\n",
    "        y_hat = fmodel.forward(x)\n",
    "        loss = self.loss_function(y_hat, y)\n",
    "\n",
    "        if return_np:\n",
    "            loss = loss.cpu().detach().numpy()\n",
    "        return loss\n",
    "    \n",
    "    def get_params(self):\n",
    "        return torch.cat([param.data.view(-1) for param in self.model.parameters()], 0).clone()\n",
    "\n",
    "    \n",
    "    def inner_loop(self, fmodel, diffopt, train_input, train_target, add_reg_loss=False, w_0=None, lam=0.):\n",
    "        train_loss = []\n",
    "        for i in range(self.inner_steps):\n",
    "            train_logit = fmodel(train_input)\n",
    "            tmpl = F.cross_entropy(train_logit, train_target)\n",
    "            inner_loss = tmpl +   self.regularization_loss(w_0, lam) if add_reg_loss else tmpl\n",
    "            diffopt.step(inner_loss)\n",
    "            train_loss.append(inner_loss.detach())\n",
    "    \n",
    "        return train_loss\n",
    "    \n",
    "    def matrix_evaluator(self, task, lam, fmodel=None, regu_coef=1.0, lam_damping=10.0, x=None, y=None):\n",
    "        \"\"\"\n",
    "        Constructor function that can be given to CG optimizer\n",
    "        Works for both type(lam) == float and type(lam) == np.ndarray\n",
    "        \"\"\"\n",
    "        if type(lam) == np.ndarray:\n",
    "            lam = torch.from_numpy(lam).float().to(self.device)\n",
    "        def evaluator(v):\n",
    "            hvp = self.hessian_vector_product(fmodel, task, v, x=x, y=y)\n",
    "            Av = (1.0 + regu_coef) * v + hvp / (lam + lam_damping)\n",
    "            return Av\n",
    "        return evaluator\n",
    "\n",
    "    def hessian_vector_product(self, fmodel, task, vector, params=None, x=None, y=None):\n",
    "        \"\"\"\n",
    "        Performs hessian vector product on the train set in task with the provided vector\n",
    "        \"\"\"\n",
    "        if x is not None and y is not None:\n",
    "            xt, yt = x, y\n",
    "        else:\n",
    "            xt, yt = task['train']\n",
    "        if params is not None:\n",
    "            self.set_params(params)\n",
    "        tloss = self.get_loss(fmodel, xt, yt)\n",
    "        grad_ft = torch.autograd.grad(tloss, fmodel.parameters(), create_graph=True)\n",
    "        flat_grad = torch.cat([g.contiguous().view(-1) for g in grad_ft])\n",
    "        vec = vector.to(self.device)\n",
    "        h = torch.sum(flat_grad * vec)\n",
    "        hvp = torch.autograd.grad(h, fmodel.parameters())\n",
    "        hvp_flat = torch.cat([g.contiguous().view(-1) for g in hvp])\n",
    "        return hvp_flat\n",
    "\n",
    "    def outer_step_with_grad(self, grad, meta_opt, flat_grad=False):\n",
    "        \"\"\"\n",
    "        Given the gradient, step with the outer optimizer using the gradient.\n",
    "        Assumed that the gradient is a tuple/list of size compatible with model.parameters()\n",
    "        If flat_grad, then the gradient is a flattened vector\n",
    "        \"\"\"\n",
    "        check = 0\n",
    "        for p in self.model.parameters():\n",
    "            check = check + 1 if type(p.grad) == type(None) else check\n",
    "        if check > 0:\n",
    "            # init grad fields as needed\n",
    "            dumdum_loss = self.regularization_loss(self.get_params())\n",
    "            dumdum_loss.backward()\n",
    "        if flat_grad:\n",
    "            offset = 0\n",
    "            grad = grad.to(self.device)\n",
    "            for p in self.model.parameters():\n",
    "                this_grad = grad[offset:offset + p.nelement()].view(p.size())\n",
    "                p.grad.copy_(this_grad)\n",
    "                offset += p.nelement()\n",
    "        else:\n",
    "            for i, p in enumerate(self.model.parameters()):\n",
    "                p.grad = grad[i]\n",
    "        meta_opt.step()\n",
    "    \n",
    "#     @torch.enable_grad()\n",
    "    def meta_learn(self, batch, batch_idx):\n",
    "        meta_optimizer, inner_optimizer = self.optimizers(use_pl_optimizer=False)     \n",
    "        tr_xs, tr_ys = batch[\"train\"][0].to(self.device), batch[\"train\"][1].to(self.device)\n",
    "        tst_xs, tst_ys = batch[\"test\"][0].to(self.device), batch[\"test\"][1].to(self.device)\n",
    "        \n",
    "        lam_grad = torch.tensor(0., device=self.device)\n",
    "        batch_size = tr_xs.shape[0]\n",
    "        outer_loss, acc = torch.tensor(0., device=self.device), torch.tensor(0., device=self.device)\n",
    "        \n",
    "        meta_grad = 0.\n",
    "        inner_opt_kwargs = {'step_size': self.inner_lr}\n",
    "        \n",
    "        torch.cuda.memory_summary(0)\n",
    "        \n",
    "        meta_optimizer.zero_grad()\n",
    "        for t_idx, (tr_x, tr_y, tst_x, tst_y) in enumerate(zip(tr_xs, tr_ys, tst_xs, tst_ys)):\n",
    "            with higher.innerloop_ctx(self.model, inner_optimizer, copy_initial_weights=True) as (fmodel, diffopt):\n",
    "                train_losses = self.inner_loop(fmodel, diffopt, tr_x, tr_y)\n",
    "            \n",
    "            regu_loss = self.regularization_loss(self.get_params(), self.lam)\n",
    "            diffopt.step(regu_loss)\n",
    "            \n",
    "            tst_loss = self.get_loss(fmodel, tst_x, tst_y)\n",
    "            outer_loss += tst_loss\n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_logit = fmodel(tst_x)\n",
    "                preds = test_logit.softmax(dim=-1)\n",
    "                acc += self.accuracy(test_logit, tst_y)\n",
    "\n",
    "            tst_grad = torch.autograd.grad(tst_loss, fmodel.parameters())\n",
    "\n",
    "            flat_grad = torch.cat([g.contiguous().view(-1) for g in tst_grad])\n",
    "\n",
    "            if self.cg_steps <= 1:\n",
    "                outer_grad = flat_grad\n",
    "            else:\n",
    "                task_matrix_eval = self.matrix_evaluator(self.lam, self.cg_damping, fmodel=fmodel, x=tr_x, y=tr_y)\n",
    "                outer_grad = cg_solve(task_matrix_eval, flat_grad, self.cg_steps, x_init=None)\n",
    "            # grad collection based on the CG solver, instead of having a normal outer grad it has to be calculated from the cg solver\n",
    "            # see MAML for what is supposedly normal\n",
    "            meta_grad += outer_grad\n",
    "\n",
    "            if self.lam_lr <= 0.:\n",
    "                task_lam_grad = torch.tensor(0., device=self.device)\n",
    "            else:\n",
    "                # TODO: lambda learning\n",
    "                train_loss = self.get_loss(fmodel, tr_x, tr_y)\n",
    "                train_grad = torch.autograd.grad(train_loss, fmodel.parameters())\n",
    "                train_grad = torch.cat([g.contiguous().view(-1) for g in train_grad])\n",
    "                inner_prod = train_grad.dot(outer_grad)\n",
    "                task_lam_grad = inner_prod / (self.lam**2 + 0.1)\n",
    "        \n",
    "            lam_grad += (task_lam_grad / batch_size)\n",
    "        meta_grad.div_(batch_size)\n",
    "        self.outer_step_with_grad(meta_grad, meta_optimizer, flat_grad=True)\n",
    "        lam_delta = - self.lam_lr * lam_grad\n",
    "        self.lam = torch.clamp(self.lam + lam_delta, self.lam_min, 5000.)\n",
    "        outer_loss.div_(batch_size).detach_()\n",
    "        acc.div_(batch_size).detach_()\n",
    "        return outer_loss, acc\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        train_loss, train_acc = self.meta_learn(batch, batch_idx)\n",
    "\n",
    "        self.log_dict({\n",
    "            'tr_accuracy': train_acc,\n",
    "            'tr_loss': train_loss\n",
    "        }, prog_bar=True, logger=True)\n",
    "        return {'tr_loss': train_loss, 'tr_acc': train_acc}\n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        val_loss, val_acc = self.meta_learn(batch, batch_idx)\n",
    "        \n",
    "        self.log_dict({\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc\n",
    "        })\n",
    "    \n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss, test_acc = self.meta_learn(batch, batch_idx)\n",
    "        self.log_dict({\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_acc\n",
    "        })\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe0875-b062-416e-80ba-a2d8a9a964f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = OmniglotDataModule(\n",
    "        \"data\",\n",
    "        shots=1,\n",
    "        ways=5,\n",
    "        shuffle_ds=True,\n",
    "        test_shots=16,\n",
    "        meta_train=True,\n",
    "        download=True,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb9918-ec50-40eb-8cd1-d23e1027773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = iMAML(model=ConvolutionalNeuralNetwork(1, 5, hidden_size=64), loss_function=F.cross_entropy, outer_lr=1e-2, inner_lr=1e-2, lam=2., lam_lr=0, inner_steps=5, cg_steps=1, cg_damping=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f556c1-4515-4855-8229-933b9b87f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = WandbLogger(\n",
    "    project='iMAML',\n",
    "    config={\n",
    "        'batch_size': 16,\n",
    "        'steps': 100,\n",
    "        'dataset': \"omniglot\",\n",
    "        'val/test': 'enabled',\n",
    "        'hessian-free': 'enabled',\n",
    "        'inner_steps': 15,\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        profiler='simple',\n",
    "        max_steps=50,\n",
    "        # val_check_interval=25,\n",
    "        limit_train_batches=50,\n",
    "        limit_val_batches=0.,\n",
    "        limit_test_batches=2,\n",
    "        fast_dev_run=False,\n",
    "        gpus=1,\n",
    "        # logger=logger,\n",
    "        log_every_n_steps=1,\n",
    "        flush_logs_every_n_steps=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab439e-8b49-4763-b740-4e823b101040",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292be7e-85bf-4196-b476-1a791bb295bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Your test_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dce1f4aeac473a91ad6c3ace73bafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/ojass/anaconda3/envs/jax/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b850b2-b10f-41ec-b914-c80719ae1792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 87994<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087fb5ed898f4dcd9c2bbd8416887b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/ojass/Projects/meta-learning/wandb/run-20210611_183053-3mrivls5/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/ojass/Projects/meta-learning/wandb/run-20210611_183053-3mrivls5/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>tr_accuracy</td><td>0.30234</td></tr><tr><td>tr_loss</td><td>1.5587</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>trainer/global_step</td><td>99</td></tr><tr><td>_runtime</td><td>113</td></tr><tr><td>_timestamp</td><td>1623429166</td></tr><tr><td>_step</td><td>99</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>tr_accuracy</td><td>█▄▂▃▂▃▃▂▁▃▁▃▃▃▂▃▄▃▃▂▂▂▄▃▃▃▂▂▂▂▁▃▃▄▃▄▃▃▄▄</td></tr><tr><td>tr_loss</td><td>▁▄▅▅▅▆▅▅▅▅▅▅▅▄▅▅▄▅▅▅▅▅▄▅▅▇█▆▇▆▅▄▄▄▄▃▅▄▄▄</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">chocolate-moon-38</strong>: <a href=\"https://wandb.ai/p0int/iMAML/runs/3mrivls5\" target=\"_blank\">https://wandb.ai/p0int/iMAML/runs/3mrivls5</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b764773-52d9-496f-9987-c00fb220c8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_nn_utils.ipynb.\n",
      "Converted 01b_data_loaders_pl.ipynb.\n",
      "Converted 01c_grad_utils.ipynb.\n",
      "Converted 01d_hessian_free.ipynb.\n",
      "Converted 02_maml_pl.ipynb.\n",
      "Converted 02b_iMAML.ipynb.\n",
      "Converted 03_protonet_pl.ipynb.\n",
      "Converted 04_cactus.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ee594-ba11-4f1f-8230-8c12943ca966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
